{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb635523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp utils.duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6ed81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from uberduck_ml_dev.models.common import LinearNorm\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import pdb\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "        sinusoidal positional encoding\n",
    "        https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[x]\n",
    "\n",
    "\n",
    "class GaussianUpsampling(nn.Module):\n",
    "    \"\"\"\n",
    "        Non-attention Tacotron:\n",
    "            - https://arxiv.org/abs/2010.04301\n",
    "        this source code is implemenation of the ExpressiveTacotron from BridgetteSong\n",
    "            - https://github.com/BridgetteSong/ExpressiveTacotron/blob/master/model_duration.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "        super(GaussianUpsampling, self).__init__()\n",
    "        self.mask_score = -1e15\n",
    "\n",
    "    def forward(self, encoder_outputs, durations, vars, input_lengths=None):\n",
    "        \"\"\" Gaussian upsampling\n",
    "        PARAMS\n",
    "        ------\n",
    "        encoder_outputs: Encoder outputs  [B, N, H]\n",
    "        durations: phoneme durations  [B, N]\n",
    "        vars : phoneme attended ranges [B, N]\n",
    "        input_lengths : [B]\n",
    "        RETURNS\n",
    "        -------\n",
    "        encoder_upsampling_outputs: upsampled encoder_output  [B, T, H]\n",
    "        \"\"\"\n",
    "        B = encoder_outputs.size(0)\n",
    "        N = encoder_outputs.size(1)\n",
    "        T = int(torch.sum(durations, dim=1).max().item())\n",
    "        c = torch.cumsum(durations, dim=1).float() - 0.5 * durations\n",
    "        c = c.unsqueeze(2)  # [B, N, 1]\n",
    "        t = (\n",
    "            torch.arange(T, device=encoder_outputs.device).expand(B, N, T).float()\n",
    "        )  # [B, N, T]\n",
    "        vars = vars.view(B, -1, 1)  # [B, N, 1]\n",
    "\n",
    "        w_t = -0.5 * (\n",
    "            np.log(2.0 * np.pi) + torch.log(vars) + torch.pow(t - c, 2) / vars\n",
    "        )  # [B, N, T]\n",
    "\n",
    "        if input_lengths is not None:\n",
    "            input_masks = ~self.get_mask_from_lengths(input_lengths, N)  # [B, N]\n",
    "            input_masks = torch.tensor(input_masks, dtype=torch.bool, device=w_t.device)\n",
    "            masks = input_masks.unsqueeze(2)\n",
    "            w_t.data.masked_fill_(masks, self.mask_score)\n",
    "        w_t = F.softmax(w_t, dim=1)\n",
    "\n",
    "        encoder_upsampling_outputs = torch.bmm(\n",
    "            w_t.transpose(1, 2), encoder_outputs\n",
    "        )  # [B, T, encoder_hidden_size]\n",
    "\n",
    "        return encoder_upsampling_outputs\n",
    "\n",
    "    def get_mask_from_lengths(self, lengths, max_len=None):\n",
    "        if max_len is None:\n",
    "            max_len = max(lengths)\n",
    "        ids = torch.arange(0, max_len, device=lengths.device)\n",
    "        mask = ids < lengths.reshape(-1, 1)\n",
    "        return mask\n",
    "\n",
    "\n",
    "class DurationPredictor(nn.Module):\n",
    "    \"\"\"Duration Predictor module:\n",
    "        - two stack of BiLSTM\n",
    "        - one projection layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "        super(DurationPredictor, self).__init__()\n",
    "        assert (\n",
    "            hparams.duration_lstm_dim % 2 == 0\n",
    "        ), \"duration_lstm_dim must be even [{}]\".format(hparams.duration_lstm_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            hparams.encoder_embedding_dim,\n",
    "            int(hparams.duration_lstm_dim / 2),\n",
    "            2,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "\n",
    "        self.proj = LinearNorm(hparams.duration_lstm_dim, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, encoder_outputs, input_lengths=None):\n",
    "        \"\"\"\n",
    "            :param encoder_outputs: [B, N, encoder_lstm_dim]\n",
    "            :param input_lengths: [B, N]\n",
    "            :return: [B, N]\n",
    "        \"\"\"\n",
    "\n",
    "        B = encoder_outputs.size(0)\n",
    "        N = encoder_outputs.size(1)\n",
    "\n",
    "        ## remove pad activations\n",
    "        if input_lengths is not None:\n",
    "            encoder_outputs = pack_padded_sequence(\n",
    "                encoder_outputs, input_lengths, batch_first=True, enforce_sorted=False\n",
    "            )\n",
    "\n",
    "        self.lstm.flatten_parameters()\n",
    "        outputs, _ = self.lstm(encoder_outputs)\n",
    "\n",
    "        if input_lengths is not None:\n",
    "            outputs, _ = pad_packed_sequence(outputs, batch_first=True)\n",
    "\n",
    "        outputs = self.relu(self.proj(outputs))\n",
    "\n",
    "        return outputs.view(B, N)\n",
    "\n",
    "\n",
    "class RangePredictor(nn.Module):\n",
    "    \"\"\"Duration Predictor module:\n",
    "        - two stack of BiLSTM\n",
    "        - one projection layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "        super(RangePredictor, self).__init__()\n",
    "        assert (\n",
    "            hparams.range_lstm_dim % 2 == 0\n",
    "        ), \"range_lstm_dim must be even [{}]\".format(hparams.range_lstm_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            hparams.encoder_embedding_dim + 1,\n",
    "            int(hparams.range_lstm_dim / 2),\n",
    "            2,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "\n",
    "        self.proj = LinearNorm(hparams.range_lstm_dim, 1)\n",
    "\n",
    "    def forward(self, encoder_outputs, durations, input_lengths=None):\n",
    "        \"\"\"\n",
    "            :param encoder_outputs:\n",
    "            :param durations:\n",
    "            :param input_lengths:\n",
    "            :return:\n",
    "        \"\"\"\n",
    "\n",
    "        concated_inputs = torch.cat([encoder_outputs, durations.unsqueeze(-1)], dim=-1)\n",
    "\n",
    "        ## remove pad activations\n",
    "        if input_lengths is not None:\n",
    "            concated_inputs = pack_padded_sequence(\n",
    "                concated_inputs, input_lengths, batch_first=True, enforce_sorted=False\n",
    "            )\n",
    "\n",
    "        self.lstm.flatten_parameters()\n",
    "        outputs, _ = self.lstm(concated_inputs)\n",
    "\n",
    "        if input_lengths is not None:\n",
    "            outputs, _ = pad_packed_sequence(outputs, batch_first=True)\n",
    "\n",
    "        outputs = self.proj(outputs)\n",
    "        outputs = F.softplus(outputs)\n",
    "        return outputs.squeeze()\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "        sinusoidal positional encoding\n",
    "        https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22698965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "def forward_extractor(tokens, log_probs, blank):\n",
    "    \"\"\"Computes states f and p.\"\"\"\n",
    "    n, m = len(tokens), log_probs.shape[0]\n",
    "    # `f[s, t]` -- max sum of log probs for `s` first codes\n",
    "    # with `t` first timesteps with ending in `tokens[s]`.\n",
    "    f = np.empty((n + 1, m + 1), dtype=float)\n",
    "    f.fill(-(10 ** 9))\n",
    "    p = np.empty((n + 1, m + 1), dtype=int)\n",
    "    f[0, 0] = 0.0  # Start\n",
    "    for s in range(1, n + 1):\n",
    "        c = tokens[s - 1]\n",
    "        for t in range((s + 1) // 2, m + 1):\n",
    "            f[s, t] = log_probs[t - 1, c]\n",
    "            # Option #1: prev char is equal to current one.\n",
    "            if s == 1 or c == blank or c == tokens[s - 3]:\n",
    "                options = f[s : (s - 2 if s > 1 else None) : -1, t - 1]\n",
    "            else:  # Is not equal to current one.\n",
    "                options = f[s : (s - 3 if s > 2 else None) : -1, t - 1]\n",
    "            f[s, t] += np.max(options)\n",
    "            p[s, t] = np.argmax(options)\n",
    "    return f, p\n",
    "\n",
    "\n",
    "def backward_extractor(f, p):\n",
    "    \"\"\"Computes durs from f and p.\"\"\"\n",
    "    n, m = f.shape\n",
    "    n -= 1\n",
    "    m -= 1\n",
    "    durs = np.zeros(n, dtype=int)\n",
    "    if f[-1, -1] >= f[-2, -1]:\n",
    "        s, t = n, m\n",
    "    else:\n",
    "        s, t = n - 1, m\n",
    "    while s > 0:\n",
    "        durs[s - 1] += 1\n",
    "        s -= p[s, t]\n",
    "        t -= 1\n",
    "    assert durs.shape[0] == n\n",
    "    assert np.sum(durs) == m\n",
    "    assert np.all(durs[1::2] > 0)\n",
    "    return durs\n",
    "\n",
    "\n",
    "def preprocess_tokens(tokens, blank):\n",
    "    new_tokens = [blank]\n",
    "    for c in tokens:\n",
    "        new_tokens.extend([c, blank])\n",
    "    tokens = new_tokens\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# data_config = {\n",
    "#     'manifest_filepath': \"allfiles.json\",\n",
    "#     'sample_rate': 22050,\n",
    "#     'labels': asr_model.decoder.vocabulary,\n",
    "#     'batch_size': 1,\n",
    "# }\n",
    "\n",
    "# parser = nemo.collections.asr.data.audio_to_text.AudioToCharWithDursF0Dataset.make_vocab(\n",
    "#     notation='phonemes', punct=True, spaces=True, stresses=False, add_blank_at=\"last\"\n",
    "# )\n",
    "\n",
    "# dataset = nemo.collections.asr.data.audio_to_text._AudioTextDataset(\n",
    "#     manifest_filepath=data_config['manifest_filepath'], sample_rate=data_config['sample_rate'], parser=parser,\n",
    "# )\n",
    "\n",
    "# dl = torch.utils.data.DataLoader(\n",
    "#     dataset=dataset, batch_size=data_config['batch_size'], collate_fn=dataset.collate_fn, shuffle=False,\n",
    "# )\n",
    "\n",
    "# blank_id = asr_model.decoder.num_classes_with_blank - 1\n",
    "\n",
    "# if os.path.exists(os.path.join(output_dir, \"durations.pt\")):\n",
    "#     print(\"durations.pt already exists; skipping\")\n",
    "# else:\n",
    "#     dur_data = {}\n",
    "#     for sample_idx, test_sample in tqdm(enumerate(dl), total=len(dl)):\n",
    "#         log_probs, _, greedy_predictions = asr_model(\n",
    "#             input_signal=test_sample[0], input_signal_length=test_sample[1]\n",
    "#         )\n",
    "\n",
    "#         log_probs = log_probs[0].cpu().detach().numpy()\n",
    "#         seq_ids = test_sample[2][0].cpu().detach().numpy()\n",
    "\n",
    "#         target_tokens = preprocess_tokens(seq_ids, blank_id)\n",
    "\n",
    "#         f, p = forward_extractor(target_tokens, log_probs, blank_id)\n",
    "#         durs = backward_extractor(f, p)\n",
    "\n",
    "#         dur_key = Path(dl.dataset.collection[sample_idx].audio_file).stem\n",
    "#         dur_data[dur_key] = {\n",
    "#             'blanks': torch.tensor(durs[::2], dtype=torch.long).cpu().detach(),\n",
    "#             'tokens': torch.tensor(durs[1::2], dtype=torch.long).cpu().detach()\n",
    "#         }\n",
    "\n",
    "#         del test_sample\n",
    "\n",
    "#     torch.save(dur_data, os.path.join(output_dir, \"durations.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f52ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-01-10 20:00:35 cloud:56] Found existing object /home/s_uberduck_ai/.cache/torch/NeMo/NeMo_1.5.1/qn5x5_libri_tts_phonemes/656c7439dd3a0d614978529371be498b/qn5x5_libri_tts_phonemes.nemo.\n",
      "[NeMo I 2022-01-10 20:00:35 cloud:62] Re-using file from: /home/s_uberduck_ai/.cache/torch/NeMo/NeMo_1.5.1/qn5x5_libri_tts_phonemes/656c7439dd3a0d614978529371be498b/qn5x5_libri_tts_phonemes.nemo\n",
      "[NeMo I 2022-01-10 20:00:35 common:728] Instantiating model from pre-trained checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-01-10 20:00:35 features:243] Using torch_stft is deprecated and will be removed in 1.1.0. Please set stft_conv and stft_exact_pad to False for FilterbankFeatures and AudioToMelSpectrogramPreprocessor. Please set exact_pad to True as needed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-01-10 20:00:35 features:265] PADDING: 1\n",
      "[NeMo I 2022-01-10 20:00:35 features:275] STFT using conv\n",
      "[NeMo I 2022-01-10 20:00:36 save_restore_connector:149] Model EncDecCTCModel was successfully restored from /home/s_uberduck_ai/.cache/torch/NeMo/NeMo_1.5.1/qn5x5_libri_tts_phonemes/656c7439dd3a0d614978529371be498b/qn5x5_libri_tts_phonemes.nemo.\n",
      "[NeMo I 2022-01-10 20:00:36 ctc_models:348] Changed decoder to output to ['_', '-', '!', \"'\", '(', ')', ',', '.', ':', ';', '?', ' ', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '@AA', '@AA0', '@AA1', '@AA2', '@AE', '@AE0', '@AE1', '@AE2', '@AH', '@AH0', '@AH1', '@AH2', '@AO', '@AO0', '@AO1', '@AO2', '@AW', '@AW0', '@AW1', '@AW2', '@AY', '@AY0', '@AY1', '@AY2', '@B', '@CH', '@D', '@DH', '@EH', '@EH0', '@EH1', '@EH2', '@ER', '@ER0', '@ER1', '@ER2', '@EY', '@EY0', '@EY1', '@EY2', '@F', '@G', '@HH', '@IH', '@IH0', '@IH1', '@IH2', '@IY', '@IY0', '@IY1', '@IY2', '@JH', '@K', '@L', '@M', '@N', '@NG', '@OW', '@OW0', '@OW1', '@OW2', '@OY', '@OY0', '@OY1', '@OY2', '@P', '@R', '@S', '@SH', '@T', '@TH', '@UH', '@UH0', '@UH1', '@UH2', '@UW', '@UW0', '@UW1', '@UW2', '@V', '@W', '@Y', '@Z', '@ZH'] vocabulary.\n"
     ]
    }
   ],
   "source": [
    "# asr_model.decoder.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28467903",
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a928f299",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uberduck_ml_dev.text.symbols import SYMBOL_SETS, NVIDIA_TACO2_SYMBOLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04971cbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '-',\n",
       " '!',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " ',',\n",
       " '.',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " ' ',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '@AA',\n",
       " '@AA0',\n",
       " '@AA1',\n",
       " '@AA2',\n",
       " '@AE',\n",
       " '@AE0',\n",
       " '@AE1',\n",
       " '@AE2',\n",
       " '@AH',\n",
       " '@AH0',\n",
       " '@AH1',\n",
       " '@AH2',\n",
       " '@AO',\n",
       " '@AO0',\n",
       " '@AO1',\n",
       " '@AO2',\n",
       " '@AW',\n",
       " '@AW0',\n",
       " '@AW1',\n",
       " '@AW2',\n",
       " '@AY',\n",
       " '@AY0',\n",
       " '@AY1',\n",
       " '@AY2',\n",
       " '@B',\n",
       " '@CH',\n",
       " '@D',\n",
       " '@DH',\n",
       " '@EH',\n",
       " '@EH0',\n",
       " '@EH1',\n",
       " '@EH2',\n",
       " '@ER',\n",
       " '@ER0',\n",
       " '@ER1',\n",
       " '@ER2',\n",
       " '@EY',\n",
       " '@EY0',\n",
       " '@EY1',\n",
       " '@EY2',\n",
       " '@F',\n",
       " '@G',\n",
       " '@HH',\n",
       " '@IH',\n",
       " '@IH0',\n",
       " '@IH1',\n",
       " '@IH2',\n",
       " '@IY',\n",
       " '@IY0',\n",
       " '@IY1',\n",
       " '@IY2',\n",
       " '@JH',\n",
       " '@K',\n",
       " '@L',\n",
       " '@M',\n",
       " '@N',\n",
       " '@NG',\n",
       " '@OW',\n",
       " '@OW0',\n",
       " '@OW1',\n",
       " '@OW2',\n",
       " '@OY',\n",
       " '@OY0',\n",
       " '@OY1',\n",
       " '@OY2',\n",
       " '@P',\n",
       " '@R',\n",
       " '@S',\n",
       " '@SH',\n",
       " '@T',\n",
       " '@TH',\n",
       " '@UH',\n",
       " '@UH0',\n",
       " '@UH1',\n",
       " '@UH2',\n",
       " '@UW',\n",
       " '@UW0',\n",
       " '@UW1',\n",
       " '@UW2',\n",
       " '@V',\n",
       " '@W',\n",
       " '@Y',\n",
       " '@Z',\n",
       " '@ZH']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da02697",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
