{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fd67b94",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Example:-partial-teacher-forcing\" data-toc-modified-id=\"Example:-partial-teacher-forcing-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Example: partial teacher forcing</a></span></li><li><span><a href=\"#Example:-attention-guided-rhythm-transfer\" data-toc-modified-id=\"Example:-attention-guided-rhythm-transfer-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Example: attention-guided rhythm transfer</a></span></li><li><span><a href=\"#Example:-has_speaker_embedding\" data-toc-modified-id=\"Example:-has_speaker_embedding-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Example: has_speaker_embedding</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12e7a06",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fc8726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.tacotron2.tacotron2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df468e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import pdb\n",
    "from torch import nn\n",
    "from uberduck_ml_dev.models.base import TTSModel\n",
    "from uberduck_ml_dev.models.common import Attention, Conv1d, LinearNorm, GST\n",
    "from uberduck_ml_dev.text.symbols import symbols\n",
    "from uberduck_ml_dev.vendor.tfcompat.hparam import HParams\n",
    "from uberduck_ml_dev.utils.utils import to_gpu, get_mask_from_lengths\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.nn import functional as F\n",
    "from uberduck_ml_dev.utils.duration import (\n",
    "    GaussianUpsampling,\n",
    "    RangePredictor,\n",
    "    PositionalEncoding,\n",
    "    DurationPredictor,\n",
    ")\n",
    "from uberduck_ml_dev.models.tacotron2.decoder import Decoder\n",
    "from uberduck_ml_dev.models.tacotron2.encoder import Encoder\n",
    "from uberduck_ml_dev.models.tacotron2.prenet import Prenet\n",
    "from uberduck_ml_dev.models.tacotron2.postnet import Postnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5805fc09",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2967063854.py, line 183)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_6159/2967063854.py\"\u001b[0;36m, line \u001b[0;32m183\u001b[0m\n\u001b[0;31m    output_lengths=output_lengths\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# export\n",
    "from uberduck_ml_dev.data.batch import Batch\n",
    "\n",
    "\n",
    "class Tacotron2(TTSModel):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__(hparams)\n",
    "\n",
    "        self.mask_padding = hparams.mask_padding\n",
    "        self.fp16_run = hparams.fp16_run\n",
    "        self.pos_weight = hparams.pos_weight\n",
    "        self.n_mel_channels = hparams.n_mel_channels\n",
    "        self.n_frames_per_step_initial = hparams.n_frames_per_step_initial\n",
    "        self.n_frames_per_step_current = hparams.n_frames_per_step_initial\n",
    "        self.embedding = nn.Embedding(self.n_symbols, hparams.symbols_embedding_dim)\n",
    "        std = np.sqrt(2.0 / (self.n_symbols + hparams.symbols_embedding_dim))\n",
    "        val = np.sqrt(3.0) * std  # uniform bounds for std\n",
    "        self.embedding.weight.data.uniform_(-val, val)\n",
    "        self.encoder = Encoder(hparams)\n",
    "        self.decoder = Decoder(hparams)\n",
    "        self.postnet = Postnet(hparams)\n",
    "        self.speaker_embedding_dim = hparams.speaker_embedding_dim\n",
    "        self.encoder_embedding_dim = hparams.encoder_embedding_dim\n",
    "        self.has_speaker_embedding = hparams.has_speaker_embedding\n",
    "        self.cudnn_enabled = hparams.cudnn_enabled\n",
    "        self.non_attentive = hparams.non_attentive\n",
    "        self.location_specific_attention = hparams.location_specific_attention\n",
    "        if self.non_attentive:\n",
    "            self.duration_predictor = DurationPredictor(hparams)\n",
    "\n",
    "        if self.n_speakers > 1 and not self.has_speaker_embedding:\n",
    "            raise Exception(\"Speaker embedding is required if n_speakers > 1\")\n",
    "        if hparams.has_speaker_embedding:\n",
    "            self.speaker_embedding = nn.Embedding(\n",
    "                self.n_speakers, hparams.speaker_embedding_dim\n",
    "            )\n",
    "        else:\n",
    "            self.speaker_embedding = None\n",
    "        if self.n_speakers > 1:\n",
    "            self.spkr_lin = nn.Linear(\n",
    "                self.speaker_embedding_dim, self.encoder_embedding_dim\n",
    "            )\n",
    "        else:\n",
    "            self.spkr_lin = lambda a: torch.zeros(\n",
    "                self.encoder_embedding_dim, device=a.device\n",
    "            )\n",
    "\n",
    "        self.gst_init(hparams)\n",
    "\n",
    "    def gst_init(self, hparams):\n",
    "        self.gst_lin = None\n",
    "        self.gst_type = None\n",
    "\n",
    "        if hparams.get(\"gst_type\") == \"torchmoji\":\n",
    "            assert hparams.gst_dim, \"gst_dim must be set\"\n",
    "            self.gst_type = hparams.get(\"gst_type\")\n",
    "            self.gst_lin = nn.Linear(hparams.gst_dim, self.encoder_embedding_dim)\n",
    "            print(\"Initialized Torchmoji GST\")\n",
    "        else:\n",
    "            print(\"Not using any style tokens\")\n",
    "\n",
    "    def parse_batch(self, batch: Batch):\n",
    "\n",
    "        durations_padded = batch.durations_padded\n",
    "        text_int_padded = batch.text_int_padded\n",
    "        input_lengths = batch.input_lengths\n",
    "        mel_padded = batch.mel_padded\n",
    "        gate_padded = batch.gate_padded\n",
    "        output_lengths = batch.output_lengths\n",
    "        speaker_ids = batch.speaker_ids\n",
    "        embedded_gst = batch.gst\n",
    "        f0_padded = batch.f0_padded\n",
    "        if self.cudnn_enabled:\n",
    "            text_int_padded = to_gpu(text_int_padded).long()\n",
    "            input_lengths = to_gpu(input_lengths).long()\n",
    "            mel_padded = to_gpu(mel_padded).float()\n",
    "            gate_padded = to_gpu(gate_padded).float()\n",
    "            speaker_ids = to_gpu(speaker_ids).long()\n",
    "            output_lengths = to_gpu(output_lengths).long()\n",
    "            if durations_padded is not None:\n",
    "                durations_padded = to_gpu(durations_padded).long()\n",
    "            if embedded_gst is not None:\n",
    "                embedded_gst = to_gpu(embedded_gst).float()\n",
    "\n",
    "        # max_len = torch.max(input_lengths.data).item()\n",
    "        ret_x = Batch(\n",
    "            text_int_padded=text_int_padded,\n",
    "            input_lengths=input_lengths,\n",
    "            mel_padded=mel_padded,\n",
    "            gate_padded=gate_padded,\n",
    "            output_lengths=output_lengths,\n",
    "            speaker_ids=speaker_ids,\n",
    "            gst=embedded_gst,\n",
    "            durations_padded=durations_padded,\n",
    "            f0_padded=f0_padded,\n",
    "            # max_len=max_len,\n",
    "        )\n",
    "        ret_y = (mel_padded, gate_padded, durations_padded)\n",
    "        return (\n",
    "            ret_x,\n",
    "            ret_y,\n",
    "        )\n",
    "\n",
    "    def parse_output(self, output_raw, output_lengths=None):\n",
    "        if self.mask_padding and output_lengths is not None:\n",
    "            mask = ~get_mask_from_lengths(output_lengths)\n",
    "            mask = mask.expand(self.n_mel_channels, mask.size(0), mask.size(1))\n",
    "            mask = F.pad(\n",
    "                mask, (0, output_raw[\"predicted_mel_specs\"].size(2) - mask.size(2))\n",
    "            )\n",
    "            mask = mask.permute(1, 0, 2)\n",
    "\n",
    "            output_raw[\"predicted_mel_specs\"].data.masked_fill_(mask, 0.0)\n",
    "            output_raw[\"mel_outputs_postnet\"].data.masked_fill_(mask, 0.0)\n",
    "            if self.location_specific_attention:\n",
    "                output_raw[\"gate_outputs\"].data.masked_fill_(mask[:, 0, :], 1e3)\n",
    "\n",
    "            outputs = output_raw\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_alignment(self, inputs):\n",
    "        (\n",
    "            input_text,\n",
    "            input_lengths,\n",
    "            targets,\n",
    "            max_len,\n",
    "            output_lengths,\n",
    "            speaker_ids,\n",
    "            *_,\n",
    "        ) = inputs\n",
    "\n",
    "        input_lengths, output_lengths = input_lengths.data, output_lengths.data\n",
    "\n",
    "        embedded_inputs = self.embedding(input_text).transpose(1, 2)\n",
    "        embedded_text = self.encoder(embedded_inputs, input_lengths)\n",
    "        encoder_outputs = embedded_text\n",
    "        if self.speaker_embedding:\n",
    "            embedded_speakers = self.speaker_embedding(speaker_ids)[:, None]\n",
    "            encoder_outputs += self.spkr_lin(embedded_speakers)\n",
    "\n",
    "        encoder_outputs = torch.cat((encoder_outputs,), dim=2)\n",
    "\n",
    "        mel_outputs, gate_outputs, alignments = self.decoder(\n",
    "            encoder_outputs,\n",
    "            targets,\n",
    "            input_lengths=input_lengths,\n",
    "            encoder_output_lengths=input_lengths,\n",
    "        )\n",
    "        return alignments\n",
    "\n",
    "    def forward(self, inputs: Batch):\n",
    "        input_text = inputs.text_int_padded\n",
    "        input_lengths = inputs.input_lengths\n",
    "        targets = inputs.mel_padded\n",
    "        output_lengths = inputs.output_lengths\n",
    "        speaker_ids = inputs.speaker_ids\n",
    "        embedded_gst = inputs.gst\n",
    "        durations_padded = inputs.durations_padded\n",
    "        # max_len = inputs.max_len\n",
    "\n",
    "        input_lengths, output_lengths = input_lengths.data, output_lengths.data\n",
    "\n",
    "        embedded_inputs = self.embedding(input_text).transpose(1, 2)\n",
    "        embedded_text = self.encoder(embedded_inputs, input_lengths)\n",
    "        encoder_outputs = embedded_text\n",
    "        if self.speaker_embedding:\n",
    "            embedded_speakers = self.speaker_embedding(speaker_ids)[:, None]\n",
    "            encoder_outputs += self.spkr_lin(embedded_speakers)\n",
    "\n",
    "        if self.gst_lin is not None:\n",
    "            assert (\n",
    "                embedded_gst is not None\n",
    "            ), f\"embedded_gst is None but gst_type was set to {self.gst_type}\"\n",
    "            encoder_outputs += self.gst_lin(embedded_gst)\n",
    "        #         encoder_outputs = torch.cat((encoder_outputs,), dim=2)\n",
    "\n",
    "        if self.location_specific_attention:\n",
    "            # pdb.set_trace()\n",
    "            mel_outputs, gate_outputs, alignments = self.decoder(\n",
    "                encoder_outputs=encoder_outputs,\n",
    "                decoder_inputs=targets,\n",
    "                encoder_output_lengths=input_lengths,\n",
    "                # output_lengths=input_lengths,\n",
    "                output_lengths=output_lengths,\n",
    "            )\n",
    "\n",
    "        if self.non_attentive:\n",
    "            predicted_durations = self.decoder.duration_predictor(\n",
    "                encoder_outputs, input_lengths.cpu()\n",
    "            )\n",
    "            # pdb.set_trace()\n",
    "            mel_outputs, predicted_durations = self.decoder(\n",
    "                encoder_outputs=encoder_outputs,\n",
    "                decoder_inputs=targets,\n",
    "                # output_lengths=input_lengths,\n",
    "                output_lengths=output_lengths,\n",
    "                input_lengths=input_lengths,\n",
    "                durations=durations_padded,\n",
    "            )\n",
    "\n",
    "        # pdb.set_trace()\n",
    "        mel_outputs_postnet = self.postnet(mel_outputs)\n",
    "        mel_outputs_postnet = mel_outputs + mel_outputs_postnet\n",
    "\n",
    "        if self.location_specific_attention:\n",
    "            output_raw = {\n",
    "                \"predicted_mel_specs\": mel_outputs,\n",
    "                \"mel_outputs_postnet\": mel_outputs_postnet,\n",
    "                \"gate_outputs\": gate_outputs,\n",
    "                \"output_lengths\": output_lengths,\n",
    "                \"alignments\": alignments,\n",
    "            }\n",
    "\n",
    "        if self.non_attentive:\n",
    "            output_raw = {\n",
    "                \"predicted_durations\": predicted_durations,\n",
    "                \"predicted_mel_specs\": mel_outputs,\n",
    "                \"mel_outputs_postnet\": mel_outputs_postnet,\n",
    "            }\n",
    "\n",
    "        output = self.parse_output(output_raw, output_lengths)\n",
    "        return output\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def inference(self, inputs):\n",
    "        text, input_lengths, speaker_ids, embedded_gst, *_ = inputs\n",
    "\n",
    "        embedded_inputs = self.embedding(text).transpose(1, 2)\n",
    "        embedded_text = self.encoder.inference(embedded_inputs, input_lengths)\n",
    "        encoder_outputs = embedded_text\n",
    "        if self.speaker_embedding:\n",
    "            embedded_speakers = self.speaker_embedding(speaker_ids)[:, None]\n",
    "            encoder_outputs += self.spkr_lin(embedded_speakers)\n",
    "\n",
    "        if self.gst_lin is not None:\n",
    "            assert (\n",
    "                embedded_gst is not None\n",
    "            ), f\"embedded_gst is None but gst_type was set to {self.gst_type}\"\n",
    "            encoder_outputs += self.gst_lin(embedded_gst)\n",
    "        #         encoder_outputs = torch.cat((encoder_outputs,), dim=2)\n",
    "\n",
    "        mel_outputs, gate_outputs, alignments, mel_lengths = self.decoder.inference(\n",
    "            encoder_outputs, input_lengths\n",
    "        )\n",
    "        mel_outputs_postnet = self.postnet(mel_outputs)\n",
    "        mel_outputs_postnet = mel_outputs + mel_outputs_postnet\n",
    "\n",
    "        return self.parse_output(\n",
    "            [mel_outputs, mel_outputs_postnet, gate_outputs, alignments, mel_lengths]\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def inference_noattention(self, inputs):\n",
    "        \"\"\"Run inference conditioned on an attention map.\"\"\"\n",
    "        text, input_lengths, speaker_ids, attention_maps = inputs\n",
    "        embedded_inputs = self.embedding(text).transpose(1, 2)\n",
    "        embedded_text = self.encoder.inference(embedded_inputs, input_lengths)\n",
    "\n",
    "        encoder_outputs = torch.cat((embedded_text,), dim=2)\n",
    "\n",
    "        mel_outputs, gate_outputs, alignments = self.decoder.inference_noattention(\n",
    "            encoder_outputs, attention_maps\n",
    "        )\n",
    "        mel_outputs_postnet = self.postnet(mel_outputs)\n",
    "        mel_outputs_postnet = mel_outputs + mel_outputs_postnet\n",
    "\n",
    "        return self.parse_output(\n",
    "            [mel_outputs, mel_outputs_postnet, gate_outputs, alignments]\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def inference_partial_tf(\n",
    "        self, inputs, tf_mel, tf_until_idx,\n",
    "    ):\n",
    "        \"\"\"Run inference with partial teacher forcing.\n",
    "\n",
    "        Teacher forcing is done until tf_until_idx in the mel spectrogram.\n",
    "        Make sure you pass the mel index and not the text index!\n",
    "\n",
    "        tf_mel: (B, T, n_mel_channels)\n",
    "        \"\"\"\n",
    "        text, input_lengths, *_ = inputs\n",
    "        embedded_inputs = self.embedding(text).transpose(1, 2)\n",
    "        embedded_text = self.encoder.inference(embedded_inputs, input_lengths)\n",
    "        encoder_outputs = torch.cat((embedded_text,), dim=2)\n",
    "\n",
    "        mel_outputs, gate_outputs, alignments = self.decoder.inference_partial_tf(\n",
    "            encoder_outputs, tf_mel, tf_until_idx,\n",
    "        )\n",
    "\n",
    "        mel_outputs_postnet = self.postnet(mel_outputs)\n",
    "        mel_outputs_postnet = mel_outputs + mel_outputs_postnet\n",
    "\n",
    "        return self.parse_output(\n",
    "            [mel_outputs, mel_outputs_postnet, gate_outputs, alignments]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177770ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from uberduck_ml_dev.vendor.tfcompat.hparam import HParams\n",
    "from uberduck_ml_dev.models.base import DEFAULTS as MODEL_DEFAULTS\n",
    "\n",
    "DEFAULTS = HParams(\n",
    "    symbols_embedding_dim=512,\n",
    "    fp16_run=False,\n",
    "    mask_padding=True,\n",
    "    n_mel_channels=80,\n",
    "    # encoder parameters\n",
    "    encoder_kernel_size=5,\n",
    "    encoder_n_convolutions=3,\n",
    "    encoder_embedding_dim=512,\n",
    "    # decoder parameters\n",
    "    coarse_n_frames_per_step=None,\n",
    "    decoder_rnn_dim=1024,\n",
    "    prenet_dim=256,\n",
    "    prenet_f0_n_layers=1,\n",
    "    prenet_f0_dim=1,\n",
    "    prenet_f0_kernel_size=1,\n",
    "    prenet_rms_dim=0,\n",
    "    prenet_fms_kernel_size=1,\n",
    "    max_decoder_steps=1000,\n",
    "    gate_threshold=0.5,\n",
    "    p_attention_dropout=0.1,\n",
    "    p_decoder_dropout=0.1,\n",
    "    p_teacher_forcing=1.0,\n",
    "    pos_weight=None,\n",
    "    # attention parameters\n",
    "    attention_rnn_dim=1024,\n",
    "    attention_dim=128,\n",
    "    # location layer parameters\n",
    "    attention_location_n_filters=32,\n",
    "    attention_location_kernel_size=31,\n",
    "    # mel post-processing network parameters\n",
    "    postnet_embedding_dim=512,\n",
    "    postnet_kernel_size=5,\n",
    "    postnet_n_convolutions=5,\n",
    "    n_speakers=1,\n",
    "    speaker_embedding_dim=128,\n",
    "    # reference encoder\n",
    "    with_gst=False,\n",
    "    ref_enc_filters=[32, 32, 64, 64, 128, 128],\n",
    "    ref_enc_size=[3, 3],\n",
    "    ref_enc_strides=[2, 2],\n",
    "    ref_enc_pad=[1, 1],\n",
    "    filter_length=1024,\n",
    "    hop_length=256,\n",
    "    include_f0=False,\n",
    "    ref_enc_gru_size=128,\n",
    "    symbol_set=\"nvidia_taco2\",\n",
    "    num_heads=8,\n",
    "    text_cleaners=[\"english_cleaners\"],\n",
    "    sampling_rate=22050,\n",
    "    checkpoint_name=None,\n",
    "    max_wav_value=32768.0,\n",
    "    mel_fmax=8000,\n",
    "    mel_fmin=0,\n",
    "    n_frames_per_step_initial=1,\n",
    "    win_length=1024,\n",
    "    has_speaker_embedding=False,\n",
    "    gst_type=None,\n",
    "    torchmoji_model_file=None,\n",
    "    torchmoji_vocabulary_file=None,\n",
    "    sample_inference_speaker_ids=None,\n",
    "    sample_inference_text=\"That quick beige fox jumped in the air loudly over the thin dog fence.\",\n",
    "    distributed_run=False,\n",
    "    cudnn_enabled=False,\n",
    "    # compute_durations=False,\n",
    "    non_attentive=False,\n",
    "    location_specific_attention=True,\n",
    "    include_durations=False,\n",
    "    compute_durations=False,\n",
    ")\n",
    "\n",
    "config = DEFAULTS.values()\n",
    "config.update(MODEL_DEFAULTS.values())\n",
    "DEFAULTS = HParams(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9eca11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from uberduck_ml_dev.vendor.tfcompat.hparam import HParams\n",
    "from uberduck_ml_dev.models.base import DEFAULTS as MODEL_DEFAULTS\n",
    "\n",
    "config = DEFAULTS.values()\n",
    "config.update(\n",
    "    dict(\n",
    "        # compute_durations=True,\n",
    "        non_attentive=True,\n",
    "        positional_embedding_dim=32,\n",
    "        range_lstm_dim=1024,\n",
    "        duration_lstm_dim=1024,\n",
    "        location_specific_attention=False,\n",
    "        cudnn_enabled=True,\n",
    "        include_durations=True,\n",
    "        compute_durations=True,\n",
    "    )\n",
    ")\n",
    "config.update(MODEL_DEFAULTS.values())\n",
    "NON_ATTENTIVE_DEFAULTS = HParams(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98ba935",
   "metadata": {},
   "outputs": [],
   "source": [
    "NON_ATTENTIVE_DEFAULTS.include_durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e53499c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uberduck_ml_dev.trainer.tacotron2 import Tacotron2Trainer\n",
    "import json\n",
    "from uberduck_ml_dev.vendor.tfcompat.hparam import HParams\n",
    "\n",
    "config = NON_ATTENTIVE_DEFAULTS.values()\n",
    "with open(\"test/fixtures/ljtest/taco2_lj2lj.json\") as f:\n",
    "    config.update(json.load(f))\n",
    "hparams = HParams(**config)\n",
    "hparams.speaker_embedding_dim = 1\n",
    "hparams.decoder_rnn_dim_nlayers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44fddde",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams.positional_embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1922130d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Tacotron2(hparams)\n",
    "if torch.cuda.is_available() and hparams.cudnn_enabled:\n",
    "    model.cuda()\n",
    "trainer = Tacotron2Trainer(hparams, rank=0, world_size=0)\n",
    "train_set, val_set, train_loader, sampler, collate_fn = trainer.initialize_loader(\n",
    "    include_durations=hparams.include_durations\n",
    ")\n",
    "batch = next(enumerate(train_loader))[1]\n",
    "X, y = model.parse_batch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08d0fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_output = model(X)\n",
    "# assert len(forward_output) == 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b93cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uberduck_ml_dev.trainer.tacotron2 import Tacotron2Trainer\n",
    "import json\n",
    "from uberduck_ml_dev.vendor.tfcompat.hparam import HParams\n",
    "\n",
    "config = DEFAULTS.values()\n",
    "with open(\"test/fixtures/ljtest/taco2_lj2lj.json\") as f:\n",
    "    config.update(json.load(f))\n",
    "hparams = HParams(**config)\n",
    "hparams.speaker_embedding_dim = 1\n",
    "model = Tacotron2(hparams)\n",
    "if torch.cuda.is_available() and hparams.cudnn_enabled:\n",
    "    model.cuda()\n",
    "trainer = Tacotron2Trainer(hparams, rank=0, world_size=0)\n",
    "train_set, val_set, train_loader, sampler, collate_fn = trainer.initialize_loader()\n",
    "batch = next(enumerate(train_loader))[1]\n",
    "\n",
    "X, y = model.parse_batch(batch)\n",
    "forward_output = model(X)\n",
    "assert len(forward_output) == 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c29d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(enumerate(train_loader))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312ef875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip\n",
    "from uberduck_ml_dev.data_loader import MelSTFT\n",
    "from uberduck_ml_dev.text.symbols import NVIDIA_TACO2_SYMBOLS\n",
    "from uberduck_ml_dev.text.util import text_to_sequence\n",
    "from uberduck_ml_dev.utils.plot import plot_attention, plot_attention_phonemes\n",
    "from uberduck_ml_dev.vocoders.hifigan import HiFiGanGenerator\n",
    "\n",
    "from IPython.display import display, Audio\n",
    "\n",
    "model = Tacotron2(DEFAULTS)\n",
    "loaded = torch.load(\"../models/tacotron2-eminem-arpabet-400-2021-12-14.pt\")\n",
    "model.load_state_dict(loaded)\n",
    "hg = HiFiGanGenerator(\"../models/config_v1.json\", \"../models/g_02590000_8spk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3629122",
   "metadata": {},
   "source": [
    "## Example: partial teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46730c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip\n",
    "%matplotlib inline\n",
    "model.eval()\n",
    "sequence = torch.LongTensor(\n",
    "    text_to_sequence(\n",
    "        \"Let's hope this is a high variance utterance.\",\n",
    "        [\"english_cleaners\"],\n",
    "        p_arpabet=1,\n",
    "        symbol_set=NVIDIA_TACO2_SYMBOLS,\n",
    "    )\n",
    ")\n",
    "mel_out, mel_out_postnet, gate_out, attn, *_ = model.inference(\n",
    "    (sequence[None], torch.LongTensor([len(sequence)]), [0])\n",
    ")\n",
    "audio = hg.infer(mel_out_postnet)\n",
    "display(Audio(audio, rate=22050))\n",
    "plot_attention_phonemes(\n",
    "    sequence, attn[0].transpose(0, 1), symbol_set=NVIDIA_TACO2_SYMBOLS\n",
    ")\n",
    "new_sequence = torch.LongTensor(\n",
    "    text_to_sequence(\n",
    "        \"Let's hope this is a highlight of your life.\",\n",
    "        [\"english_cleaners\"],\n",
    "        p_arpabet=1,\n",
    "        symbol_set=NVIDIA_TACO2_SYMBOLS,\n",
    "    )\n",
    ")\n",
    "_mel_out, _mel_out_postnet, _gate_out, _attn = model.inference_partial_tf(\n",
    "    (new_sequence[None], torch.LongTensor([len(new_sequence)])), mel_out_postnet, 90,\n",
    ")\n",
    "audio = hg.infer(_mel_out_postnet)\n",
    "display(Audio(audio, rate=22050))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a95e52",
   "metadata": {},
   "source": [
    "## Example: attention-guided rhythm transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0c1ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip\n",
    "model.eval()\n",
    "sequence = torch.LongTensor(\n",
    "    text_to_sequence(\n",
    "        \"Let's hope this is a high variance utterance.\",\n",
    "        [\"english_cleaners\"],\n",
    "        p_arpabet=1,\n",
    "        symbol_set=NVIDIA_TACO2_SYMBOLS,\n",
    "    )\n",
    ")\n",
    "mel_out, mel_out_postnet, gate_out, prev_attn, *_ = model.inference(\n",
    "    (sequence[None], torch.LongTensor([len(sequence)]), [0])\n",
    ")\n",
    "shortened_attn = torch.empty(\n",
    "    prev_attn.shape[1] // 2, prev_attn.shape[0], prev_attn.shape[2]\n",
    ")\n",
    "shortened_attn.shape\n",
    "for idx in range(len(shortened_attn)):\n",
    "    shortened_attn[idx, :, :] = (\n",
    "        prev_attn[:, 2 * idx, :] + prev_attn[:, 2 * idx + 1, :]\n",
    "    ) / 2\n",
    "\n",
    "plot_attention(shortened_attn.transpose(0, 1)[0])\n",
    "\n",
    "transcription = \"Well you know as you know the web's a pretty miraculous thing and it was a very simple paradigm that was invented which was.\"\n",
    "mel = torch.load(\"./test/fixtures/stevejobs-1.pt\")\n",
    "\n",
    "input_text = text_to_sequence(\n",
    "    transcription,\n",
    "    p_arpabet=1,\n",
    "    symbol_set=NVIDIA_TACO2_SYMBOLS,\n",
    "    cleaner_names=[\"english_cleaners\"],\n",
    ")\n",
    "input_lengths = torch.LongTensor([len(input_text)])\n",
    "input_text = torch.LongTensor(input_text)[None]\n",
    "print(input_text.shape)\n",
    "targets = mel[None]\n",
    "print(targets.shape)\n",
    "max_len = targets.size(2)\n",
    "output_lengths = torch.LongTensor([targets.size(2)])\n",
    "speaker_ids = torch.LongTensor([0])\n",
    "input_ = (input_text, input_lengths, targets, max_len, output_lengths, speaker_ids)\n",
    "\n",
    "model_out = model.forward(input_)\n",
    "print(len(model_out))\n",
    "mel_out, mel_out_postnet, gate_out, attn = model_out\n",
    "print(input_text.shape, input_lengths.shape, speaker_ids.shape, attn.shape)\n",
    "\n",
    "plot_attention_phonemes(input_text[0], attn[0].transpose(0, 1), NVIDIA_TACO2_SYMBOLS)\n",
    "\n",
    "mel, mel_postnet, gate, attn = model.inference_noattention(\n",
    "    [input_text, input_lengths, speaker_ids, attn.transpose(0, 1)]\n",
    ")\n",
    "\n",
    "input_text.shape, attn.shape\n",
    "\n",
    "audio = hg.infer(mel_postnet)\n",
    "display(Audio(audio, rate=22050))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f157e0",
   "metadata": {},
   "source": [
    "## Example: has_speaker_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2e6a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip\n",
    "import IPython.display as ipd\n",
    "\n",
    "from uberduck_ml_dev.text.symbols import NVIDIA_TACO2_SYMBOLS\n",
    "from uberduck_ml_dev.text.util import text_to_sequence\n",
    "from uberduck_ml_dev.utils.audio import mel_to_audio\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "tt = Tacotron2(DEFAULTS)\n",
    "t1_count = count_parameters(tt)\n",
    "config = dict(DEFAULTS.values())\n",
    "config[\"has_speaker_embedding\"] = True\n",
    "tt2 = Tacotron2(HParams(**config))\n",
    "t2_count = count_parameters(tt2)\n",
    "assert t1_count < t2_count\n",
    "tt2.from_pretrained(\n",
    "    model_dict=torch.load(\"../models/tacotron2-eminem-arpabet-400-2021-12-14.pt\")\n",
    ")\n",
    "tt.from_pretrained(\n",
    "    model_dict=torch.load(\"../models/tacotron2-eminem-arpabet-400-2021-12-14.pt\")\n",
    ")\n",
    "seq = text_to_sequence(\n",
    "    \"The quick brown fox jumped over the lazy dog.\",\n",
    "    [\"english_cleaners\"],\n",
    "    p_arpabet=1.0,\n",
    "    symbol_set=NVIDIA_TACO2_SYMBOLS,\n",
    ")\n",
    "seq = torch.IntTensor(seq)[None]\n",
    "print(seq.shape)\n",
    "mel, mel_postnet, _, _, _ = tt.inference(\n",
    "    (seq, torch.LongTensor([seq.size(1)]), torch.LongTensor([0]))\n",
    ")\n",
    "audio = mel_to_audio(mel_postnet[0])\n",
    "\n",
    "ipd.display(ipd.Audio(audio.data.numpy(), rate=22050))\n",
    "mel, mel_postnet, *_ = tt2.inference(\n",
    "    (seq, torch.LongTensor([seq.size(1)]), torch.LongTensor([0]))\n",
    ")\n",
    "audio = mel_to_audio(mel_postnet[0])\n",
    "ipd.display(ipd.Audio(audio.data.numpy(), rate=22050))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
