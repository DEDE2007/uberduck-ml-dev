{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d0c3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.tacotron2.prenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2f93b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "import pdb\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.nn import functional as F\n",
    "from uberduck_ml_dev.models.common import LinearNorm\n",
    "import math\n",
    "\n",
    "\n",
    "class Dummy(nn.Module):\n",
    "    \"\"\"\n",
    "        Dummy function for None activation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Dummy, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "## Activation Function used in Other Module\n",
    "ACTIVATION_GROUP = {\"linear\": Dummy, \"tanh\": nn.Tanh, \"relu\": nn.ReLU}\n",
    "\n",
    "# class Prenet(nn.Module):\n",
    "#     def __init__(self, in_dim, sizes):\n",
    "#         super().__init__()\n",
    "#         in_sizes = [in_dim] + sizes[:-1]\n",
    "#         self.layers = nn.ModuleList(\n",
    "#             [\n",
    "#                 LinearNorm(in_size, out_size, bias=False)\n",
    "#                 for (in_size, out_size) in zip(in_sizes, sizes)\n",
    "#             ]\n",
    "#         )\n",
    "#         self.dropout_rate = 0.5\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         for linear in self.layers:\n",
    "#             x = F.dropout(F.relu(linear(x)), p=self.dropout_rate, training=True)\n",
    "#         return x\n",
    "\n",
    "# NOTE (Sam): non-attentive taco version\n",
    "class Prenet(nn.Module):\n",
    "    def __init__(self, in_dim, sizes, dropout_p=0.5, activation=\"relu\"):\n",
    "        super(Prenet, self).__init__()\n",
    "\n",
    "        assert (\n",
    "            activation in ACTIVATION_GROUP\n",
    "        ), \"activation must either one of them [{}]\".format(\n",
    "            \", \".join(ACTIVATION_GROUP.keys())\n",
    "        )\n",
    "\n",
    "        in_sizes = [in_dim] + sizes[:-1]\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                LinearNorm(in_size, out_size, bias=False)\n",
    "                for (in_size, out_size) in zip(in_sizes, sizes)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "        self.activation = ACTIVATION_GROUP[activation]()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for linear in self.layers:\n",
    "            x = self.dropout(self.activation(linear(x)))\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
