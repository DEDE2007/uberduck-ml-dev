{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "936d9e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.tacotron2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34ce9f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from torch import nn\n",
    "from uberduck_ml_dev.models.base import TTSModel\n",
    "from uberduck_ml_dev.models.common import Attention, Conv1d, LinearNorm, GST\n",
    "from uberduck_ml_dev.text.symbols import symbols\n",
    "from uberduck_ml_dev.vendor.tfcompat.hparam import HParams\n",
    "from uberduck_ml_dev.utils.utils import to_gpu, get_mask_from_lengths\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# mix\n",
    "# import pdb\n",
    "\n",
    "# class Decoder(nn.Module):\n",
    "#     def __init__(self, hparams):\n",
    "#         super().__init__()\n",
    "#         self.n_mel_channels = hparams.n_mel_channels\n",
    "#         self.n_frames_per_step_initial = hparams.n_frames_per_step_initial\n",
    "#         self.n_frames_per_step_current = hparams.n_frames_per_step_initial\n",
    "#         self.encoder_embedding_dim = hparams.encoder_embedding_dim\n",
    "#         self.attention_rnn_dim = hparams.attention_rnn_dim\n",
    "#         self.decoder_rnn_dim = hparams.decoder_rnn_dim\n",
    "#         self.prenet_dim = hparams.prenet_dim\n",
    "#         self.max_decoder_steps = hparams.max_decoder_steps\n",
    "#         self.gate_threshold = hparams.gate_threshold\n",
    "#         self.p_attention_dropout = hparams.p_attention_dropout\n",
    "#         self.p_decoder_dropout = hparams.p_decoder_dropout\n",
    "#         self.p_teacher_forcing = hparams.p_teacher_forcing\n",
    "\n",
    "#         self.prenet = Prenet(\n",
    "#             hparams.n_mel_channels,\n",
    "#             [hparams.prenet_dim, hparams.prenet_dim],\n",
    "#         )\n",
    "\n",
    "#         self.attention_rnn = nn.LSTMCell(\n",
    "#             hparams.prenet_dim + self.encoder_embedding_dim,\n",
    "#             hparams.attention_rnn_dim,\n",
    "#         )\n",
    "\n",
    "#         self.attention_layer = Attention(\n",
    "#             hparams.attention_rnn_dim,\n",
    "#             self.encoder_embedding_dim,\n",
    "#             hparams.attention_dim,\n",
    "#             hparams.attention_location_n_filters,\n",
    "#             hparams.attention_location_kernel_size,\n",
    "#             fp16_run=hparams.fp16_run,\n",
    "#         )\n",
    "\n",
    "#         self.decoder_rnn = nn.LSTMCell(\n",
    "#             hparams.attention_rnn_dim + self.encoder_embedding_dim,\n",
    "#             hparams.decoder_rnn_dim,\n",
    "#             1,\n",
    "#         )\n",
    "\n",
    "#         self.linear_projection = LinearNorm(\n",
    "#             hparams.decoder_rnn_dim + self.encoder_embedding_dim,\n",
    "#             hparams.n_mel_channels * hparams.n_frames_per_step_initial,\n",
    "#         )\n",
    "\n",
    "#         self.gate_layer = LinearNorm(\n",
    "#             hparams.decoder_rnn_dim + self.encoder_embedding_dim,\n",
    "#             1,\n",
    "#             bias=True,\n",
    "#             w_init_gain=\"sigmoid\",\n",
    "#         )\n",
    "\n",
    "#     def set_current_frames_per_step(self, n_frames: int):\n",
    "#         self.n_frames_per_step_current = n_frames\n",
    "\n",
    "#     def get_go_frame(self, memory):\n",
    "#         \"\"\"Gets all zeros frames to use as first decoder input\n",
    "#         PARAMS\n",
    "#         ------\n",
    "#         memory: decoder outputs\n",
    "\n",
    "#         RETURNS\n",
    "#         -------\n",
    "#         decoder_input: all zeros frames\n",
    "#         \"\"\"\n",
    "#         B = memory.size(0)\n",
    "#         decoder_input = Variable(memory.data.new(B, self.n_mel_channels).zero_())\n",
    "#         return decoder_input\n",
    "\n",
    "#     def initialize_decoder_states(self, memory, mask):\n",
    "#         \"\"\"Initializes attention rnn states, decoder rnn states, attention\n",
    "#         weights, attention cumulative weights, attention context, stores memory\n",
    "#         and stores processed memory\n",
    "#         PARAMS\n",
    "#         ------\n",
    "#         memory: Encoder outputs\n",
    "#         mask: Mask for padded data if training, expects None for inference\n",
    "#         \"\"\"\n",
    "#         B = memory.size(0)\n",
    "#         MAX_TIME = memory.size(1)\n",
    "\n",
    "#         self.attention_hidden = Variable(\n",
    "#             memory.data.new(B, self.attention_rnn_dim).zero_()\n",
    "#         )\n",
    "#         self.attention_cell = Variable(\n",
    "#             memory.data.new(B, self.attention_rnn_dim).zero_()\n",
    "#         )\n",
    "\n",
    "#         self.decoder_hidden = Variable(memory.data.new(B, self.decoder_rnn_dim).zero_())\n",
    "#         self.decoder_cell = Variable(memory.data.new(B, self.decoder_rnn_dim).zero_())\n",
    "\n",
    "#         self.attention_weights = Variable(memory.data.new(B, MAX_TIME).zero_())\n",
    "#         self.attention_weights_cum = Variable(memory.data.new(B, MAX_TIME).zero_())\n",
    "#         self.attention_context = Variable(\n",
    "#             memory.data.new(B, self.encoder_embedding_dim).zero_()\n",
    "#         )\n",
    "\n",
    "#         self.memory = memory\n",
    "#         self.processed_memory = self.attention_layer.memory_layer(memory)\n",
    "#         self.mask = mask\n",
    "# #     def initialize_decoder_states(self, memory):\n",
    "# #         \"\"\"Initializes attention rnn states, decoder rnn states, attention\n",
    "# #         weights, attention cumulative weights, attention context, stores memory\n",
    "# #         and stores processed memory\n",
    "# #         PARAMS\n",
    "# #         ------\n",
    "# #         memory: Encoder outputs\n",
    "# #         mask: Mask for padded data if training, expects None for inference\n",
    "# #         \"\"\"\n",
    "# #         B = memory.size(0)\n",
    "# #         MAX_TIME = memory.size(1)\n",
    "# #         dtype = memory.dtype\n",
    "# #         device = memory.device\n",
    "\n",
    "# #         attention_hidden = torch.zeros(\n",
    "# #             B, self.attention_rnn_dim, dtype=dtype, device=device\n",
    "# #         )\n",
    "# #         attention_cell = torch.zeros(\n",
    "# #             B, self.attention_rnn_dim, dtype=dtype, device=device\n",
    "# #         )\n",
    "\n",
    "# #         decoder_hidden = torch.zeros(\n",
    "# #             B, self.decoder_rnn_dim, dtype=dtype, device=device\n",
    "# #         )\n",
    "# #         decoder_cell = torch.zeros(B, self.decoder_rnn_dim, dtype=dtype, device=device)\n",
    "\n",
    "# #         attention_weights = torch.zeros(B, MAX_TIME, dtype=dtype, device=device)\n",
    "# #         attention_weights_cum = torch.zeros(B, MAX_TIME, dtype=dtype, device=device)\n",
    "# #         attention_context = torch.zeros(\n",
    "# #             B, self.encoder_embedding_dim, dtype=dtype, device=device\n",
    "# #         )\n",
    "\n",
    "# #         processed_memory = self.attention_layer.memory_layer(memory)\n",
    "\n",
    "# #         return (\n",
    "# #             attention_hidden,\n",
    "# #             attention_cell,\n",
    "# #             decoder_hidden,\n",
    "# #             decoder_cell,\n",
    "# #             attention_weights,\n",
    "# #             attention_weights_cum,\n",
    "# #             attention_context,\n",
    "# #             processed_memory,\n",
    "# #         )\n",
    "\n",
    "#     def parse_decoder_inputs(self, decoder_inputs):\n",
    "#         \"\"\"Prepares decoder inputs, i.e. mel outputs\n",
    "#         PARAMS\n",
    "#         ------\n",
    "#         decoder_inputs: inputs used for teacher-forced training, i.e. mel-specs\n",
    "\n",
    "#         RETURNS\n",
    "#         -------\n",
    "#         inputs: processed decoder inputs\n",
    "\n",
    "#         \"\"\"\n",
    "#         # (B, n_mel_channels, T_out) -> (B, T_out, n_mel_channels)\n",
    "\n",
    "#         decoder_inputs = decoder_inputs.transpose(1, 2)\n",
    "#         decoder_inputs = decoder_inputs.contiguous()\n",
    "#         # print(decoder_inputs.shape, 'dec_in')\n",
    "#         # print(decoder_inputs.size(0), int(decoder_inputs.size(1)), self.n_frames_per_step_current)\n",
    "#         decoder_inputs = decoder_inputs.view(\n",
    "#             decoder_inputs.size(0),\n",
    "#             int(decoder_inputs.size(1) / self.n_frames_per_step_current),\n",
    "#             -1,\n",
    "#         )\n",
    "#         # (B, T_out, n_mel_channels) -> (T_out, B, n_mel_channels)\n",
    "#         decoder_inputs = decoder_inputs.transpose(0, 1)\n",
    "#         return decoder_inputs\n",
    "\n",
    "#     def parse_decoder_outputs(self, mel_outputs, gate_outputs, alignments):\n",
    "#         \"\"\"Prepares decoder outputs for output\n",
    "#         PARAMS\n",
    "#         ------\n",
    "#         mel_outputs:\n",
    "#         gate_outputs: gate output energies\n",
    "#         alignments:\n",
    "\n",
    "#         RETURNS\n",
    "#         -------\n",
    "#         mel_outputs:\n",
    "#         gate_outpust: gate output energies\n",
    "#         alignments:\n",
    "#         \"\"\"\n",
    "#         # (T_out, B) -> (B, T_out)\n",
    "#         alignments = torch.stack(alignments).transpose(0, 1)\n",
    "#         # (T_out, B) -> (B, T_out)\n",
    "#         gate_outputs = torch.stack(gate_outputs)\n",
    "#         if len(gate_outputs.size()) > 1:\n",
    "#             gate_outputs = gate_outputs.transpose(0, 1)\n",
    "#         else:\n",
    "#             gate_outputs = gate_outputs[None]\n",
    "#         gate_outputs = gate_outputs.contiguous()\n",
    "#         # (B, T_out, n_mel_channels * n_frames_per_step) -> (B, T_out * n_frames_per_step, n_mel_channels)\n",
    "#         mel_outputs = mel_outputs.view(mel_outputs.size(0), -1, self.n_mel_channels)\n",
    "#         # (B, T_out, n_mel_channels) -> (B, n_mel_channels, T_out)\n",
    "#         mel_outputs = mel_outputs.transpose(1, 2)\n",
    "\n",
    "#         return mel_outputs, gate_outputs, alignments\n",
    "\n",
    "#     def decode(self, decoder_input, attention_weights=None):\n",
    "#         \"\"\"Decoder step using stored states, attention and memory\n",
    "#         PARAMS\n",
    "#         ------\n",
    "#         decoder_input: previous mel output\n",
    "\n",
    "#         RETURNS\n",
    "#         -------\n",
    "#         mel_output:\n",
    "#         gate_output: gate output energies\n",
    "#         attention_weights:\n",
    "#         \"\"\"\n",
    "#         cell_input = torch.cat((decoder_input, self.attention_context), -1)\n",
    "#         self.attention_hidden, self.attention_cell = self.attention_rnn(\n",
    "#             cell_input, (self.attention_hidden, self.attention_cell)\n",
    "#         )\n",
    "#         self.attention_hidden = F.dropout(\n",
    "#             self.attention_hidden, self.p_attention_dropout, self.training\n",
    "#         )\n",
    "#         self.attention_cell = F.dropout(\n",
    "#             self.attention_cell, self.p_attention_dropout, self.training\n",
    "#         )\n",
    "\n",
    "#         attention_weights_cat = torch.cat(\n",
    "#             (\n",
    "#                 self.attention_weights.unsqueeze(1),\n",
    "#                 self.attention_weights_cum.unsqueeze(1),\n",
    "#             ),\n",
    "#             dim=1,\n",
    "#         )\n",
    "#         self.attention_context, self.attention_weights = self.attention_layer(\n",
    "#             self.attention_hidden,\n",
    "#             self.memory,\n",
    "#             self.processed_memory,\n",
    "#             attention_weights_cat,\n",
    "#             self.mask,\n",
    "#             attention_weights,\n",
    "#         )\n",
    "\n",
    "#         self.attention_weights_cum += self.attention_weights\n",
    "#         decoder_input = torch.cat((self.attention_hidden, self.attention_context), -1)\n",
    "#         self.decoder_hidden, self.decoder_cell = self.decoder_rnn(\n",
    "#             decoder_input, (self.decoder_hidden, self.decoder_cell)\n",
    "#         )\n",
    "#         self.decoder_hidden = F.dropout(\n",
    "#             self.decoder_hidden, self.p_decoder_dropout, self.training\n",
    "#         )\n",
    "#         self.decoder_cell = F.dropout(\n",
    "#             self.decoder_cell, self.p_decoder_dropout, self.training\n",
    "#         )\n",
    "\n",
    "#         decoder_hidden_attention_context = torch.cat(\n",
    "#             (self.decoder_hidden, self.attention_context), dim=1\n",
    "#         )\n",
    "\n",
    "#         decoder_output = self.linear_projection(decoder_hidden_attention_context)\n",
    "\n",
    "#         gate_prediction = self.gate_layer(decoder_hidden_attention_context)\n",
    "#         return decoder_output, gate_prediction, self.attention_weights\n",
    "\n",
    "#     def forward(self, memory, decoder_inputs, memory_lengths):\n",
    "#         \"\"\"Decoder forward pass for training\n",
    "#         PARAMS\n",
    "#         ------\n",
    "#         memory: Encoder outputs\n",
    "#         decoder_inputs: Decoder inputs for teacher forcing. i.e. mel-specs\n",
    "#         memory_lengths: Encoder output lengths for attention masking.\n",
    "\n",
    "#         RETURNS\n",
    "#         -------\n",
    "#         mel_outputs: mel outputs from the decoder\n",
    "#         gate_outputs: gate outputs from the decoder\n",
    "#         alignments: sequence of attention weights from the decoder\n",
    "#         \"\"\"\n",
    "#         B = memory.size(0)\n",
    "#         decoder_inputs = self.parse_decoder_inputs(decoder_inputs)\n",
    "#         decoder_inputs = decoder_inputs.reshape(\n",
    "#             -1, decoder_inputs.size(1), self.n_mel_channels\n",
    "#         )\n",
    "#         decoder_input = self.get_go_frame(memory).unsqueeze(0)\n",
    "#         decoder_inputs = torch.cat((decoder_input, decoder_inputs), dim=0)\n",
    "#         decoder_inputs = self.prenet(decoder_inputs)\n",
    "\n",
    "#         self.initialize_decoder_states(\n",
    "#             memory, mask=~get_mask_from_lengths(memory_lengths)\n",
    "#         )\n",
    "\n",
    "#         mel_outputs = torch.empty(\n",
    "#             B, 0, self.n_frames_per_step_current * self.n_mel_channels\n",
    "#         )\n",
    "#         if torch.cuda.is_available():\n",
    "#             mel_outputs = mel_outputs.cuda()\n",
    "#         gate_outputs, alignments = [], []\n",
    "#         desired_output_frames = decoder_inputs.size(0) / self.n_frames_per_step_current\n",
    "#         while mel_outputs.size(1) < desired_output_frames - 1:\n",
    "#             if (\n",
    "#                 mel_outputs.size(1) == 0\n",
    "#                 or np.random.uniform(0.0, 1.0) <= self.p_teacher_forcing\n",
    "#             ):\n",
    "#                 teacher_forced_frame = decoder_inputs[\n",
    "#                     mel_outputs.size(1) * self.n_frames_per_step_current\n",
    "#                 ]\n",
    "\n",
    "#                 to_concat = (teacher_forced_frame,)\n",
    "#                 decoder_input = torch.cat(to_concat, dim=1)\n",
    "#             else:\n",
    "\n",
    "#                 # NOTE(zach): we may need to concat these as we go to ensure that\n",
    "#                 # it's easy to retrieve the last n_frames_per_step_init frames.\n",
    "#                 to_concat = (\n",
    "#                     self.prenet(\n",
    "#                         mel_outputs[:, -1, -1 * self.n_frames_per_step_current :]\n",
    "#                     ),\n",
    "#                 )\n",
    "#                 decoder_input = torch.cat(to_concat, dim=1)\n",
    "#             # NOTE(zach): When training with fp16_run == True, decoder_rnn seems to run into\n",
    "#             # issues with NaNs in gradient, maybe due to vanishing gradients.\n",
    "#             # Disable half-precision for this call to work around the issue.\n",
    "#             with autocast(enabled=False):\n",
    "#                 mel_output, gate_output, attention_weights = self.decode(decoder_input)\n",
    "#             mel_outputs = torch.cat(\n",
    "#                 [\n",
    "#                     mel_outputs,\n",
    "#                     mel_output[\n",
    "#                         :, 0 : self.n_mel_channels * self.n_frames_per_step_current\n",
    "#                     ].unsqueeze(1),\n",
    "#                 ],\n",
    "#                 dim=1,\n",
    "#             )\n",
    "#             gate_outputs += [gate_output.squeeze()] * self.n_frames_per_step_current\n",
    "#             alignments += [attention_weights]\n",
    "\n",
    "#         mel_outputs, gate_outputs, alignments = self.parse_decoder_outputs(\n",
    "#             mel_outputs, gate_outputs, alignments\n",
    "#         )\n",
    "\n",
    "#         return mel_outputs, gate_outputs, alignments\n",
    "\n",
    "#     def inference(self, memory, memory_lengths):\n",
    "#         \"\"\"Decoder inference\n",
    "#         PARAMS\n",
    "#         ------\n",
    "#         memory: Encoder outputs\n",
    "#         RETURNS\n",
    "#         -------\n",
    "#         mel_outputs: mel outputs from the decoder\n",
    "#         gate_outputs: gate outputs from the decoder\n",
    "#         alignments: sequence of attention weights from the decoder\n",
    "#         \"\"\"\n",
    "#         decoder_input = self.get_go_frame(memory)\n",
    "#         pdb.set_trace()\n",
    "#         mask = get_mask_from_lengths(memory_lengths)\n",
    "#         (\n",
    "#             attention_hidden,\n",
    "#             attention_cell,\n",
    "#             decoder_hidden,\n",
    "#             decoder_cell,\n",
    "#             attention_weights,\n",
    "#             attention_weights_cum,\n",
    "#             attention_context,\n",
    "#             processed_memory,\n",
    "#         ) = self.initialize_decoder_states(memory, mask=~get_mask_from_lengths(memory_lengths))\n",
    "\n",
    "#         mel_lengths = torch.zeros(\n",
    "#             [memory.size(0)], dtype=torch.int32, device=memory.device\n",
    "#         )\n",
    "#         not_finished = torch.ones(\n",
    "#             [memory.size(0)], dtype=torch.int32, device=memory.device\n",
    "#         )\n",
    "\n",
    "#         mel_outputs, gate_outputs, alignments = (\n",
    "#             torch.zeros(1),\n",
    "#             torch.zeros(1),\n",
    "#             torch.zeros(1),\n",
    "#         )\n",
    "#         first_iter = True\n",
    "#         while True:\n",
    "#             decoder_input = self.prenet(decoder_input)\n",
    "#             (\n",
    "#                 mel_output,\n",
    "#                 gate_output,\n",
    "#                 attention_hidden,\n",
    "#                 attention_cell,\n",
    "#                 decoder_hidden,\n",
    "#                 decoder_cell,\n",
    "#                 attention_weights,\n",
    "#                 attention_weights_cum,\n",
    "#                 attention_context,\n",
    "#             ) = self.decode(\n",
    "#                 decoder_input,\n",
    "#                 attention_hidden,\n",
    "#                 attention_cell,\n",
    "#                 decoder_hidden,\n",
    "#                 decoder_cell,\n",
    "#                 attention_weights,\n",
    "#                 attention_weights_cum,\n",
    "#                 attention_context,\n",
    "#                 memory,\n",
    "#                 processed_memory,\n",
    "#                 mask,\n",
    "#             )\n",
    "\n",
    "#             if first_iter:\n",
    "#                 mel_outputs = mel_output.unsqueeze(0)\n",
    "#                 gate_outputs = gate_output\n",
    "#                 alignments = attention_weights\n",
    "#                 first_iter = False\n",
    "#             else:\n",
    "#                 mel_outputs = torch.cat((mel_outputs, mel_output.unsqueeze(0)), dim=0)\n",
    "#                 gate_outputs = torch.cat((gate_outputs, gate_output), dim=0)\n",
    "#                 alignments = torch.cat((alignments, attention_weights), dim=0)\n",
    "\n",
    "#             dec = (\n",
    "#                 torch.le(torch.sigmoid(gate_output), self.gate_threshold)\n",
    "#                 .to(torch.int32)\n",
    "#                 .squeeze(1)\n",
    "#             )\n",
    "\n",
    "#             not_finished = not_finished * dec\n",
    "#             mel_lengths += not_finished\n",
    "\n",
    "#             if self.early_stopping and torch.sum(not_finished) == 0:\n",
    "#                 break\n",
    "#             if len(mel_outputs) == self.max_decoder_steps:\n",
    "#                 print(\"Warning! Reached max decoder steps\")\n",
    "#                 break\n",
    "\n",
    "#             decoder_input = mel_output\n",
    "\n",
    "#         mel_outputs, gate_outputs, alignments = self.parse_decoder_outputs(\n",
    "#             mel_outputs, gate_outputs, alignments\n",
    "#         )\n",
    "\n",
    "#         return mel_outputs, gate_outputs, alignments, mel_lengths\n",
    "\n",
    "#     def inference_noattention(self, memory, attention_map):\n",
    "#         \"\"\"Decoder inference\n",
    "#         PARAMS\n",
    "#         ------\n",
    "#         memory: Encoder outputs\n",
    "\n",
    "#         RETURNS\n",
    "#         -------\n",
    "#         mel_outputs: mel outputs from the decoder\n",
    "#         gate_outputs: gate outputs from the decoder\n",
    "#         alignments: sequence of attention weights from the decoder\n",
    "#         \"\"\"\n",
    "#         decoder_input = self.get_go_frame(memory)\n",
    "\n",
    "#         self.initialize_decoder_states(memory, mask=None)\n",
    "\n",
    "#         B = memory.size(0)\n",
    "#         mel_outputs = torch.empty(\n",
    "#             B, 0, self.n_frames_per_step_current * self.n_mel_channels\n",
    "#         )\n",
    "#         if torch.cuda.is_available():\n",
    "#             mel_outputs = mel_outputs.cuda()\n",
    "#         gate_outputs, alignments = [], []\n",
    "#         for i in range(len(attention_map)):\n",
    "\n",
    "#             attention = attention_map[i]\n",
    "#             decoder_input = torch.cat((self.prenet(decoder_input)), dim=1)\n",
    "#             mel_output, gate_output, alignment = self.decode(decoder_input, attention)\n",
    "#             mel_output, gate_output, alignment = self.decode(decoder_input)\n",
    "#             mel_output = mel_output[\n",
    "#                 :, 0 : self.n_mel_channels * self.n_frames_per_step_current\n",
    "#             ].unsqueeze(1)\n",
    "\n",
    "#             mel_outputs = torch.cat([mel_outputs, mel_output], dim=1)\n",
    "#             gate_outputs += [gate_output.squeeze()] * self.n_frames_per_step_current\n",
    "#             alignments += [alignment]\n",
    "\n",
    "#             decoder_input = mel_output[:, -1, -1 * self.n_mel_channels :]\n",
    "\n",
    "#         mel_outputs, gate_outputs, alignments = self.parse_decoder_outputs(\n",
    "#             mel_outputs, gate_outputs, alignments\n",
    "#         )\n",
    "\n",
    "#         return mel_outputs, gate_outputs, alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0f37cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #mix v2\n",
    "# class Decoder(nn.Module):\n",
    "\n",
    "#     def __init__(self, hparams):\n",
    "#         super(Decoder, self).__init__()\n",
    "#         self.n_mel_channels = hparams.n_mel_channels\n",
    "#         self.n_frames_per_step_initial = hparams.n_frames_per_step_initial\n",
    "#         self.n_frames_per_step_current = hparams.n_frames_per_step_initial\n",
    "#         self.encoder_embedding_dim = hparams.encoder_embedding_dim\n",
    "#         self.attention_rnn_dim = hparams.attention_rnn_dim\n",
    "#         self.decoder_rnn_dim = hparams.decoder_rnn_dim\n",
    "#         self.prenet_dim = hparams.prenet_dim\n",
    "#         self.max_decoder_steps = hparams.max_decoder_steps\n",
    "#         self.gate_threshold = hparams.gate_threshold\n",
    "#         self.p_attention_dropout = hparams.p_attention_dropout\n",
    "#         self.p_decoder_dropout = hparams.p_decoder_dropout\n",
    "\n",
    "#         self.prenet = Prenet(\n",
    "#             hparams.n_mel_channels,\n",
    "#             [hparams.prenet_dim, hparams.prenet_dim],\n",
    "#         )\n",
    "\n",
    "#         self.attention_rnn = nn.LSTMCell(\n",
    "#             hparams.prenet_dim + hparams.encoder_embedding_dim,\n",
    "#             hparams.attention_rnn_dim)\n",
    "\n",
    "#         self.attention_layer = Attention(\n",
    "#             hparams.attention_rnn_dim, hparams.encoder_embedding_dim,\n",
    "#             hparams.attention_dim, hparams.attention_location_n_filters,\n",
    "#             hparams.attention_location_kernel_size,fp16_run=hparams.fp16_run)\n",
    "\n",
    "#         self.decoder_rnn = nn.LSTMCell(\n",
    "#             hparams.attention_rnn_dim + hparams.encoder_embedding_dim,\n",
    "#             hparams.decoder_rnn_dim, 1)\n",
    "\n",
    "#         self.linear_projection = LinearNorm(\n",
    "#             hparams.decoder_rnn_dim + self.encoder_embedding_dim,\n",
    "#             hparams.n_mel_channels * hparams.n_frames_per_step_initial,\n",
    "#         )\n",
    "\n",
    "#         self.gate_layer = LinearNorm(\n",
    "#             hparams.decoder_rnn_dim + hparams.encoder_embedding_dim, 1,\n",
    "#             bias=True, w_init_gain='sigmoid')\n",
    "\n",
    "\n",
    "#     def get_go_frame(self, memory):\n",
    "#         \"\"\"Gets all zeros frames to use as first decoder input\n",
    "#         PARAMS\n",
    "#         ------\n",
    "#         memory: decoder outputs\n",
    "#         RETURNS\n",
    "#         -------\n",
    "#         decoder_input: all zeros frames\n",
    "#         \"\"\"\n",
    "#         B = memory.size(0)\n",
    "#         dtype = memory.dtype\n",
    "#         device = memory.device\n",
    "#         decoder_input = torch.zeros(\n",
    "#             B, self.n_mel_channels * self.n_frames_per_step_initial, dtype=dtype, device=device\n",
    "#         )\n",
    "#         return decoder_input\n",
    "\n",
    "# #     def get_go_frame(self, memory):\n",
    "# #         \"\"\"Gets all zeros frames to use as first decoder input\n",
    "# #         PARAMS\n",
    "# #         ------\n",
    "# #         memory: decoder outputs\n",
    "\n",
    "# #         RETURNS\n",
    "# #         -------\n",
    "# #         decoder_input: all zeros frames\n",
    "# #         \"\"\"\n",
    "# #         B = memory.size(0)\n",
    "# #         decoder_input = Variable(memory.data.new(B, self.n_mel_channels).zero_())\n",
    "# #         return decoder_input\n",
    "\n",
    "#     def initialize_decoder_states(self, memory):\n",
    "#         \"\"\"Initializes attention rnn states, decoder rnn states, attention\n",
    "#         weights, attention cumulative weights, attention context, stores memory\n",
    "#         and stores processed memory\n",
    "#         PARAMS\n",
    "#         ------\n",
    "#         memory: Encoder outputs\n",
    "#         mask: Mask for padded data if training, expects None for inference\n",
    "#         \"\"\"\n",
    "#         B = memory.size(0)\n",
    "#         MAX_TIME = memory.size(1)\n",
    "#         dtype = memory.dtype\n",
    "#         device = memory.device\n",
    "\n",
    "#         attention_hidden = torch.zeros(\n",
    "#             B, self.attention_rnn_dim, dtype=dtype, device=device\n",
    "#         )\n",
    "#         attention_cell = torch.zeros(\n",
    "#             B, self.attention_rnn_dim, dtype=dtype, device=device\n",
    "#         )\n",
    "\n",
    "#         decoder_hidden = torch.zeros(\n",
    "#             B, self.decoder_rnn_dim, dtype=dtype, device=device\n",
    "#         )\n",
    "#         decoder_cell = torch.zeros(B, self.decoder_rnn_dim, dtype=dtype, device=device)\n",
    "\n",
    "#         attention_weights = torch.zeros(B, MAX_TIME, dtype=dtype, device=device)\n",
    "#         attention_weights_cum = torch.zeros(B, MAX_TIME, dtype=dtype, device=device)\n",
    "#         attention_context = torch.zeros(\n",
    "#             B, self.encoder_embedding_dim, dtype=dtype, device=device\n",
    "#         )\n",
    "\n",
    "#         processed_memory = self.attention_layer.memory_layer(memory)\n",
    "\n",
    "#         return (\n",
    "#             attention_hidden,\n",
    "#             attention_cell,\n",
    "#             decoder_hidden,\n",
    "#             decoder_cell,\n",
    "#             attention_weights,\n",
    "#             attention_weights_cum,\n",
    "#             attention_context,\n",
    "#             processed_memory,\n",
    "#         )\n",
    "\n",
    "#     def parse_decoder_inputs(self, decoder_inputs):\n",
    "#         \"\"\"Prepares decoder inputs, i.e. mel outputs\n",
    "#         PARAMS\n",
    "#         ------\n",
    "#         decoder_inputs: inputs used for teacher-forced training, i.e. mel-specs\n",
    "#         RETURNS\n",
    "#         -------\n",
    "#         inputs: processed decoder inputs\n",
    "#         \"\"\"\n",
    "#         # (B, n_mel_channels, T_out) -> (B, T_out, n_mel_channels)\n",
    "#         decoder_inputs = decoder_inputs.transpose(1, 2)\n",
    "#         decoder_inputs = decoder_inputs.view(\n",
    "#             decoder_inputs.size(0),\n",
    "#             int(decoder_inputs.size(1) / self.n_frames_per_step),\n",
    "#             -1,\n",
    "#         )\n",
    "#         # (B, T_out, n_mel_channels) -> (T_out, B, n_mel_channels)\n",
    "#         decoder_inputs = decoder_inputs.transpose(0, 1)\n",
    "#         return decoder_inputs\n",
    "\n",
    "#     def parse_decoder_outputs(self, mel_outputs, gate_outputs, alignments):\n",
    "#         \"\"\"Prepares decoder outputs for output\n",
    "#         PARAMS\n",
    "#         ------\n",
    "#         mel_outputs:\n",
    "#         gate_outputs: gate output energies\n",
    "#         alignments:\n",
    "#         RETURNS\n",
    "#         -------\n",
    "#         mel_outputs:\n",
    "#         gate_outpust: gate output energies\n",
    "#         alignments:\n",
    "#         \"\"\"\n",
    "#         # (T_out, B) -> (B, T_out)\n",
    "#         alignments = alignments.transpose(0, 1).contiguous()\n",
    "#         # (T_out, B) -> (B, T_out)\n",
    "#         gate_outputs = gate_outputs.transpose(0, 1).contiguous()\n",
    "#         # (T_out, B, n_mel_channels) -> (B, T_out, n_mel_channels)\n",
    "#         mel_outputs = mel_outputs.transpose(0, 1).contiguous()\n",
    "#         # decouple frames per step\n",
    "#         shape = (mel_outputs.shape[0], -1, self.n_mel_channels)\n",
    "#         mel_outputs = mel_outputs.view(*shape)\n",
    "#         # (B, T_out, n_mel_channels) -> (B, n_mel_channels, T_out)\n",
    "#         mel_outputs = mel_outputs.transpose(1, 2)\n",
    "\n",
    "#         return mel_outputs, gate_outputs, alignments\n",
    "\n",
    "#     def decode(\n",
    "#         self,\n",
    "#         decoder_input,\n",
    "#         attention_hidden,\n",
    "#         attention_cell,\n",
    "#         decoder_hidden,\n",
    "#         decoder_cell,\n",
    "#         attention_weights,\n",
    "#         attention_weights_cum,\n",
    "#         attention_context,\n",
    "#         memory,\n",
    "#         processed_memory,\n",
    "#         mask,\n",
    "#     ):\n",
    "#         \"\"\"Decoder step using stored states, attention and memory\n",
    "#         PARAMS\n",
    "#         ------\n",
    "#         decoder_input: previous mel output\n",
    "#         RETURNS\n",
    "#         -------\n",
    "#         mel_output:\n",
    "#         gate_output: gate output energies\n",
    "#         attention_weights:\n",
    "#         \"\"\"\n",
    "#         cell_input = torch.cat((decoder_input, attention_context), -1)\n",
    "\n",
    "#         attention_hidden, attention_cell = self.attention_rnn(\n",
    "#             cell_input, (attention_hidden, attention_cell)\n",
    "#         )\n",
    "#         attention_hidden = F.dropout(\n",
    "#             attention_hidden, self.p_attention_dropout, self.training\n",
    "#         )\n",
    "\n",
    "#         attention_weights_cat = torch.cat(\n",
    "#             (attention_weights.unsqueeze(1), attention_weights_cum.unsqueeze(1)), dim=1\n",
    "#         )\n",
    "#         attention_context, attention_weights = self.attention_layer(\n",
    "#             attention_hidden, memory, processed_memory, attention_weights_cat, mask\n",
    "#         )\n",
    "\n",
    "#         attention_weights_cum += attention_weights\n",
    "#         decoder_input = torch.cat((attention_hidden, attention_context), -1)\n",
    "\n",
    "#         decoder_hidden, decoder_cell = self.decoder_rnn(\n",
    "#             decoder_input, (decoder_hidden, decoder_cell)\n",
    "#         )\n",
    "#         decoder_hidden = F.dropout(\n",
    "#             decoder_hidden, self.p_decoder_dropout, self.training\n",
    "#         )\n",
    "\n",
    "#         decoder_hidden_attention_context = torch.cat(\n",
    "#             (decoder_hidden, attention_context), dim=1\n",
    "#         )\n",
    "#         decoder_output = self.linear_projection(decoder_hidden_attention_context)\n",
    "\n",
    "#         gate_prediction = self.gate_layer(decoder_hidden_attention_context)\n",
    "\n",
    "#         return (\n",
    "#             decoder_output,\n",
    "#             gate_prediction,\n",
    "#             attention_hidden,\n",
    "#             attention_cell,\n",
    "#             decoder_hidden,\n",
    "#             decoder_cell,\n",
    "#             attention_weights,\n",
    "#             attention_weights_cum,\n",
    "#             attention_context,\n",
    "#         )\n",
    "\n",
    "#     @torch.jit.ignore\n",
    "#     def forward(self, memory, decoder_inputs, memory_lengths):\n",
    "#         \"\"\"Decoder forward pass for training\n",
    "#         PARAMS\n",
    "#         ------\n",
    "#         memory: Encoder outputs\n",
    "#         decoder_inputs: Decoder inputs for teacher forcing. i.e. mel-specs\n",
    "#         memory_lengths: Encoder output lengths for attention masking.\n",
    "#         RETURNS\n",
    "#         -------\n",
    "#         mel_outputs: mel outputs from the decoder\n",
    "#         gate_outputs: gate outputs from the decoder\n",
    "#         alignments: sequence of attention weights from the decoder\n",
    "#         \"\"\"\n",
    "\n",
    "#         decoder_input = self.get_go_frame(memory).unsqueeze(0)\n",
    "#         decoder_inputs = self.parse_decoder_inputs(decoder_inputs)\n",
    "#         decoder_inputs = torch.cat((decoder_input, decoder_inputs), dim=0)\n",
    "#         decoder_inputs = self.prenet(decoder_inputs)\n",
    "\n",
    "#         mask = get_mask_from_lengths(memory_lengths)\n",
    "#         (\n",
    "#             attention_hidden,\n",
    "#             attention_cell,\n",
    "#             decoder_hidden,\n",
    "#             decoder_cell,\n",
    "#             attention_weights,\n",
    "#             attention_weights_cum,\n",
    "#             attention_context,\n",
    "#             processed_memory,\n",
    "#         ) = self.initialize_decoder_states(memory)\n",
    "\n",
    "#         mel_outputs, gate_outputs, alignments = [], [], []\n",
    "#         while len(mel_outputs) < decoder_inputs.size(0) - 1:\n",
    "#             decoder_input = decoder_inputs[len(mel_outputs)]\n",
    "#             (\n",
    "#                 mel_output,\n",
    "#                 gate_output,\n",
    "#                 attention_hidden,\n",
    "#                 attention_cell,\n",
    "#                 decoder_hidden,\n",
    "#                 decoder_cell,\n",
    "#                 attention_weights,\n",
    "#                 attention_weights_cum,\n",
    "#                 attention_context,\n",
    "#             ) = self.decode(\n",
    "#                 decoder_input,\n",
    "#                 attention_hidden,\n",
    "#                 attention_cell,\n",
    "#                 decoder_hidden,\n",
    "#                 decoder_cell,\n",
    "#                 attention_weights,\n",
    "#                 attention_weights_cum,\n",
    "#                 attention_context,\n",
    "#                 memory,\n",
    "#                 processed_memory,\n",
    "#                 mask,\n",
    "#             )\n",
    "\n",
    "#             mel_outputs += [mel_output.squeeze(1)]\n",
    "#             gate_outputs += [gate_output.squeeze(1)]\n",
    "#             alignments += [attention_weights]\n",
    "\n",
    "#         mel_outputs, gate_outputs, alignments = self.parse_decoder_outputs(\n",
    "#             torch.stack(mel_outputs), torch.stack(gate_outputs), torch.stack(alignments)\n",
    "#         )\n",
    "\n",
    "#         return mel_outputs, gate_outputs, alignments\n",
    "\n",
    "#     def inference(self, memory, memory_lengths):\n",
    "#         \"\"\"Decoder inference\n",
    "#         PARAMS\n",
    "#         ------\n",
    "#         memory: Encoder outputs\n",
    "#         RETURNS\n",
    "#         -------\n",
    "#         mel_outputs: mel outputs from the decoder\n",
    "#         gate_outputs: gate outputs from the decoder\n",
    "#         alignments: sequence of attention weights from the decoder\n",
    "#         \"\"\"\n",
    "#         decoder_input = self.get_go_frame(memory)\n",
    "\n",
    "#         mask = get_mask_from_lengths(memory_lengths)\n",
    "#         (\n",
    "#             attention_hidden,\n",
    "#             attention_cell,\n",
    "#             decoder_hidden,\n",
    "#             decoder_cell,\n",
    "#             attention_weights,\n",
    "#             attention_weights_cum,\n",
    "#             attention_context,\n",
    "#             processed_memory,\n",
    "#         ) = self.initialize_decoder_states(memory)\n",
    "\n",
    "#         mel_lengths = torch.zeros(\n",
    "#             [memory.size(0)], dtype=torch.int32, device=memory.device\n",
    "#         )\n",
    "#         not_finished = torch.ones(\n",
    "#             [memory.size(0)], dtype=torch.int32, device=memory.device\n",
    "#         )\n",
    "\n",
    "#         mel_outputs, gate_outputs, alignments = (\n",
    "#             torch.zeros(1),\n",
    "#             torch.zeros(1),\n",
    "#             torch.zeros(1),\n",
    "#         )\n",
    "#         first_iter = True\n",
    "#         while True:\n",
    "#             decoder_input = self.prenet(decoder_input)\n",
    "#             (\n",
    "#                 mel_output,\n",
    "#                 gate_output,\n",
    "#                 attention_hidden,\n",
    "#                 attention_cell,\n",
    "#                 decoder_hidden,\n",
    "#                 decoder_cell,\n",
    "#                 attention_weights,\n",
    "#                 attention_weights_cum,\n",
    "#                 attention_context,\n",
    "#             ) = self.decode(\n",
    "#                 decoder_input,\n",
    "#                 attention_hidden,\n",
    "#                 attention_cell,\n",
    "#                 decoder_hidden,\n",
    "#                 decoder_cell,\n",
    "#                 attention_weights,\n",
    "#                 attention_weights_cum,\n",
    "#                 attention_context,\n",
    "#                 memory,\n",
    "#                 processed_memory,\n",
    "#                 mask,\n",
    "#             )\n",
    "\n",
    "#             if first_iter:\n",
    "#                 mel_outputs = mel_output.unsqueeze(0)\n",
    "#                 gate_outputs = gate_output\n",
    "#                 alignments = attention_weights\n",
    "#                 first_iter = False\n",
    "#             else:\n",
    "#                 mel_outputs = torch.cat((mel_outputs, mel_output.unsqueeze(0)), dim=0)\n",
    "#                 gate_outputs = torch.cat((gate_outputs, gate_output), dim=0)\n",
    "#                 alignments = torch.cat((alignments, attention_weights), dim=0)\n",
    "\n",
    "#             dec = (\n",
    "#                 torch.le(torch.sigmoid(gate_output), self.gate_threshold)\n",
    "#                 .to(torch.int32)\n",
    "#                 .squeeze(1)\n",
    "#             )\n",
    "\n",
    "#             not_finished = not_finished * dec\n",
    "#             mel_lengths += not_finished\n",
    "\n",
    "# #             if self.early_stopping and torch.sum(not_finished) == 0:\n",
    "# #                 break\n",
    "#             if len(mel_outputs) == self.max_decoder_steps:\n",
    "#                 print(\"Warning! Reached max decoder steps\")\n",
    "#                 break\n",
    "\n",
    "#             decoder_input = mel_output\n",
    "\n",
    "#         mel_outputs, gate_outputs, alignments = self.parse_decoder_outputs(\n",
    "#             mel_outputs, gate_outputs, alignments\n",
    "#         )\n",
    "\n",
    "#         return mel_outputs, gate_outputs, alignments, mel_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "628d928d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mix v3\n",
    "from torch import nn\n",
    "from uberduck_ml_dev.models.base import TTSModel\n",
    "from uberduck_ml_dev.models.common import Attention, Conv1d, LinearNorm, GST\n",
    "from uberduck_ml_dev.text.symbols import symbols\n",
    "from uberduck_ml_dev.vendor.tfcompat.hparam import HParams\n",
    "from uberduck_ml_dev.utils.utils import to_gpu, get_mask_from_lengths\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.n_mel_channels = hparams.n_mel_channels\n",
    "        self.n_frames_per_step_initial = hparams.n_frames_per_step_initial\n",
    "        self.n_frames_per_step_current = hparams.n_frames_per_step_initial\n",
    "        self.encoder_embedding_dim = hparams.encoder_embedding_dim\n",
    "        self.attention_rnn_dim = hparams.attention_rnn_dim\n",
    "        self.decoder_rnn_dim = hparams.decoder_rnn_dim\n",
    "        self.prenet_dim = hparams.prenet_dim\n",
    "        self.max_decoder_steps = hparams.max_decoder_steps\n",
    "        self.gate_threshold = hparams.gate_threshold\n",
    "        self.p_attention_dropout = hparams.p_attention_dropout\n",
    "        self.p_decoder_dropout = hparams.p_decoder_dropout\n",
    "        self.p_teacher_forcing = hparams.p_teacher_forcing\n",
    "\n",
    "        self.prenet = Prenet(\n",
    "            hparams.n_mel_channels,\n",
    "            [hparams.prenet_dim, hparams.prenet_dim],\n",
    "        )\n",
    "\n",
    "        self.attention_rnn = nn.LSTMCell(\n",
    "            hparams.prenet_dim + self.encoder_embedding_dim,\n",
    "            hparams.attention_rnn_dim,\n",
    "        )\n",
    "\n",
    "        self.attention_layer = Attention(\n",
    "            hparams.attention_rnn_dim,\n",
    "            self.encoder_embedding_dim,\n",
    "            hparams.attention_dim,\n",
    "            hparams.attention_location_n_filters,\n",
    "            hparams.attention_location_kernel_size,\n",
    "            fp16_run=hparams.fp16_run,\n",
    "        )\n",
    "\n",
    "        self.decoder_rnn = nn.LSTMCell(\n",
    "            hparams.attention_rnn_dim + self.encoder_embedding_dim,\n",
    "            hparams.decoder_rnn_dim,\n",
    "            1,\n",
    "        )\n",
    "\n",
    "        self.linear_projection = LinearNorm(\n",
    "            hparams.decoder_rnn_dim + self.encoder_embedding_dim,\n",
    "            hparams.n_mel_channels * hparams.n_frames_per_step_initial,\n",
    "        )\n",
    "\n",
    "        self.gate_layer = LinearNorm(\n",
    "            hparams.decoder_rnn_dim + self.encoder_embedding_dim,\n",
    "            1,\n",
    "            bias=True,\n",
    "            w_init_gain=\"sigmoid\",\n",
    "        )\n",
    "\n",
    "    def set_current_frames_per_step(self, n_frames: int):\n",
    "        self.n_frames_per_step_current = n_frames\n",
    "\n",
    "    def get_go_frame(self, memory):\n",
    "        \"\"\"Gets all zeros frames to use as first decoder input\n",
    "        PARAMS\n",
    "        ------\n",
    "        memory: decoder outputs\n",
    "\n",
    "        RETURNS\n",
    "        -------\n",
    "        decoder_input: all zeros frames\n",
    "        \"\"\"\n",
    "        B = memory.size(0)\n",
    "        decoder_input = Variable(memory.data.new(B, self.n_mel_channels).zero_())\n",
    "        return decoder_input\n",
    "\n",
    "    def initialize_decoder_states(self, memory, mask):\n",
    "        \"\"\"Initializes attention rnn states, decoder rnn states, attention\n",
    "        weights, attention cumulative weights, attention context, stores memory\n",
    "        and stores processed memory\n",
    "        PARAMS\n",
    "        ------\n",
    "        memory: Encoder outputs\n",
    "        mask: Mask for padded data if training, expects None for inference\n",
    "        \"\"\"\n",
    "        B = memory.size(0)\n",
    "        MAX_TIME = memory.size(1)\n",
    "\n",
    "        self.attention_hidden = Variable(\n",
    "            memory.data.new(B, self.attention_rnn_dim).zero_()\n",
    "        )\n",
    "        self.attention_cell = Variable(\n",
    "            memory.data.new(B, self.attention_rnn_dim).zero_()\n",
    "        )\n",
    "\n",
    "        self.decoder_hidden = Variable(memory.data.new(B, self.decoder_rnn_dim).zero_())\n",
    "        self.decoder_cell = Variable(memory.data.new(B, self.decoder_rnn_dim).zero_())\n",
    "\n",
    "        self.attention_weights = Variable(memory.data.new(B, MAX_TIME).zero_())\n",
    "        self.attention_weights_cum = Variable(memory.data.new(B, MAX_TIME).zero_())\n",
    "        self.attention_context = Variable(\n",
    "            memory.data.new(B, self.encoder_embedding_dim).zero_()\n",
    "        )\n",
    "\n",
    "        self.memory = memory\n",
    "        self.processed_memory = self.attention_layer.memory_layer(memory)\n",
    "        self.mask = mask\n",
    "\n",
    "    def parse_decoder_inputs(self, decoder_inputs):\n",
    "        \"\"\"Prepares decoder inputs, i.e. mel outputs\n",
    "        PARAMS\n",
    "        ------\n",
    "        decoder_inputs: inputs used for teacher-forced training, i.e. mel-specs\n",
    "\n",
    "        RETURNS\n",
    "        -------\n",
    "        inputs: processed decoder inputs\n",
    "\n",
    "        \"\"\"\n",
    "        # (B, n_mel_channels, T_out) -> (B, T_out, n_mel_channels)\n",
    "\n",
    "        decoder_inputs = decoder_inputs.transpose(1, 2)\n",
    "        decoder_inputs = decoder_inputs.contiguous()\n",
    "        # print(decoder_inputs.shape, 'dec_in')\n",
    "        # print(decoder_inputs.size(0), int(decoder_inputs.size(1)), self.n_frames_per_step_current)\n",
    "        decoder_inputs = decoder_inputs.view(\n",
    "            decoder_inputs.size(0),\n",
    "            int(decoder_inputs.size(1) / self.n_frames_per_step_current),\n",
    "            -1,\n",
    "        )\n",
    "        # (B, T_out, n_mel_channels) -> (T_out, B, n_mel_channels)\n",
    "        decoder_inputs = decoder_inputs.transpose(0, 1)\n",
    "        return decoder_inputs\n",
    "\n",
    "    def parse_decoder_outputs(self, mel_outputs, gate_outputs, alignments):\n",
    "        \"\"\"Prepares decoder outputs for output\n",
    "        PARAMS\n",
    "        ------\n",
    "        mel_outputs:\n",
    "        gate_outputs: gate output energies\n",
    "        alignments:\n",
    "\n",
    "        RETURNS\n",
    "        -------\n",
    "        mel_outputs:\n",
    "        gate_outpust: gate output energies\n",
    "        alignments:\n",
    "        \"\"\"\n",
    "        # (T_out, B) -> (B, T_out)\n",
    "        alignments = torch.stack(alignments).transpose(0, 1)\n",
    "        # (T_out, B) -> (B, T_out)\n",
    "        gate_outputs = torch.stack(gate_outputs)\n",
    "        if len(gate_outputs.size()) > 1:\n",
    "            gate_outputs = gate_outputs.transpose(0, 1)\n",
    "        else:\n",
    "            gate_outputs = gate_outputs[None]\n",
    "        gate_outputs = gate_outputs.contiguous()\n",
    "        # (B, T_out, n_mel_channels * n_frames_per_step) -> (B, T_out * n_frames_per_step, n_mel_channels)\n",
    "        mel_outputs = mel_outputs.view(mel_outputs.size(0), -1, self.n_mel_channels)\n",
    "        # (B, T_out, n_mel_channels) -> (B, n_mel_channels, T_out)\n",
    "        mel_outputs = mel_outputs.transpose(1, 2)\n",
    "\n",
    "        return mel_outputs, gate_outputs, alignments\n",
    "\n",
    "    def decode(self, decoder_input, attention_weights=None):\n",
    "        \"\"\"Decoder step using stored states, attention and memory\n",
    "        PARAMS\n",
    "        ------\n",
    "        decoder_input: previous mel output\n",
    "\n",
    "        RETURNS\n",
    "        -------\n",
    "        mel_output:\n",
    "        gate_output: gate output energies\n",
    "        attention_weights:\n",
    "        \"\"\"\n",
    "        cell_input = torch.cat((decoder_input, self.attention_context), -1)\n",
    "        self.attention_hidden, self.attention_cell = self.attention_rnn(\n",
    "            cell_input, (self.attention_hidden, self.attention_cell)\n",
    "        )\n",
    "        self.attention_hidden = F.dropout(\n",
    "            self.attention_hidden, self.p_attention_dropout, self.training\n",
    "        )\n",
    "        self.attention_cell = F.dropout(\n",
    "            self.attention_cell, self.p_attention_dropout, self.training\n",
    "        )\n",
    "\n",
    "        attention_weights_cat = torch.cat(\n",
    "            (\n",
    "                self.attention_weights.unsqueeze(1),\n",
    "                self.attention_weights_cum.unsqueeze(1),\n",
    "            ),\n",
    "            dim=1,\n",
    "        )\n",
    "        self.attention_context, self.attention_weights = self.attention_layer(\n",
    "            self.attention_hidden,\n",
    "            self.memory,\n",
    "            self.processed_memory,\n",
    "            attention_weights_cat,\n",
    "            self.mask,\n",
    "            attention_weights,\n",
    "        )\n",
    "\n",
    "        self.attention_weights_cum += self.attention_weights\n",
    "        decoder_input = torch.cat((self.attention_hidden, self.attention_context), -1)\n",
    "        self.decoder_hidden, self.decoder_cell = self.decoder_rnn(\n",
    "            decoder_input, (self.decoder_hidden, self.decoder_cell)\n",
    "        )\n",
    "        self.decoder_hidden = F.dropout(\n",
    "            self.decoder_hidden, self.p_decoder_dropout, self.training\n",
    "        )\n",
    "        self.decoder_cell = F.dropout(\n",
    "            self.decoder_cell, self.p_decoder_dropout, self.training\n",
    "        )\n",
    "\n",
    "        decoder_hidden_attention_context = torch.cat(\n",
    "            (self.decoder_hidden, self.attention_context), dim=1\n",
    "        )\n",
    "\n",
    "        decoder_output = self.linear_projection(decoder_hidden_attention_context)\n",
    "\n",
    "        gate_prediction = self.gate_layer(decoder_hidden_attention_context)\n",
    "        return decoder_output, gate_prediction, self.attention_weights\n",
    "\n",
    "    def forward(self, memory, decoder_inputs, memory_lengths):\n",
    "        \"\"\"Decoder forward pass for training\n",
    "        PARAMS\n",
    "        ------\n",
    "        memory: Encoder outputs\n",
    "        decoder_inputs: Decoder inputs for teacher forcing. i.e. mel-specs\n",
    "        memory_lengths: Encoder output lengths for attention masking.\n",
    "\n",
    "        RETURNS\n",
    "        -------\n",
    "        mel_outputs: mel outputs from the decoder\n",
    "        gate_outputs: gate outputs from the decoder\n",
    "        alignments: sequence of attention weights from the decoder\n",
    "        \"\"\"\n",
    "        B = memory.size(0)\n",
    "        decoder_inputs = self.parse_decoder_inputs(decoder_inputs)\n",
    "        decoder_inputs = decoder_inputs.reshape(\n",
    "            -1, decoder_inputs.size(1), self.n_mel_channels\n",
    "        )\n",
    "        decoder_input = self.get_go_frame(memory).unsqueeze(0)\n",
    "        decoder_inputs = torch.cat((decoder_input, decoder_inputs), dim=0)\n",
    "        decoder_inputs = self.prenet(decoder_inputs)\n",
    "\n",
    "        self.initialize_decoder_states(\n",
    "            memory, mask=~get_mask_from_lengths(memory_lengths)\n",
    "        )\n",
    "\n",
    "        mel_outputs = torch.empty(\n",
    "            B, 0, self.n_frames_per_step_current * self.n_mel_channels\n",
    "        )\n",
    "        if torch.cuda.is_available():\n",
    "            mel_outputs = mel_outputs.cuda()\n",
    "        gate_outputs, alignments = [], []\n",
    "        desired_output_frames = decoder_inputs.size(0) / self.n_frames_per_step_current\n",
    "        while mel_outputs.size(1) < desired_output_frames - 1:\n",
    "            if (\n",
    "                mel_outputs.size(1) == 0\n",
    "                or np.random.uniform(0.0, 1.0) <= self.p_teacher_forcing\n",
    "            ):\n",
    "                teacher_forced_frame = decoder_inputs[\n",
    "                    mel_outputs.size(1) * self.n_frames_per_step_current\n",
    "                ]\n",
    "\n",
    "                to_concat = (teacher_forced_frame,)\n",
    "                decoder_input = torch.cat(to_concat, dim=1)\n",
    "            else:\n",
    "\n",
    "                # NOTE(zach): we may need to concat these as we go to ensure that\n",
    "                # it's easy to retrieve the last n_frames_per_step_init frames.\n",
    "                to_concat = (\n",
    "                    self.prenet(\n",
    "                        mel_outputs[:, -1, -1 * self.n_frames_per_step_current :]\n",
    "                    ),\n",
    "                )\n",
    "                decoder_input = torch.cat(to_concat, dim=1)\n",
    "            # NOTE(zach): When training with fp16_run == True, decoder_rnn seems to run into\n",
    "            # issues with NaNs in gradient, maybe due to vanishing gradients.\n",
    "            # Disable half-precision for this call to work around the issue.\n",
    "            with autocast(enabled=False):\n",
    "                mel_output, gate_output, attention_weights = self.decode(decoder_input)\n",
    "            mel_outputs = torch.cat(\n",
    "                [\n",
    "                    mel_outputs,\n",
    "                    mel_output[\n",
    "                        :, 0 : self.n_mel_channels * self.n_frames_per_step_current\n",
    "                    ].unsqueeze(1),\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "            gate_outputs += [gate_output.squeeze()] * self.n_frames_per_step_current\n",
    "            alignments += [attention_weights]\n",
    "\n",
    "        mel_outputs, gate_outputs, alignments = self.parse_decoder_outputs(\n",
    "            mel_outputs, gate_outputs, alignments\n",
    "        )\n",
    "\n",
    "        return mel_outputs, gate_outputs, alignments\n",
    "\n",
    "    def inference(self, memory, memory_lengths):\n",
    "        \"\"\"Decoder inference\n",
    "        PARAMS\n",
    "        ------\n",
    "        memory: Encoder outputs\n",
    "\n",
    "        RETURNS\n",
    "        -------\n",
    "        mel_outputs: mel outputs from the decoder\n",
    "        gate_outputs: gate outputs from the decoder\n",
    "        alignments: sequence of attention weights from the decoder\n",
    "        \"\"\"\n",
    "        # pdb.set_trace()\n",
    "        decoder_input = self.get_go_frame(memory)\n",
    "        self.initialize_decoder_states(\n",
    "            memory, mask=~get_mask_from_lengths(memory_lengths)\n",
    "        )\n",
    "\n",
    "        B = memory.size(0)\n",
    "        mel_outputs = torch.empty(\n",
    "            B, 0, self.n_frames_per_step_current * self.n_mel_channels\n",
    "        )\n",
    "        if torch.cuda.is_available():\n",
    "            mel_outputs = mel_outputs.cuda()\n",
    "        gate_outputs, alignments = [], []\n",
    "\n",
    "        mel_lengths = torch.zeros(\n",
    "            [memory.size(0)], dtype=torch.int32, device=memory.device\n",
    "        )\n",
    "        not_finished = torch.ones(\n",
    "            [memory.size(0)], dtype=torch.int32, device=memory.device\n",
    "        )\n",
    "\n",
    "        while True:\n",
    "\n",
    "            to_cat = (self.prenet(decoder_input),)\n",
    "\n",
    "            decoder_input = torch.cat(to_cat, dim=1)\n",
    "            mel_output, gate_output, alignment = self.decode(decoder_input)\n",
    "            mel_output = mel_output[\n",
    "                :, 0 : self.n_mel_channels * self.n_frames_per_step_current\n",
    "            ].unsqueeze(1)\n",
    "\n",
    "            mel_outputs = torch.cat([mel_outputs, mel_output], dim=1)\n",
    "            gate_outputs += [gate_output.squeeze()] * self.n_frames_per_step_current\n",
    "            alignments += [alignment]\n",
    "\n",
    "            #             if torch.sigmoid(gate_output.data) > self.gate_threshold:\n",
    "            #                 break\n",
    "            dec = (\n",
    "                torch.le(torch.sigmoid(gate_output), self.gate_threshold)\n",
    "                .to(torch.int32)\n",
    "                .squeeze(1)\n",
    "            )\n",
    "\n",
    "            not_finished = not_finished * dec\n",
    "            mel_lengths += not_finished\n",
    "            if torch.sum(not_finished) == 0:\n",
    "                break\n",
    "            if len(mel_outputs) == self.max_decoder_steps:\n",
    "                print(\"Warning! Reached max decoder steps\")\n",
    "                break\n",
    "\n",
    "            decoder_input = mel_output[:, -1, -1 * self.n_mel_channels :]\n",
    "\n",
    "        mel_outputs, gate_outputs, alignments = self.parse_decoder_outputs(\n",
    "            mel_outputs, gate_outputs, alignments\n",
    "        )\n",
    "\n",
    "        return mel_outputs, gate_outputs, alignments\n",
    "\n",
    "    def inference_noattention(self, memory, attention_map):\n",
    "        \"\"\"Decoder inference\n",
    "        PARAMS\n",
    "        ------\n",
    "        memory: Encoder outputs\n",
    "\n",
    "        RETURNS\n",
    "        -------\n",
    "        mel_outputs: mel outputs from the decoder\n",
    "        gate_outputs: gate outputs from the decoder\n",
    "        alignments: sequence of attention weights from the decoder\n",
    "        \"\"\"\n",
    "        decoder_input = self.get_go_frame(memory)\n",
    "\n",
    "        self.initialize_decoder_states(memory, mask=None)\n",
    "\n",
    "        B = memory.size(0)\n",
    "        mel_outputs = torch.empty(\n",
    "            B, 0, self.n_frames_per_step_current * self.n_mel_channels\n",
    "        )\n",
    "        if torch.cuda.is_available():\n",
    "            mel_outputs = mel_outputs.cuda()\n",
    "        gate_outputs, alignments = [], []\n",
    "        for i in range(len(attention_map)):\n",
    "\n",
    "            attention = attention_map[i]\n",
    "            decoder_input = torch.cat((self.prenet(decoder_input)), dim=1)\n",
    "            mel_output, gate_output, alignment = self.decode(decoder_input, attention)\n",
    "            mel_output, gate_output, alignment = self.decode(decoder_input)\n",
    "            mel_output = mel_output[\n",
    "                :, 0 : self.n_mel_channels * self.n_frames_per_step_current\n",
    "            ].unsqueeze(1)\n",
    "\n",
    "            mel_outputs = torch.cat([mel_outputs, mel_output], dim=1)\n",
    "            gate_outputs += [gate_output.squeeze()] * self.n_frames_per_step_current\n",
    "            alignments += [alignment]\n",
    "\n",
    "            decoder_input = mel_output[:, -1, -1 * self.n_mel_channels :]\n",
    "\n",
    "        mel_outputs, gate_outputs, alignments = self.parse_decoder_outputs(\n",
    "            mel_outputs, gate_outputs, alignments\n",
    "        )\n",
    "\n",
    "        return mel_outputs, gate_outputs, alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdf3c91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ub_ml_dev\n",
    "# class Decoder(nn.Module):\n",
    "#     def __init__(self, hparams):\n",
    "#         super(Decoder, self).__init__()\n",
    "#         self.n_mel_channels = hparams.n_mel_channels\n",
    "#         self.n_frames_per_step = hparams.n_frames_per_step\n",
    "#         self.encoder_embedding_dim = hparams.encoder_embedding_dim\n",
    "#         self.attention_rnn_dim = hparams.attention_rnn_dim\n",
    "#         self.decoder_rnn_dim = hparams.decoder_rnn_dim\n",
    "#         self.prenet_dim = hparams.prenet_dim\n",
    "#         self.max_decoder_steps = hparams.max_decoder_steps\n",
    "#         self.gate_threshold = hparams.gate_threshold\n",
    "#         self.p_attention_dropout = hparams.p_attention_dropout\n",
    "#         self.p_decoder_dropout = hparams.p_decoder_dropout\n",
    "\n",
    "#         self.prenet = Prenet(\n",
    "#             hparams.n_mel_channels * hparams.n_frames_per_step,\n",
    "#             [hparams.prenet_dim, hparams.prenet_dim])\n",
    "\n",
    "#         self.attention_rnn = nn.LSTMCell(\n",
    "#             hparams.prenet_dim + hparams.encoder_embedding_dim,\n",
    "#             hparams.attention_rnn_dim)\n",
    "\n",
    "#         self.attention_layer = Attention(\n",
    "#             hparams.attention_rnn_dim, hparams.encoder_embedding_dim,\n",
    "#             hparams.attention_dim, hparams.attention_location_n_filters,\n",
    "#             hparams.attention_location_kernel_size)\n",
    "\n",
    "#         self.decoder_rnn = nn.LSTMCell(\n",
    "#             hparams.attention_rnn_dim + hparams.encoder_embedding_dim,\n",
    "#             hparams.decoder_rnn_dim, 1)\n",
    "\n",
    "#         self.linear_projection = LinearNorm(\n",
    "#             hparams.decoder_rnn_dim + hparams.encoder_embedding_dim,\n",
    "#             hparams.n_mel_channels * hparams.n_frames_per_step)\n",
    "\n",
    "#         self.gate_layer = LinearNorm(\n",
    "#             hparams.decoder_rnn_dim + hparams.encoder_embedding_dim, 1,\n",
    "#             bias=True, w_init_gain='sigmoid')\n",
    "\n",
    "#     def get_go_frame(self, memory):\n",
    "#         \"\"\" Gets all zeros frames to use as first decoder input\n",
    "#         PARAMS\n",
    "#         ------\n",
    "#         memory: decoder outputs\n",
    "#         RETURNS\n",
    "#         -------\n",
    "#         decoder_input: all zeros frames\n",
    "#         \"\"\"\n",
    "#         B = memory.size(0)\n",
    "#         decoder_input = Variable(memory.data.new(\n",
    "#             B, self.n_mel_channels * self.n_frames_per_step).zero_())\n",
    "#         return decoder_input\n",
    "\n",
    "#     def initialize_decoder_states(self, memory, mask):\n",
    "#         \"\"\" Initializes attention rnn states, decoder rnn states, attention\n",
    "#         weights, attention cumulative weights, attention context, stores memory\n",
    "#         and stores processed memory\n",
    "#         PARAMS\n",
    "#         ------\n",
    "#         memory: Encoder outputs\n",
    "#         mask: Mask for padded data if training, expects None for inference\n",
    "#         \"\"\"\n",
    "#         B = memory.size(0)\n",
    "#         MAX_TIME = memory.size(1)\n",
    "\n",
    "#         self.attention_hidden = Variable(memory.data.new(\n",
    "#             B, self.attention_rnn_dim).zero_())\n",
    "#         self.attention_cell = Variable(memory.data.new(\n",
    "#             B, self.attention_rnn_dim).zero_())\n",
    "\n",
    "#         self.decoder_hidden = Variable(memory.data.new(\n",
    "#             B, self.decoder_rnn_dim).zero_())\n",
    "#         self.decoder_cell = Variable(memory.data.new(\n",
    "#             B, self.decoder_rnn_dim).zero_())\n",
    "\n",
    "#         self.attention_weights = Variable(memory.data.new(\n",
    "#             B, MAX_TIME).zero_())\n",
    "#         self.attention_weights_cum = Variable(memory.data.new(\n",
    "#             B, MAX_TIME).zero_())\n",
    "#         self.attention_context = Variable(memory.data.new(\n",
    "#             B, self.encoder_embedding_dim).zero_())\n",
    "\n",
    "#         self.memory = memory\n",
    "#         self.processed_memory = self.attention_layer.memory_layer(memory)\n",
    "#         self.mask = mask\n",
    "\n",
    "#     def parse_decoder_inputs(self, decoder_inputs):\n",
    "#         \"\"\" Prepares decoder inputs, i.e. mel outputs\n",
    "#         PARAMS\n",
    "#         ------\n",
    "#         decoder_inputs: inputs used for teacher-forced training, i.e. mel-specs\n",
    "#         RETURNS\n",
    "#         -------\n",
    "#         inputs: processed decoder inputs\n",
    "#         \"\"\"\n",
    "#         # (B, n_mel_channels, T_out) -> (B, T_out, n_mel_channels)\n",
    "#         decoder_inputs = decoder_inputs.transpose(1, 2)\n",
    "#         decoder_inputs = decoder_inputs.view(\n",
    "#             decoder_inputs.size(0),\n",
    "#             int(decoder_inputs.size(1)/self.n_frames_per_step), -1)\n",
    "#         # (B, T_out, n_mel_channels) -> (T_out, B, n_mel_channels)\n",
    "#         decoder_inputs = decoder_inputs.transpose(0, 1)\n",
    "#         return decoder_inputs\n",
    "\n",
    "#     def parse_decoder_outputs(self, mel_outputs, gate_outputs, alignments):\n",
    "#         \"\"\" Prepares decoder outputs for output\n",
    "#         PARAMS\n",
    "#         ------\n",
    "#         mel_outputs:\n",
    "#         gate_outputs: gate output energies\n",
    "#         alignments:\n",
    "#         RETURNS\n",
    "#         -------\n",
    "#         mel_outputs:\n",
    "#         gate_outpust: gate output energies\n",
    "#         alignments:\n",
    "#         \"\"\"\n",
    "#         # (T_out, B) -> (B, T_out)\n",
    "#         alignments = torch.stack(alignments).transpose(0, 1)\n",
    "#         # (T_out, B) -> (B, T_out)\n",
    "#         gate_outputs = torch.stack(gate_outputs).transpose(0, 1)\n",
    "#         gate_outputs = gate_outputs.contiguous()\n",
    "#         # (T_out, B, n_mel_channels) -> (B, T_out, n_mel_channels)\n",
    "#         mel_outputs = torch.stack(mel_outputs).transpose(0, 1).contiguous()\n",
    "#         # decouple frames per step\n",
    "#         mel_outputs = mel_outputs.view(\n",
    "#             mel_outputs.size(0), -1, self.n_mel_channels)\n",
    "#         # (B, T_out, n_mel_channels) -> (B, n_mel_channels, T_out)\n",
    "#         mel_outputs = mel_outputs.transpose(1, 2)\n",
    "\n",
    "#         return mel_outputs, gate_outputs, alignments\n",
    "\n",
    "#     def decode(self, decoder_input):\n",
    "#         \"\"\" Decoder step using stored states, attention and memory\n",
    "#         PARAMS\n",
    "#         ------\n",
    "#         decoder_input: previous mel output\n",
    "#         RETURNS\n",
    "#         -------\n",
    "#         mel_output:\n",
    "#         gate_output: gate output energies\n",
    "#         attention_weights:\n",
    "#         \"\"\"\n",
    "#         cell_input = torch.cat((decoder_input, self.attention_context), -1)\n",
    "#         self.attention_hidden, self.attention_cell = self.attention_rnn(\n",
    "#             cell_input, (self.attention_hidden, self.attention_cell))\n",
    "#         self.attention_hidden = F.dropout(\n",
    "#             self.attention_hidden, self.p_attention_dropout, self.training)\n",
    "\n",
    "#         attention_weights_cat = torch.cat(\n",
    "#             (self.attention_weights.unsqueeze(1),\n",
    "#              self.attention_weights_cum.unsqueeze(1)), dim=1)\n",
    "#         self.attention_context, self.attention_weights = self.attention_layer(\n",
    "#             self.attention_hidden, self.memory, self.processed_memory,\n",
    "#             attention_weights_cat, self.mask)\n",
    "\n",
    "#         self.attention_weights_cum += self.attention_weights\n",
    "#         decoder_input = torch.cat(\n",
    "#             (self.attention_hidden, self.attention_context), -1)\n",
    "#         self.decoder_hidden, self.decoder_cell = self.decoder_rnn(\n",
    "#             decoder_input, (self.decoder_hidden, self.decoder_cell))\n",
    "#         self.decoder_hidden = F.dropout(\n",
    "#             self.decoder_hidden, self.p_decoder_dropout, self.training)\n",
    "\n",
    "#         decoder_hidden_attention_context = torch.cat(\n",
    "#             (self.decoder_hidden, self.attention_context), dim=1)\n",
    "#         decoder_output = self.linear_projection(\n",
    "#             decoder_hidden_attention_context)\n",
    "\n",
    "#         gate_prediction = self.gate_layer(decoder_hidden_attention_context)\n",
    "#         return decoder_output, gate_prediction, self.attention_weights\n",
    "\n",
    "#     def forward(self, memory, decoder_inputs, memory_lengths):\n",
    "#         \"\"\" Decoder forward pass for training\n",
    "#         PARAMS\n",
    "#         ------\n",
    "#         memory: Encoder outputs\n",
    "#         decoder_inputs: Decoder inputs for teacher forcing. i.e. mel-specs\n",
    "#         memory_lengths: Encoder output lengths for attention masking.\n",
    "#         RETURNS\n",
    "#         -------\n",
    "#         mel_outputs: mel outputs from the decoder\n",
    "#         gate_outputs: gate outputs from the decoder\n",
    "#         alignments: sequence of attention weights from the decoder\n",
    "#         \"\"\"\n",
    "\n",
    "#         decoder_input = self.get_go_frame(memory).unsqueeze(0)\n",
    "#         decoder_inputs = self.parse_decoder_inputs(decoder_inputs)\n",
    "#         decoder_inputs = torch.cat((decoder_input, decoder_inputs), dim=0)\n",
    "#         decoder_inputs = self.prenet(decoder_inputs)\n",
    "\n",
    "#         self.initialize_decoder_states(\n",
    "#             memory, mask=~get_mask_from_lengths(memory_lengths))\n",
    "\n",
    "#         mel_outputs, gate_outputs, alignments = [], [], []\n",
    "#         while len(mel_outputs) < decoder_inputs.size(0) - 1:\n",
    "#             decoder_input = decoder_inputs[len(mel_outputs)]\n",
    "#             mel_output, gate_output, attention_weights = self.decode(\n",
    "#                 decoder_input)\n",
    "#             mel_outputs += [mel_output.squeeze(1)]\n",
    "#             gate_outputs += [gate_output.squeeze(1)]\n",
    "#             alignments += [attention_weights]\n",
    "\n",
    "#         mel_outputs, gate_outputs, alignments = self.parse_decoder_outputs(\n",
    "#             mel_outputs, gate_outputs, alignments)\n",
    "\n",
    "#         return mel_outputs, gate_outputs, alignments\n",
    "\n",
    "#     def inference(self, memory):\n",
    "#         \"\"\" Decoder inference\n",
    "#         PARAMS\n",
    "#         ------\n",
    "#         memory: Encoder outputs\n",
    "#         RETURNS\n",
    "#         -------\n",
    "#         mel_outputs: mel outputs from the decoder\n",
    "#         gate_outputs: gate outputs from the decoder\n",
    "#         alignments: sequence of attention weights from the decoder\n",
    "#         \"\"\"\n",
    "#         decoder_input = self.get_go_frame(memory)\n",
    "\n",
    "#         self.initialize_decoder_states(memory, mask=None)\n",
    "\n",
    "#         mel_outputs, gate_outputs, alignments = [], [], []\n",
    "#         while True:\n",
    "#             decoder_input = self.prenet(decoder_input)\n",
    "#             mel_output, gate_output, alignment = self.decode(decoder_input)\n",
    "\n",
    "#             mel_outputs += [mel_output.squeeze(1)]\n",
    "#             gate_outputs += [gate_output]\n",
    "#             alignments += [alignment]\n",
    "\n",
    "#             if torch.sigmoid(gate_output.data) > self.gate_threshold:\n",
    "#                 break\n",
    "#             elif len(mel_outputs) == self.max_decoder_steps:\n",
    "#                 print(\"Warning! Reached max decoder steps\")\n",
    "#                 break\n",
    "\n",
    "#             decoder_input = mel_output\n",
    "\n",
    "#         mel_outputs, gate_outputs, alignments = self.parse_decoder_outputs(\n",
    "#             mel_outputs, gate_outputs, alignments)\n",
    "\n",
    "#         return mel_outputs, gate_outputs, alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07df470c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class Postnet(nn.Module):\n",
    "    \"\"\"Postnet\n",
    "    - Five 1-d convolution with 512 channels and kernel size 5\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "        super(Postnet, self).__init__()\n",
    "        self.dropout_rate = 0.5\n",
    "        self.convolutions = nn.ModuleList()\n",
    "\n",
    "        self.convolutions.append(\n",
    "            nn.Sequential(\n",
    "                Conv1d(\n",
    "                    hparams.n_mel_channels,\n",
    "                    hparams.postnet_embedding_dim,\n",
    "                    kernel_size=hparams.postnet_kernel_size,\n",
    "                    stride=1,\n",
    "                    padding=int((hparams.postnet_kernel_size - 1) / 2),\n",
    "                    dilation=1,\n",
    "                    w_init_gain=\"tanh\",\n",
    "                ),\n",
    "                nn.BatchNorm1d(hparams.postnet_embedding_dim),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        for i in range(1, hparams.postnet_n_convolutions - 1):\n",
    "            self.convolutions.append(\n",
    "                nn.Sequential(\n",
    "                    Conv1d(\n",
    "                        hparams.postnet_embedding_dim,\n",
    "                        hparams.postnet_embedding_dim,\n",
    "                        kernel_size=hparams.postnet_kernel_size,\n",
    "                        stride=1,\n",
    "                        padding=int((hparams.postnet_kernel_size - 1) / 2),\n",
    "                        dilation=1,\n",
    "                        w_init_gain=\"tanh\",\n",
    "                    ),\n",
    "                    nn.BatchNorm1d(hparams.postnet_embedding_dim),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.convolutions.append(\n",
    "            nn.Sequential(\n",
    "                Conv1d(\n",
    "                    hparams.postnet_embedding_dim,\n",
    "                    hparams.n_mel_channels,\n",
    "                    kernel_size=hparams.postnet_kernel_size,\n",
    "                    stride=1,\n",
    "                    padding=int((hparams.postnet_kernel_size - 1) / 2),\n",
    "                    dilation=1,\n",
    "                    w_init_gain=\"linear\",\n",
    "                ),\n",
    "                nn.BatchNorm1d(hparams.n_mel_channels),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.convolutions) - 1):\n",
    "            x = F.dropout(\n",
    "                torch.tanh(self.convolutions[i](x)), self.dropout_rate, self.training\n",
    "            )\n",
    "        x = F.dropout(self.convolutions[-1](x), self.dropout_rate, self.training)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2038e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class Prenet(nn.Module):\n",
    "    def __init__(self, in_dim, sizes):\n",
    "        super().__init__()\n",
    "        in_sizes = [in_dim] + sizes[:-1]\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                LinearNorm(in_size, out_size, bias=False)\n",
    "                for (in_size, out_size) in zip(in_sizes, sizes)\n",
    "            ]\n",
    "        )\n",
    "        self.dropout_rate = 0.5\n",
    "\n",
    "    def forward(self, x):\n",
    "        for linear in self.layers:\n",
    "            x = F.dropout(F.relu(linear(x)), p=self.dropout_rate, training=True)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2697c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encoder module:\n",
    "    - Three 1-d convolution banks\n",
    "    - Bidirectional LSTM\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "\n",
    "        convolutions = []\n",
    "        for _ in range(hparams.encoder_n_convolutions):\n",
    "            conv_layer = nn.Sequential(\n",
    "                Conv1d(\n",
    "                    hparams.encoder_embedding_dim,\n",
    "                    hparams.encoder_embedding_dim,\n",
    "                    kernel_size=hparams.encoder_kernel_size,\n",
    "                    stride=1,\n",
    "                    padding=int((hparams.encoder_kernel_size - 1) / 2),\n",
    "                    dilation=1,\n",
    "                    w_init_gain=\"relu\",\n",
    "                ),\n",
    "                nn.BatchNorm1d(hparams.encoder_embedding_dim),\n",
    "            )\n",
    "            convolutions.append(conv_layer)\n",
    "        self.convolutions = nn.ModuleList(convolutions)\n",
    "        self.dropout_rate = 0.5\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            hparams.encoder_embedding_dim,\n",
    "            int(hparams.encoder_embedding_dim / 2),\n",
    "            1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, input_lengths):\n",
    "        if x.size()[0] > 1:\n",
    "            print(\"here\")\n",
    "            x_embedded = []\n",
    "            for b_ind in range(x.size()[0]):  # TODO: Speed up\n",
    "                curr_x = x[b_ind : b_ind + 1, :, : input_lengths[b_ind]].clone()\n",
    "                for conv in self.convolutions:\n",
    "                    curr_x = F.dropout(\n",
    "                        F.relu(conv(curr_x)), self.dropout_rate, self.training\n",
    "                    )\n",
    "                x_embedded.append(curr_x[0].transpose(0, 1))\n",
    "            x = torch.nn.utils.rnn.pad_sequence(x_embedded, batch_first=True)\n",
    "        else:\n",
    "            for conv in self.convolutions:\n",
    "                x = F.dropout(F.relu(conv(x)), self.dropout_rate, self.training)\n",
    "            x = x.transpose(1, 2)\n",
    "\n",
    "        # pytorch tensor are not reversible, hence the conversion\n",
    "        input_lengths = input_lengths.cpu().numpy()\n",
    "        x = nn.utils.rnn.pack_padded_sequence(\n",
    "            x, input_lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        self.lstm.flatten_parameters()\n",
    "        outputs, _ = self.lstm(x)\n",
    "\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
    "        return outputs\n",
    "\n",
    "    def inference(self, x, input_lengths):\n",
    "        device = x.device\n",
    "        for conv in self.convolutions:\n",
    "            x = F.dropout(F.relu(conv(x.to(device))), 0.5, self.training)\n",
    "\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        input_lengths = input_lengths.cpu()\n",
    "        x = nn.utils.rnn.pack_padded_sequence(\n",
    "            x, input_lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        outputs, _ = self.lstm(x)\n",
    "\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93bfd94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Tacotron2(TTSModel):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__(hparams)\n",
    "\n",
    "        self.mask_padding = hparams.mask_padding\n",
    "        self.fp16_run = hparams.fp16_run\n",
    "        self.pos_weight = hparams.pos_weight\n",
    "        self.n_mel_channels = hparams.n_mel_channels\n",
    "        self.n_frames_per_step_initial = hparams.n_frames_per_step_initial\n",
    "        self.n_frames_per_step_current = hparams.n_frames_per_step_initial\n",
    "        self.embedding = nn.Embedding(self.n_symbols, hparams.symbols_embedding_dim)\n",
    "        std = np.sqrt(2.0 / (self.n_symbols + hparams.symbols_embedding_dim))\n",
    "        val = np.sqrt(3.0) * std  # uniform bounds for std\n",
    "        self.embedding.weight.data.uniform_(-val, val)\n",
    "        self.encoder = Encoder(hparams)\n",
    "        self.decoder = Decoder(hparams)\n",
    "        self.postnet = Postnet(hparams)\n",
    "\n",
    "    def parse_batch(self, batch):\n",
    "        (\n",
    "            text_padded,\n",
    "            input_lengths,\n",
    "            mel_padded,\n",
    "            gate_padded,\n",
    "            output_lengths,\n",
    "            speaker_ids,\n",
    "            *_,\n",
    "        ) = batch\n",
    "\n",
    "        text_padded = to_gpu(text_padded).long()\n",
    "        input_lengths = to_gpu(input_lengths).long()\n",
    "        max_len = torch.max(input_lengths.data).item()\n",
    "        mel_padded = to_gpu(mel_padded).float()\n",
    "        gate_padded = to_gpu(gate_padded).float()\n",
    "        output_lengths = to_gpu(output_lengths).long()\n",
    "        ret_x = [\n",
    "            text_padded,\n",
    "            input_lengths,\n",
    "            mel_padded,\n",
    "            max_len,\n",
    "            output_lengths,\n",
    "            speaker_ids,\n",
    "        ]\n",
    "        return (\n",
    "            tuple(ret_x),\n",
    "            (mel_padded, gate_padded),\n",
    "        )\n",
    "\n",
    "    def parse_output(self, outputs, output_lengths=None):\n",
    "        if self.mask_padding and output_lengths is not None:\n",
    "            mask = ~get_mask_from_lengths(output_lengths)\n",
    "            mask = mask.expand(self.n_mel_channels, mask.size(0), mask.size(1))\n",
    "            mask = F.pad(mask, (0, outputs[0].size(2) - mask.size(2)))\n",
    "            mask = mask.permute(1, 0, 2)\n",
    "\n",
    "            outputs[0].data.masked_fill_(mask, 0.0)\n",
    "            outputs[1].data.masked_fill_(mask, 0.0)\n",
    "            outputs[2].data.masked_fill_(mask[:, 0, :], 1e3)  # gate energies\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        (\n",
    "            input_text,\n",
    "            input_lengths,\n",
    "            targets,\n",
    "            max_len,\n",
    "            output_lengths,\n",
    "            *_,\n",
    "        ) = inputs\n",
    "\n",
    "        input_lengths, output_lengths = input_lengths.data, output_lengths.data\n",
    "\n",
    "        embedded_inputs = self.embedding(input_text).transpose(1, 2)\n",
    "        embedded_text = self.encoder(embedded_inputs, input_lengths)\n",
    "\n",
    "        encoder_outputs = torch.cat((embedded_text,), dim=2)\n",
    "\n",
    "        mel_outputs, gate_outputs, alignments = self.decoder(\n",
    "            encoder_outputs, targets, memory_lengths=input_lengths\n",
    "        )\n",
    "\n",
    "        mel_outputs_postnet = self.postnet(mel_outputs)\n",
    "        mel_outputs_postnet = mel_outputs + mel_outputs_postnet\n",
    "\n",
    "        return self.parse_output(\n",
    "            [mel_outputs, mel_outputs_postnet, gate_outputs, alignments], output_lengths\n",
    "        )\n",
    "\n",
    "    def inference(self, inputs):\n",
    "        text, input_lengths, speaker_ids, *_ = inputs\n",
    "\n",
    "        embedded_inputs = self.embedding(text).transpose(1, 2)\n",
    "        embedded_text = self.encoder.inference(embedded_inputs, input_lengths)\n",
    "\n",
    "        encoder_outputs = torch.cat((embedded_text,), dim=2)\n",
    "\n",
    "        memory_lengths = input_lengths\n",
    "        mel_outputs, gate_outputs, alignments = self.decoder.inference(\n",
    "            encoder_outputs, memory_lengths\n",
    "        )\n",
    "        # mel_outputs, gate_outputs, alignments, mel_lengths = self.decoder.inference(encoder_outputs, memory_lengths)\n",
    "        mel_outputs_postnet = self.postnet(mel_outputs)\n",
    "        mel_outputs_postnet = mel_outputs + mel_outputs_postnet\n",
    "\n",
    "        return self.parse_output(\n",
    "            [mel_outputs, mel_outputs_postnet, gate_outputs, alignments]\n",
    "        )\n",
    "\n",
    "    def inference_noattention(self, inputs):\n",
    "        \"\"\"Run inference conditioned on an attention map.\n",
    "\n",
    "        NOTE(zach): I don't think it is necessary to do a version\n",
    "        of this without f0s passed as well, since it seems like we\n",
    "        would always want to condition on pitch when conditioning on rhythm.\n",
    "        \"\"\"\n",
    "        text, attention_map = inputs\n",
    "        embedded_inputs = self.embedding(text).transpose(1, 2)\n",
    "        embedded_text = self.encoder.inference(embedded_inputs)\n",
    "\n",
    "        encoder_outputs = torch.cat((embedded_text,), dim=2)\n",
    "\n",
    "        mel_outputs, gate_outputs, alignments = self.decoder.inference_noattention(\n",
    "            encoder_outputs, attention_map\n",
    "        )\n",
    "\n",
    "        mel_outputs_postnet = self.postnet(mel_outputs)\n",
    "        mel_outputs_postnet = mel_outputs + mel_outputs_postnet\n",
    "\n",
    "        return self.parse_output(\n",
    "            [mel_outputs, mel_outputs_postnet, gate_outputs, alignments]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9d07eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uberduck_ml_dev.text.symbols import (\n",
    "    DEFAULT_SYMBOLS,\n",
    "    IPA_SYMBOLS,\n",
    "    NVIDIA_TACO2_SYMBOLS,\n",
    "    GRAD_TTS_SYMBOLS,\n",
    ")\n",
    "\n",
    "\n",
    "def pad_sequences(batch):\n",
    "    input_lengths = torch.LongTensor([len(x) for x in batch])\n",
    "    max_input_len = input_lengths.max()\n",
    "\n",
    "    text_padded = torch.LongTensor(len(batch), max_input_len)\n",
    "    text_padded.zero_()\n",
    "    for i in range(len(batch)):\n",
    "        text = batch[i]\n",
    "        text_padded[i, : text.size(0)] = text\n",
    "\n",
    "    return text_padded, input_lengths\n",
    "\n",
    "\n",
    "def prepare_input_sequence(\n",
    "    texts, cpu_run=False, arpabet=False, symbol_set=NVIDIA_TACO2_SYMBOLS\n",
    "):\n",
    "    p_arpabet = float(arpabet)\n",
    "    seqs = []\n",
    "    for text in texts:\n",
    "        seqs.append(\n",
    "            torch.IntTensor(\n",
    "                text_to_sequence(\n",
    "                    text,\n",
    "                    [\"english_cleaners\"],\n",
    "                    p_arpabet=p_arpabet,\n",
    "                    symbol_set=symbol_set,\n",
    "                )[:]\n",
    "            )\n",
    "        )\n",
    "    text_padded, input_lengths = pad_sequences(seqs)\n",
    "    if not cpu_run:\n",
    "        text_padded = text_padded.cuda().long()\n",
    "        input_lengths = input_lengths.cuda().long()\n",
    "    else:\n",
    "        text_padded = text_padded.long()\n",
    "        input_lengths = input_lengths.long()\n",
    "\n",
    "    return text_padded, input_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3066963a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"/mnt/disks/uberduck-experiments-v0/uberduck-ml-exp/configs/old/taco2_lj_wsnvidia.json\"\n",
    "from uberduck_ml_dev.models.tacotron2 import DEFAULTS as TACOTRON2_DEFAULTS\n",
    "import json\n",
    "from uberduck_ml_dev.text.util import text_to_sequence\n",
    "from uberduck_ml_dev.vendor.tfcompat.hparam import HParams\n",
    "import numpy as np\n",
    "import torch\n",
    "from uberduck_ml_dev.utils.audio import mel_to_audio\n",
    "from uberduck_ml_dev.models.common import MelSTFT\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "905d22f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TACOTRON2_DEFAULTS.values()\n",
    "with open(config_file) as f:\n",
    "    config.update(json.load(f))\n",
    "hparams = HParams(**config)\n",
    "hparams.warm_start_name = (\n",
    "    \"/mnt/disks/uberduck-experiments-v0/models/tacotron2_statedict.pt\"\n",
    ")\n",
    "symbol_set = \"nvidia_taco2\"\n",
    "device = \"cuda\"\n",
    "hparams.symbol_set = symbol_set\n",
    "hparams.device = \"cuda\"\n",
    "model = Tacotron2(hparams)\n",
    "model = model.cuda()\n",
    "model.from_pretrained(warm_start_path=hparams.warm_start_name, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7ca07c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5c47bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"hello i am sam\", \"Long beach compton.\"]\n",
    "# texts = [\"hello again its me sam im back\", \"Long beach compton.\"]\n",
    "# speaker_ids = [0,0]\n",
    "# texts = [\"hello again its me sam\"]\n",
    "speaker_ids = [0, 0]\n",
    "\n",
    "text_padded, input_lengths = prepare_input_sequence(\n",
    "    texts, cpu_run=False, arpabet=False, symbol_set=symbol_set\n",
    ")\n",
    "input_ = text_padded, input_lengths, speaker_ids\n",
    "\n",
    "# data0 = [\"i am yes\", 0]\n",
    "# data1 = [\"i am not\", 0]\n",
    "# to_infer = [data0, data1]\n",
    "# to_infer_batch = Collate()._to_inference(to_infer)\n",
    "# tacotron wants input = sequence, speakerid, length\n",
    "output = model.inference(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efe756e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14, 19], device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5e4102d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14, 19], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ccf160dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[45, 42, 49, 49, 52, 11, 46, 11, 38, 50, 11, 56, 38, 50,  0,  0,  0,  0,\n",
       "          0],\n",
       "        [49, 52, 51, 44, 11, 39, 42, 38, 40, 45, 11, 40, 52, 50, 53, 57, 52, 51,\n",
       "          7]], device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f611833b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" >\n",
       "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_mel = mel_to_audio(mel=output[0][1], algorithm=\"griffin-lim\")\n",
    "ipd.Audio(post_mel.numpy(), rate=22050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d937e217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 80, 125])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "42929493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         False, False, False, False, False, False, False, False, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.decoder.inference(encoder_outputs, memory_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cbdcae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f26e953",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a39c400",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adac19b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6ee403",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869e92e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#these classes are under development\n",
    "#make a class containing e.g. texts, sequences, that can be read as a batch in either forward passes and inference\n",
    "#create lists for particular off-the-shelf models?\n",
    "\n",
    "class TTSDataset(Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        audiopaths_and_text: str,\n",
    "        text_cleaners: List[str],\n",
    "        p_arpabet: float,\n",
    "        n_mel_channels: int,\n",
    "        sample_rate: int,\n",
    "        mel_fmin: float,\n",
    "        mel_fmax: float,\n",
    "        filter_length: int,\n",
    "        hop_length: int,\n",
    "        padding: int,\n",
    "        win_length: int,\n",
    "        symbol_set: str,\n",
    "        max_wav_value: float = 32768.0,\n",
    "        include_f0: bool = False,\n",
    "        pos_weight: float = 10,\n",
    "        f0_min: int = 80,\n",
    "        f0_max: int = 880,\n",
    "        harmonic_thresh=0.25,\n",
    "        debug: bool = False,\n",
    "        debug_dataset_size: int = None,\n",
    "        oversample_weights=None,\n",
    "        intersperse_text=False,\n",
    "        intersperse_token=0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        #oversample\n",
    "        path = audiopaths_and_text\n",
    "        oversample_weights = oversample_weights or {}\n",
    "        self.audiopaths_and_text = oversample(\n",
    "            load_filepaths_and_text(path), oversample_weights\n",
    "        )\n",
    "        \n",
    "        #text to seq parameters\n",
    "        self.text_cleaners = text_cleaners\n",
    "        self.p_arpabet = p_arpabet\n",
    "        self.intersperse_text = intersperse_text\n",
    "        self.intersperse_token = intersperse_token\n",
    "        \n",
    "        self.stft = MelSTFT(\n",
    "            filter_length=filter_length,\n",
    "            hop_length=hop_length,\n",
    "            win_length=win_length,\n",
    "            n_mel_channels=n_mel_channels,\n",
    "            sampling_rate=sample_rate,\n",
    "            mel_fmin=mel_fmin,\n",
    "            mel_fmax=mel_fmax,\n",
    "            padding=padding,\n",
    "        )\n",
    "        self.max_wav_value = max_wav_value\n",
    "        self.sample_rate = sample_rate\n",
    "        self.filter_length = filter_length\n",
    "        self.hop_length = hop_length\n",
    "        self.mel_fmin = mel_fmin\n",
    "        self.mel_fmax = mel_fmax\n",
    "        self.include_f0 = include_f0\n",
    "        self.f0_min = f0_min\n",
    "        self.f0_max = f0_max\n",
    "        self.harmonic_threshold = harmonic_thresh\n",
    "        # speaker id lookup table\n",
    "        speaker_ids = [i[2] for i in self.audiopaths_and_text]\n",
    "        self.symbol_set = symbol_set\n",
    "\n",
    "\n",
    "    def _get_f0(self, audio):\n",
    "        f0, harmonic_rates, argmins, times = compute_yin(\n",
    "            audio,\n",
    "            self.sample_rate,\n",
    "            self.filter_length,\n",
    "            self.hop_length,\n",
    "            self.f0_min,\n",
    "            self.f0_max,\n",
    "            self.harmonic_threshold,\n",
    "        )\n",
    "        pad = int((self.filter_length / self.hop_length) / 2)\n",
    "        f0 = [0.0] * pad + f0 + [0.0] * pad\n",
    "        f0 = np.array(f0, dtype=np.float32)\n",
    "        return f0\n",
    "\n",
    "    def _text_to_seq(self, audiopath_and_text):\n",
    "        path, transcription, speaker_id = audiopath_and_text\n",
    "        text_sequence = torch.LongTensor(\n",
    "            text_to_sequence(\n",
    "                transcription,\n",
    "                self.text_cleaners,\n",
    "                p_arpabet=self.p_arpabet,\n",
    "                symbol_set=self.symbol_set,\n",
    "            )\n",
    "        )\n",
    "        if self.intersperse_text:\n",
    "            text_sequence = torch.LongTensor(\n",
    "                intersperse(text_sequence.numpy(), self.intersperse_token)\n",
    "            )  # add a blank token, whose id number is len(symbols)\n",
    "        return(text_to_sequence)\n",
    "\n",
    "       \n",
    "    def _get_f0(self,audio):\n",
    "        \n",
    "        if self.include_f0:\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def _get_mel(self,):\n",
    "        \n",
    "    def _get_data(self, audiopath_and_text):\n",
    "\n",
    "        sequence = self._text_to_seq(audiopath_and_text)\n",
    "        audio = self._get_audio(audiopath_and_text)\n",
    "        melspec = self._get_mel(audio)\n",
    "        f0 = self._get_f0(audio)\n",
    "        speaker_id = self._get_sid(audiopath_and_text)\n",
    "\n",
    "        return (text_sequence, melspec, speaker_id, f0)\n",
    "    \n",
    "class Collate:\n",
    "    '''\n",
    "    Collate assembles batches from list indexed by sample id\n",
    "    text, spectragram, etc'''\n",
    "    def __init__(**args):\n",
    "        \n",
    "        n_frames_per_step: int = 1, \n",
    "        include_f0: bool = False,\n",
    "        include_sid: bool = False,\n",
    "        batch_format: str\n",
    "            \n",
    "    def _pad_sequence(self, batch):\n",
    "        \n",
    "        batch_size = len(batch)\n",
    "        input_lengths = [len(x[0].shape[1]) for x in batch]\n",
    "        max_input_len = input_lengths.max()\n",
    "        text_padded = torch.LongTensor(batch_size, max_input_len)\n",
    "        text_padded.zero_()\n",
    "        for i in range(batch_size):\n",
    "            text_padded[i, : batch[0][i].shape[1]] = batch[0][i]\n",
    "            \n",
    "        return(text_padded)\n",
    "\n",
    "    def _pad_mel(self, batch):\n",
    "        \n",
    "        batch_size = len(batch)\n",
    "        target_lengths = [len(x[0].shape[1]) for x in batch]\n",
    "        max_target_len = max(target_len)\n",
    "        textint_padded = torch.LongTensor(batch_size, max_input_len)\n",
    "        textint_padded.zero_()\n",
    "        for i in range(batch_size):\n",
    "            textint_padded[i, : batch[0][i].shape[1]] = batch[0][i]\n",
    "            \n",
    "        #assert len(f0) = len(mel)\n",
    "        return(text_padded)\n",
    "    \n",
    "    def _pad_f0(self, batch):\n",
    "        return(None)\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "\n",
    "        text_padded, input_lengths = _pad_sequence(batch) #idx\n",
    "        mel_padded, gate_padded, output_lengths = _pad_mel(batch)\n",
    "        f0 = None\n",
    "        batch = Batch(text = text_padded,\n",
    "                     input_lengths = input_lengths,\n",
    "                     mel_padded = mel_padded,\n",
    "                     gate_padded = gate_padded,\n",
    "                     output_lengths = output_lengths\n",
    "                     f0 = f0,\n",
    "                     speaker_ids = speaker_ids)\n",
    "\n",
    "        if batch_format == 'taco2ss':\n",
    "            return(text_padded, mel_padded, mel_padded, output_lengths, input_lengths)\n",
    "        if batch_format == 'taco2ms':\n",
    "            \n",
    "        return(batch)  \n",
    "\n",
    "    def inference(self, batch):\n",
    "        \n",
    "        if batch_format == 'taco2ss':\n",
    "            return(self.text_padded, self.input_lengths)\n",
    "        if batch_format == 'taco2ms':\n",
    "            return(self.text_padded, self.input_lengths)\n",
    "    #need to have pad_sequences equivalent\n",
    "    def _to_tacotron2_singlespeaker_inference(self, batch):\n",
    "        \n",
    "        text_padded, input_lengths = _pad_sequence(batch_list) #idx\n",
    "        mel_padded, gate_padded, output_lengths = _pad_mel(batch_list)        \n",
    "        return(self.text_padded, self.input_lengths)\n",
    "\n",
    "    # NOTE(zach): would model_inputs be better as a namedtuple or dataclass?\n",
    "    def _to_mellotron_train_f0():\n",
    "        \n",
    "        batch = Batch\n",
    "        return(\n",
    "            text_padded,\n",
    "            input_lengths,\n",
    "            mel_padded,\n",
    "            gate_padded,\n",
    "            output_lengths,\n",
    "            speaker_ids,\n",
    "            f0_padded,\n",
    "        )\n",
    "    if self.include_f0:\n",
    "        model_inputs = \n",
    "    else:\n",
    "        model_inputs = (\n",
    "            text_padded,\n",
    "            input_lengths,\n",
    "            mel_padded,\n",
    "            gate_padded,\n",
    "            output_lengths,\n",
    "            speaker_ids,\n",
    "        )\n",
    "\n",
    "\n",
    "    def _to_tacotron2_multispeaker_inference(batch):\n",
    "\n",
    "        input_lengths, ids_sorted_decreasing = torch.sort(\n",
    "            torch.LongTensor([len(x) for x in batch]), dim=0, descending=True\n",
    "        )\n",
    "        max_input_len = input_lengths[0]\n",
    "\n",
    "        text_padded = torch.LongTensor(len(batch), max_input_len)\n",
    "        text_padded.zero_()\n",
    "        for i in range(len(ids_sorted_decreasing)):\n",
    "            text_padded[i, : text.size(0)] = batch[i]\n",
    "\n",
    "        return(self.text_padded, self.speakers, self.input_lengths, self.sort_indices)\n",
    "    \n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Batch:\n",
    "    textint_padded: torch.Tensor,\n",
    "    input_lengths: list\n",
    "    mel_padded: torch.Tensor\n",
    "    gate_padded: \n",
    "    output_length: list,\n",
    "    speaker_ids: list,\n",
    "    f0_padded: list,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3611bac2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ac3d7c5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "13393a6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "075b5441",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2ef1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TACOTRON2_DEFAULTS.values()\n",
    "with open(config_file) as f:\n",
    "    config.update(json.load(f))\n",
    "hparams = HParams(**config)\n",
    "hparams.warm_start_name = \"/mnt/disks/uberduck-experiments-v0/models/taco2ljdefault\"\n",
    "model = Tacotron2(hparams)\n",
    "model = model.cuda()\n",
    "device = \"cuda\"\n",
    "symbol_set = \"nvidia_taco2\"\n",
    "model.from_pretrained(warm_start_path=hparams.warm_start_name, device=device)\n",
    "# for inference\n",
    "sequence1 = text_to_sequence(\n",
    "    \"You can see he is passionate about the company and what they are doing.\",\n",
    "    [\"english_cleaners\"],\n",
    "    1.0,\n",
    "    symbol_set=symbol_set,\n",
    ")\n",
    "sequence2 = text_to_sequence(\n",
    "    \"Long beach compton.\", [\"english_cleaners\"], 1.0, symbol_set=symbol_set\n",
    ")\n",
    "# texts = [\"hello i am sam.\"]#,\"Long beach compton.\"]\n",
    "texts = [\"hello again its me sam\", \"Long beach compton.\"]\n",
    "# print(torch.LongTensor(sequence)[None, :].shape)\n",
    "sequence = torch.LongTensor(sequence1)[None].cuda()\n",
    "input_ = [sequence, 0]\n",
    "text_padded, input_lengths, sort_indices = prepare_input_sequence(\n",
    "    texts, cpu_run=False, arpabet=True, symbol_set=symbol_set\n",
    ")\n",
    "speaker_ids = [0, 0]\n",
    "input_ = text_padded, input_lengths, speaker_ids\n",
    "\n",
    "data0 = [\"i am yes\", 0]\n",
    "data1 = [\"i am not\", 0]\n",
    "to_infer = [data0, data1]\n",
    "# to_infer_batch = Collate()._to_inference(to_infer)\n",
    "\n",
    "# tacotron wants input = sequence, speakerid, length\n",
    "output = model.inference(input_)\n",
    "output[0].shape\n",
    "import torch\n",
    "\n",
    "# input2 = torch.cat([input_[0],input_[0]], axis = 0)\n",
    "# input2_ = [input2, [0,0]]\n",
    "# output = model.inference(input2_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
