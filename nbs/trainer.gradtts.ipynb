{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e21cc08",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Trainer\" data-toc-modified-id=\"Trainer-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Trainer</a></span></li><li><span><a href=\"#Grad-TTS-Trainer\" data-toc-modified-id=\"Grad-TTS-Trainer-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Grad TTS Trainer</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535085d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp trainer.gradtts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653d4f95",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47500540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.distributed as dist\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "\n",
    "from uberduck_ml_dev.models.common import MelSTFT\n",
    "from uberduck_ml_dev.utils.plot import (\n",
    "    plot_attention,\n",
    "    plot_gate_outputs,\n",
    "    plot_spectrogram,\n",
    "    plot_tensor,\n",
    ")\n",
    "from uberduck_ml_dev.text.util import text_to_sequence, random_utterance\n",
    "from uberduck_ml_dev.text.symbols import symbols_with_ipa\n",
    "from uberduck_ml_dev.trainer.base import TTSTrainer\n",
    "\n",
    "from uberduck_ml_dev.data_loader import (\n",
    "    TextAudioSpeakerLoader,\n",
    "    TextMelCollate,\n",
    "    DistributedBucketSampler,\n",
    "    TextMelDataset,\n",
    ")\n",
    "from uberduck_ml_dev.vendor.tfcompat.hparam import HParams\n",
    "from uberduck_ml_dev.utils.plot import save_figure_to_numpy, plot_spectrogram\n",
    "from uberduck_ml_dev.utils.utils import slice_segments, clip_grad_value_\n",
    "from uberduck_ml_dev.text.symbols import SYMBOL_SETS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b34fa71",
   "metadata": {},
   "source": [
    "# Grad TTS Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e61d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from tqdm import tqdm\n",
    "from uberduck_ml_dev.text.util import text_to_sequence, random_utterance\n",
    "from uberduck_ml_dev.models.gradtts import (\n",
    "    GradTTS,\n",
    ")\n",
    "from uberduck_ml_dev.utils.utils import intersperse\n",
    "\n",
    "\n",
    "class GradTTSTrainer(TTSTrainer):\n",
    "    REQUIRED_HPARAMS = [\n",
    "        \"training_audiopaths_and_text\",\n",
    "        \"test_audiopaths_and_text\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        for param in self.REQUIRED_HPARAMS:\n",
    "            if not hasattr(self, param):\n",
    "                raise Exception(f\"GradTTSTrainer missing a required param: {param}\")\n",
    "        self.sampling_rate = self.hparams.sampling_rate\n",
    "        self.checkpoint_path = self.hparams.log_dir\n",
    "\n",
    "    def sample_inference(self, model, timesteps=10, spk=None):\n",
    "        with torch.no_grad():\n",
    "            sequence = text_to_sequence(\n",
    "                random_utterance(),\n",
    "                self.text_cleaners,\n",
    "                1.0,\n",
    "                symbol_set=self.hparams.symbol_set,\n",
    "            )\n",
    "            if self.hparams.intersperse_text:\n",
    "                sequence = intersperse(\n",
    "                    sequence, (len(SYMBOL_SETS[self.hparams.symbol_set]))\n",
    "                )\n",
    "            x = torch.LongTensor(sequence).cuda()[None]\n",
    "            x_lengths = torch.LongTensor([x.shape[-1]]).cuda()\n",
    "            y_enc, y_dec, attn = model(\n",
    "                x,\n",
    "                x_lengths,\n",
    "                n_timesteps=50,\n",
    "                temperature=1.5,\n",
    "                stoc=False,\n",
    "                spk=spk,\n",
    "                length_scale=0.91,\n",
    "            )\n",
    "            if self.hparams.vocoder_algorithm == \"hifigan\":\n",
    "                audio = self.sample(\n",
    "                    y_dec,\n",
    "                    algorithm=self.hparams.vocoder_algorithm,\n",
    "                    hifigan_config=self.hparams.hifigan_config,\n",
    "                    hifigan_checkpoint=self.hparams.hifigan_checkpoint,\n",
    "                    cudnn_enabled=self.hparams.cudnn_enabled,\n",
    "                )\n",
    "            else:\n",
    "                audio = self.sample(y_dec.cpu()[0])\n",
    "            return audio\n",
    "\n",
    "    def train(self, checkpoint=None):\n",
    "        if self.distributed_run:\n",
    "            self.init_distributed()\n",
    "\n",
    "        train_dataset = TextMelDataset(\n",
    "            self.hparams.training_audiopaths_and_text,\n",
    "            self.hparams.text_cleaners,\n",
    "            1.0,\n",
    "            self.hparams.n_feats,\n",
    "            self.hparams.sampling_rate,\n",
    "            self.hparams.mel_fmin,\n",
    "            self.hparams.mel_fmax,\n",
    "            self.hparams.filter_length,\n",
    "            self.hparams.hop_length,\n",
    "            (self.hparams.filter_length - self.hparams.hop_length) // 2,\n",
    "            self.hparams.win_length,\n",
    "            intersperse_text=self.hparams.intersperse_text,\n",
    "            intersperse_token=(len(SYMBOL_SETS[self.hparams.symbol_set])),\n",
    "            symbol_set=self.hparams.symbol_set,\n",
    "        )\n",
    "        collate_fn = TextMelCollate()\n",
    "\n",
    "        loader = DataLoader(\n",
    "            dataset=train_dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            collate_fn=collate_fn,\n",
    "            drop_last=True,\n",
    "            num_workers=0,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "        test_dataset = TextMelDataset(\n",
    "            self.hparams.test_audiopaths_and_text,\n",
    "            self.hparams.text_cleaners,\n",
    "            1.0,\n",
    "            self.hparams.n_feats,\n",
    "            self.hparams.sampling_rate,\n",
    "            self.hparams.mel_fmin,\n",
    "            self.hparams.mel_fmax,\n",
    "            self.hparams.filter_length,\n",
    "            self.hparams.hop_length,\n",
    "            (self.hparams.filter_length - self.hparams.hop_length) // 2,\n",
    "            self.hparams.win_length,\n",
    "            intersperse_text=self.hparams.intersperse_text,\n",
    "            intersperse_token=(len(SYMBOL_SETS[self.hparams.symbol_set])),\n",
    "            symbol_set=self.hparams.symbol_set,\n",
    "        )\n",
    "\n",
    "        model = GradTTS(self.hparams)\n",
    "\n",
    "        if self.hparams.checkpoint:\n",
    "            model.load_state_dict(torch.load(self.hparams.checkpoint))\n",
    "        model = model.cuda()\n",
    "\n",
    "        print(\n",
    "            \"Number of encoder + duration predictor parameters: %.2fm\"\n",
    "            % (model.encoder.nparams / 1e6)\n",
    "        )\n",
    "        print(\"Number of decoder parameters: %.2fm\" % (model.decoder.nparams / 1e6))\n",
    "        print(\"Total parameters: %.2fm\" % (model.nparams / 1e6))\n",
    "\n",
    "        print(\"Initializing optimizer...\")\n",
    "        optimizer = torch.optim.Adam(\n",
    "            params=model.parameters(), lr=self.hparams.learning_rate\n",
    "        )\n",
    "        test_batch = test_dataset.sample_test_batch(size=self.hparams.test_size)\n",
    "        for i, item in enumerate(test_batch):\n",
    "            text, mel, spk = item\n",
    "            self.log(\n",
    "                f\"image_{i}/ground_truth\",\n",
    "                0,\n",
    "                image=plot_tensor(mel.squeeze()),\n",
    "            )\n",
    "        iteration = 0\n",
    "        last_time = time.time()\n",
    "        for epoch in range(0, self.hparams.n_epochs):\n",
    "            model.train()\n",
    "            dur_losses = []\n",
    "            prior_losses = []\n",
    "            diff_losses = []\n",
    "            for batch_idx, batch in enumerate(loader):\n",
    "                model.zero_grad()\n",
    "                x, x_lengths, y, _, y_lengths, speaker_ids = batch\n",
    "\n",
    "                dur_loss, prior_loss, diff_loss = model.compute_loss(\n",
    "                    x, x_lengths, y, y_lengths, out_size=self.hparams.out_size\n",
    "                )\n",
    "                loss = sum([dur_loss, prior_loss, diff_loss])\n",
    "                loss.backward()\n",
    "\n",
    "                enc_grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                    model.encoder.parameters(), max_norm=1\n",
    "                )\n",
    "                dec_grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                    model.decoder.parameters(), max_norm=1\n",
    "                )\n",
    "                optimizer.step()\n",
    "\n",
    "                self.log(\"training/duration_loss\", iteration, dur_loss.item())\n",
    "                self.log(\"training/prior_loss\", iteration, prior_loss.item())\n",
    "                self.log(\"training/diffusion_loss\", iteration, diff_loss.item())\n",
    "                self.log(\"training/encoder_grad_norm\", iteration, enc_grad_norm)\n",
    "                self.log(\"training/decoder_grad_norm\", iteration, dec_grad_norm)\n",
    "\n",
    "                dur_losses.append(dur_loss.item())\n",
    "                prior_losses.append(prior_loss.item())\n",
    "                diff_losses.append(diff_loss.item())\n",
    "\n",
    "                iteration += 1\n",
    "\n",
    "            log_msg = f\"Epoch {epoch}, iter: {iteration}: dur_loss: {np.mean(dur_losses):.4f} | prior_loss: {np.mean(prior_losses):.4f} | diff_loss: {np.mean(diff_losses):.4f} | time: {time.time()-last_time:.2f}s\"\n",
    "            last_time = time.time()\n",
    "            with open(f\"{self.hparams.log_dir}/train.log\", \"a\") as f:\n",
    "                f.write(log_msg + \"\\n\")\n",
    "                print(log_msg)\n",
    "\n",
    "            if epoch % self.log_interval == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    for i, item in enumerate(test_batch):\n",
    "                        x, _y, _speaker_id = item\n",
    "                        x = x.to(torch.long).unsqueeze(0)\n",
    "                        x_lengths = torch.LongTensor([x.shape[-1]])\n",
    "                        y_enc, y_dec, attn = model(x, x_lengths, n_timesteps=50)\n",
    "                        self.log(\n",
    "                            f\"image_{i}/generated_enc\",\n",
    "                            iteration,\n",
    "                            image=plot_tensor(y_enc.squeeze().cpu()),\n",
    "                        )\n",
    "                        self.log(\n",
    "                            f\"image_{i}/generated_dec\",\n",
    "                            iteration,\n",
    "                            image=plot_tensor(y_dec.squeeze().cpu()),\n",
    "                        )\n",
    "                        self.log(\n",
    "                            f\"image_{i}/alignment\",\n",
    "                            iteration,\n",
    "                            image=plot_tensor(attn.squeeze().cpu()),\n",
    "                        )\n",
    "                        self.log(\n",
    "                            f\"audio/inference_{i}\",\n",
    "                            iteration,\n",
    "                            audio=self.sample_inference(model),\n",
    "                        )\n",
    "\n",
    "            if epoch % self.save_every == 0:\n",
    "                torch.save(\n",
    "                    model.state_dict(), f=f\"{self.hparams.log_dir}/grad_{epoch}.pt\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb6e9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTSTrainer start 1358715.628787123\n",
      "Initializing trainer with hparams:\n",
      "{'batch_size': 1,\n",
      " 'beta_max': 20.0,\n",
      " 'beta_min': 0.05,\n",
      " 'checkpoint': None,\n",
      " 'cudnn_enabled': True,\n",
      " 'dec_dim': 64,\n",
      " 'distributed_run': False,\n",
      " 'enc_dropout': 0.1,\n",
      " 'enc_kernel': 3,\n",
      " 'filter_channels': 768,\n",
      " 'filter_channels_dp': 256,\n",
      " 'filter_length': 1024,\n",
      " 'hop_length': 256,\n",
      " 'intersperse_text': True,\n",
      " 'learning_rate': 0.0001,\n",
      " 'log_dir': 'output',\n",
      " 'log_interval': 100,\n",
      " 'max_wav_value': 32768.0,\n",
      " 'mel_fmax': 8000,\n",
      " 'mel_fmin': 0.0,\n",
      " 'n_enc_channels': 192,\n",
      " 'n_enc_layers': 6,\n",
      " 'n_epochs': 10000,\n",
      " 'n_feats': 80,\n",
      " 'n_heads': 2,\n",
      " 'n_spks': 1,\n",
      " 'out_size': 172,\n",
      " 'oversample_weights': None,\n",
      " 'pe_scale': 1000,\n",
      " 'rank': 0,\n",
      " 'sampling_rate': 22050,\n",
      " 'save_every': 1000,\n",
      " 'seed': 37,\n",
      " 'spk_emb_dim': 64,\n",
      " 'symbol_set': 'gradtts',\n",
      " 'test_audiopaths_and_text': 'val.txt',\n",
      " 'test_size': 2,\n",
      " 'text_cleaners': ['english_cleaners'],\n",
      " 'training_audiopaths_and_text': 'train.txt',\n",
      " 'win_length': 1024,\n",
      " 'window_size': 4}\n"
     ]
    }
   ],
   "source": [
    "DEFAULTS = HParams(\n",
    "    training_audiopaths_and_text=\"train.txt\",\n",
    "    test_audiopaths_and_text=\"val.txt\",\n",
    "    cudnn_enabled=True,\n",
    "    log_dir=\"output\",\n",
    "    symbol_set=\"gradtts\",\n",
    "    intersperse_text=True,\n",
    "    n_spks=1,\n",
    "    spk_emb_dim=64,\n",
    "    sampling_rate=22050,\n",
    "    hop_length=256,\n",
    "    win_length=1024,\n",
    "    n_enc_channels=192,\n",
    "    filter_channels=768,\n",
    "    filter_channels_dp=256,\n",
    "    n_enc_layers=6,\n",
    "    enc_kernel=3,\n",
    "    enc_dropout=0.1,\n",
    "    n_heads=2,\n",
    "    window_size=4,\n",
    "    dec_dim=64,\n",
    "    beta_min=0.05,\n",
    "    beta_max=20.0,\n",
    "    pe_scale=1000,\n",
    "    test_size=2,\n",
    "    n_epochs=10000,\n",
    "    batch_size=1,\n",
    "    learning_rate=1e-4,\n",
    "    seed=37,\n",
    "    out_size=2 * 22050 // 256,\n",
    "    filter_length=1024,\n",
    "    rank=0,\n",
    "    distributed_run=False,\n",
    "    oversample_weights=None,\n",
    "    text_cleaners=[\"english_cleaners\"],\n",
    "    max_wav_value=32768.0,\n",
    "    n_feats=80,\n",
    "    mel_fmax=8000,\n",
    "    mel_fmin=0.0,\n",
    "    checkpoint=None,\n",
    "    log_interval=100,\n",
    "    save_every=1000,\n",
    ")\n",
    "trainer = GradTTSTrainer(DEFAULTS, rank=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ca56c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-ffeaf8a53149>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, checkpoint)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mintersperse_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersperse_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mintersperse_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSYMBOL_SETS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbol_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0msymbol_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbol_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         )\n\u001b[1;32m     79\u001b[0m         \u001b[0mcollate_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextMelCollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/uberduck-ml-dev/uberduck_ml_dev/data_loader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, audiopaths_and_text, text_cleaners, p_arpabet, n_mel_channels, sample_rate, mel_fmin, mel_fmax, filter_length, hop_length, padding, win_length, symbol_set, max_wav_value, include_f0, pos_weight, f0_min, f0_max, harmonic_thresh, debug, debug_dataset_size, oversample_weights, intersperse_text, intersperse_token)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0moversample_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moversample_weights\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         self.audiopaths_and_text = oversample(\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0mload_filepaths_and_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moversample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         )\n\u001b[1;32m     87\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_cleaners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_cleaners\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/uberduck-ml-dev/uberduck_ml_dev/utils/utils.py\u001b[0m in \u001b[0;36mload_filepaths_and_text\u001b[0;34m(filename, split)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_filepaths_and_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"|\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mfilepaths_and_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfilepaths_and_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.txt'"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63e750f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec332df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b836c787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2efcf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
