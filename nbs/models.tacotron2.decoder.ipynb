{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831d7bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.tacotron2.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb2cc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import pdb\n",
    "from torch import nn\n",
    "from uberduck_ml_dev.models.base import TTSModel\n",
    "from uberduck_ml_dev.models.common import Attention, Conv1d, LinearNorm, GST\n",
    "from uberduck_ml_dev.text.symbols import symbols\n",
    "from uberduck_ml_dev.vendor.tfcompat.hparam import HParams\n",
    "from uberduck_ml_dev.utils.utils import to_gpu, get_mask_from_lengths\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.nn import functional as F\n",
    "from uberduck_ml_dev.utils.duration import (\n",
    "    GaussianUpsampling,\n",
    "    RangePredictor,\n",
    "    PositionalEncoding,\n",
    "    DurationPredictor,\n",
    ")\n",
    "from uberduck_ml_dev.models.tacotron2.prenet import Prenet\n",
    "from einops import rearrange\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.n_mel_channels = hparams.n_mel_channels\n",
    "        self.n_frames_per_step_initial = hparams.n_frames_per_step_initial\n",
    "        self.n_frames_per_step_current = hparams.n_frames_per_step_initial\n",
    "        self.encoder_embedding_dim = hparams.encoder_embedding_dim\n",
    "        self.attention_rnn_dim = hparams.attention_rnn_dim\n",
    "        self.decoder_rnn_dim = hparams.decoder_rnn_dim\n",
    "        self.prenet_dim = hparams.prenet_dim\n",
    "        self.max_decoder_steps = hparams.max_decoder_steps\n",
    "        self.gate_threshold = hparams.gate_threshold\n",
    "        self.p_attention_dropout = hparams.p_attention_dropout\n",
    "        self.p_decoder_dropout = hparams.p_decoder_dropout\n",
    "        self.p_teacher_forcing = hparams.p_teacher_forcing\n",
    "        self.cudnn_enabled = hparams.cudnn_enabled\n",
    "        self.non_attentive = hparams.non_attentive\n",
    "        self.positional_embedding_dim = hparams.positional_embedding_dim\n",
    "        self.decoder_rnn_dim_nlayers = hparams.decoder_rnn_dim_nlayers\n",
    "        self.location_specific_attention = hparams.location_specific_attention\n",
    "        if hparams.non_attentive:\n",
    "            self.gaussian_upsampling = GaussianUpsampling(hparams)\n",
    "            self.range_predictor = RangePredictor(hparams)\n",
    "            self.duration_predictor = DurationPredictor(hparams)\n",
    "        self.prenet = Prenet(\n",
    "            hparams.n_mel_channels, [hparams.prenet_dim, hparams.prenet_dim],\n",
    "        )\n",
    "\n",
    "        if self.location_specific_attention:\n",
    "            self.attention_rnn = nn.LSTMCell(\n",
    "                hparams.prenet_dim + self.encoder_embedding_dim,\n",
    "                hparams.attention_rnn_dim,\n",
    "            )\n",
    "\n",
    "            self.attention_layer = Attention(\n",
    "                hparams.attention_rnn_dim,\n",
    "                self.encoder_embedding_dim,\n",
    "                hparams.attention_dim,\n",
    "                hparams.attention_location_n_filters,\n",
    "                hparams.attention_location_kernel_size,\n",
    "                fp16_run=hparams.fp16_run,\n",
    "            )\n",
    "            self.linear_projection = LinearNorm(\n",
    "                hparams.decoder_rnn_dim + self.encoder_embedding_dim,\n",
    "                hparams.n_mel_channels * hparams.n_frames_per_step_initial,\n",
    "            )\n",
    "            self.decoder_rnn = nn.LSTMCell(\n",
    "                hparams.attention_rnn_dim + self.encoder_embedding_dim,\n",
    "                hparams.decoder_rnn_dim,\n",
    "                1,\n",
    "            )\n",
    "        if self.non_attentive:\n",
    "            self.linear_projection = LinearNorm(\n",
    "                hparams.decoder_rnn_dim\n",
    "                + hparams.encoder_embedding_dim\n",
    "                + self.positional_embedding_dim,\n",
    "                hparams.n_mel_channels,\n",
    "            )\n",
    "            self.positional_embedding = PositionalEncoding(\n",
    "                hparams.positional_embedding_dim\n",
    "            )\n",
    "\n",
    "            #NOTE (SAM): https://stackoverflow.com/questions/57048120/pytorch-lstm-vs-lstmcell\n",
    "            self.decoder_rnn = nn.LSTM(input_size=self.prenet_dim + self.encoder_embedding_dim + self.positional_embedding_dim,hidden_size=self.decoder_rnn_dim,num_layers=self.decoder_rnn_dim_nlayers,batch_first=True)\n",
    "        # self.linear_projection2 = LinearNorm(hparams.positional_embedding_dim, hparams.n_mel_channels)\n",
    "        self.gate_layer = LinearNorm(\n",
    "            hparams.decoder_rnn_dim + self.encoder_embedding_dim,\n",
    "            1,\n",
    "            bias=True,\n",
    "            w_init_gain=\"sigmoid\",\n",
    "        )\n",
    "\n",
    "    def set_current_frames_per_step(self, n_frames: int):\n",
    "        self.n_frames_per_step_current = n_frames\n",
    "\n",
    "    def get_go_frame(self, memory):\n",
    "        \"\"\"Gets all zeros frames to use as first decoder input\n",
    "        PARAMS\n",
    "        ------\n",
    "        memory: decoder outputs\n",
    "\n",
    "        RETURNS\n",
    "        -------\n",
    "        decoder_input: all zeros frames\n",
    "        \"\"\"\n",
    "        B = memory.size(0)\n",
    "        decoder_input = Variable(memory.data.new(B, self.n_mel_channels).zero_())\n",
    "        return decoder_input\n",
    "\n",
    "    def get_positional_embedding(self, durations):\n",
    "        \"\"\"\n",
    "            :param durations: [B, N]\n",
    "            :return positional_embedding_outputs: [B, T, positional_hidden]\n",
    "        \"\"\"\n",
    "\n",
    "        B = durations.size(0)\n",
    "        N = durations.size(1)\n",
    "\n",
    "        pos_durations = durations.long()\n",
    "        sum_len = pos_durations.sum(dim=1)\n",
    "        max_len = sum_len.max()\n",
    "        diff_len = max_len - sum_len\n",
    "        pos_durations[:, -1] = pos_durations[:, -1] + diff_len\n",
    "\n",
    "        ids = torch.arange(max_len, device=durations.device).expand(B, N, max_len)\n",
    "        pos_mask = ids < pos_durations.view(B, N, 1)\n",
    "        pos_ids = ids[pos_mask].view(-1, max_len)  # [B, T]\n",
    "        positional_embedding_outputs = self.positional_embedding(pos_ids)\n",
    "\n",
    "        return positional_embedding_outputs\n",
    "\n",
    "    def initialize_decoder_states(self, memory, mask):\n",
    "        \"\"\"Initializes attention rnn states, decoder rnn states, attention\n",
    "        weights, attention cumulative weights, attention context, stores memory\n",
    "        and stores processed memory\n",
    "        PARAMS\n",
    "        ------\n",
    "        memory: Encoder outputs\n",
    "        mask: Mask for padded data if training, expects None for inference\n",
    "        \"\"\"\n",
    "        B = memory.size(0)\n",
    "        MAX_TIME = memory.size(1)\n",
    "\n",
    "        if self.location_specific_attention:\n",
    "            self.attention_hidden = Variable(\n",
    "                memory.data.new(B, self.attention_rnn_dim).zero_()\n",
    "            )\n",
    "            self.attention_cell = Variable(\n",
    "                memory.data.new(B, self.attention_rnn_dim).zero_()\n",
    "            )\n",
    "            self.attention_weights = Variable(memory.data.new(B, MAX_TIME).zero_())\n",
    "            self.attention_weights_cum = Variable(memory.data.new(B, MAX_TIME).zero_())\n",
    "            self.attention_context = Variable(\n",
    "                memory.data.new(B, self.encoder_embedding_dim).zero_()\n",
    "            )\n",
    "            self.processed_memory = self.attention_layer.memory_layer(memory)\n",
    "            self.decoder_hidden = Variable(memory.data.new(B, self.decoder_rnn_dim).zero_())\n",
    "            self.decoder_cell = Variable(memory.data.new(B, self.decoder_rnnself.decoder_rnn_dim).zero_())\n",
    "            \n",
    "        if self.non_attentive:\n",
    "            self.decoder_hidden = Variable(memory.data.new(B, self.decoder_rnn_dim_nlayers, self.decoder_rnn_dim).zero_())\n",
    "\n",
    "\n",
    "        self.memory = memory\n",
    "\n",
    "        self.mask = mask\n",
    "\n",
    "    def parse_decoder_inputs(self, decoder_inputs):\n",
    "        \"\"\"Prepares decoder inputs, i.e. mel outputs\n",
    "        PARAMS\n",
    "        ------\n",
    "        decoder_inputs: inputs used for teacher-forced training, i.e. mel-specs\n",
    "\n",
    "        RETURNS\n",
    "        -------\n",
    "        inputs: processed decoder inputs\n",
    "\n",
    "        \"\"\"\n",
    "        # NOTE (SAM): Rewrite with einops rearrange\n",
    "        # (B, n_mel_channels, T_out) -> (B, T_out, n_mel_channels)\n",
    "\n",
    "        decoder_inputs = decoder_inputs.transpose(1, 2)\n",
    "        decoder_inputs = decoder_inputs.contiguous()\n",
    "        # print(decoder_inputs.shape, 'dec_in')\n",
    "        # print(decoder_inputs.size(0), int(decoder_inputs.size(1)), self.n_frames_per_step_current)\n",
    "        decoder_inputs = decoder_inputs.view(\n",
    "            decoder_inputs.size(0),\n",
    "            int(decoder_inputs.size(1) / self.n_frames_per_step_current),\n",
    "            -1,\n",
    "        )\n",
    "        # (B, T_out, n_mel_channels) -> (T_out, B, n_mel_channels)\n",
    "        decoder_inputs = decoder_inputs.transpose(0, 1)\n",
    "        return decoder_inputs\n",
    "\n",
    "    def parse_decoder_outputs(self, mel_outputs, gate_outputs, alignments):\n",
    "        \"\"\"Prepares decoder outputs for output\n",
    "        PARAMS\n",
    "        ------\n",
    "        mel_outputs:\n",
    "        gate_outputs: gate output energies\n",
    "        alignments:\n",
    "\n",
    "        RETURNS\n",
    "        -------\n",
    "        mel_outputs:\n",
    "        gate_outpust: gate output energies\n",
    "        alignments:\n",
    "        \"\"\"\n",
    "        # (T_out, B) -> (B, T_out)\n",
    "        alignments = torch.stack(alignments).transpose(0, 1)\n",
    "        # (T_out, B) -> (B, T_out)\n",
    "        gate_outputs = torch.stack(gate_outputs)\n",
    "        if len(gate_outputs.size()) > 1:\n",
    "            gate_outputs = gate_outputs.transpose(0, 1)\n",
    "        else:\n",
    "            gate_outputs = gate_outputs[None]\n",
    "        gate_outputs = gate_outputs.contiguous()\n",
    "        # (B, T_out, n_mel_channels * n_frames_per_step) -> (B, T_out * n_frames_per_step, n_mel_channels)\n",
    "        mel_outputs = mel_outputs.view(mel_outputs.size(0), -1, self.n_mel_channels)\n",
    "        # (B, T_out, n_mel_channels) -> (B, n_mel_channels, T_out)\n",
    "        mel_outputs = mel_outputs.transpose(1, 2)\n",
    "\n",
    "        return mel_outputs, gate_outputs, alignments\n",
    "\n",
    "    def decode(self, decoder_input, attention_weights=None):\n",
    "        \"\"\"Decoder step using stored states, attention and memory\n",
    "        PARAMS\n",
    "        ------\n",
    "        decoder_input: previous mel output\n",
    "\n",
    "        RETURNS\n",
    "        -------\n",
    "        mel_output:\n",
    "        gate_output: gate output energies\n",
    "        attention_weights:\n",
    "        \"\"\"\n",
    "\n",
    "        cell_input = torch.cat((decoder_input, self.attention_context), -1)\n",
    "\n",
    "        self.attention_hidden, self.attention_cell = self.attention_rnn(\n",
    "            cell_input, (self.attention_hidden, self.attention_cell)\n",
    "        )\n",
    "        self.attention_hidden = F.dropout(\n",
    "            self.attention_hidden, self.p_attention_dropout, self.training\n",
    "        )\n",
    "        self.attention_cell = F.dropout(\n",
    "            self.attention_cell, self.p_attention_dropout, self.training\n",
    "        )\n",
    "\n",
    "        attention_weights_cat = torch.cat(\n",
    "            (\n",
    "                self.attention_weights.unsqueeze(1),\n",
    "                self.attention_weights_cum.unsqueeze(1),\n",
    "            ),\n",
    "            dim=1,\n",
    "        )\n",
    "        self.attention_context, self.attention_weights = self.attention_layer(\n",
    "            self.attention_hidden,\n",
    "            self.memory,\n",
    "            self.processed_memory,\n",
    "            attention_weights_cat,\n",
    "            self.mask,\n",
    "            attention_weights,\n",
    "        )\n",
    "\n",
    "        self.attention_weights_cum += self.attention_weights\n",
    "\n",
    "        decoder_input = torch.cat((self.attention_hidden, self.attention_context), -1)\n",
    "        self.decoder_hidden, self.decoder_cell = self.decoder_rnn(\n",
    "            decoder_input, (self.decoder_hidden, self.decoder_cell)\n",
    "        )\n",
    "        self.decoder_hidden = F.dropout(\n",
    "            self.decoder_hidden, self.p_decoder_dropout, self.training\n",
    "        )\n",
    "        self.decoder_cell = F.dropout(\n",
    "            self.decoder_cell, self.p_decoder_dropout, self.training\n",
    "        )\n",
    "\n",
    "        decoder_hidden_attention_context = torch.cat(\n",
    "            (self.decoder_hidden, self.attention_context), dim=1\n",
    "        )\n",
    "\n",
    "        decoder_output = self.linear_projection(decoder_hidden_attention_context)\n",
    "\n",
    "        gate_prediction = self.gate_layer(decoder_hidden_attention_context)\n",
    "        return decoder_output, gate_prediction, self.attention_weights\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        encoder_outputs,\n",
    "        decoder_inputs,\n",
    "        output_lengths,\n",
    "        input_lengths=None,\n",
    "        durations=None,\n",
    "    ):\n",
    "        \"\"\"Decoder forward pass for training\n",
    "        PARAMS\n",
    "        ------\n",
    "        encoder_outputs: Encoder outputs\n",
    "        decoder_inputs: Decoder inputs for teacher forcing. i.e. mel-specs\n",
    "        output_lengths: Encoder output lengths for attention masking.\n",
    "        input_lenghts: phrase lengths ntokens (same as inut lengths?)\n",
    "\n",
    "        RETURNS\n",
    "        -------\n",
    "        mel_outputs: mel outputs from the decoder\n",
    "        gate_outputs: gate outputs from the decoder\n",
    "        alignments: sequence of attention weights from the decoder\n",
    "        \"\"\"\n",
    "        B = encoder_outputs.size(0)\n",
    "        mel_outputs = torch.empty(\n",
    "            B, 0, self.n_frames_per_step_current * self.n_mel_channels\n",
    "        )\n",
    "        if torch.cuda.is_available() and self.cudnn_enabled:\n",
    "            mel_outputs = mel_outputs.cuda()\n",
    "\n",
    "        if self.location_specific_attention:\n",
    "\n",
    "            #pdb.set_trace()  # NOTE (SAM): FIX THIS USING EINOPS REARRANGE TO NOT BE INSANE\n",
    "\n",
    "            decoder_inputs = self.parse_decoder_inputs(decoder_inputs)\n",
    "            decoder_inputs = decoder_inputs.reshape(\n",
    "                -1, decoder_inputs.size(1), self.n_mel_channels\n",
    "            )\n",
    "            decoder_input = self.get_go_frame(encoder_outputs).unsqueeze(0)\n",
    "            decoder_inputs = torch.cat((decoder_input, decoder_inputs), dim=0)\n",
    "            decoder_inputs = self.prenet(decoder_inputs)\n",
    "\n",
    "            self.initialize_decoder_states(\n",
    "                encoder_outputs, mask=~get_mask_from_lengths(output_lengths)\n",
    "            )\n",
    "\n",
    "            gate_outputs, alignments = [], []\n",
    "            desired_output_frames = (\n",
    "                decoder_inputs.size(0) / self.n_frames_per_step_current\n",
    "            )\n",
    "            while mel_outputs.size(1) < desired_output_frames - 1:\n",
    "                if (\n",
    "                    mel_outputs.size(1) == 0\n",
    "                    or np.random.uniform(0.0, 1.0) <= self.p_teacher_forcing\n",
    "                ):\n",
    "                    teacher_forced_frame = decoder_inputs[\n",
    "                        mel_outputs.size(1) * self.n_frames_per_step_current\n",
    "                    ]\n",
    "\n",
    "                    to_concat = (teacher_forced_frame,)\n",
    "                    decoder_input = torch.cat(to_concat, dim=1)\n",
    "                else:\n",
    "\n",
    "                    # NOTE(zach): we may need to concat these as we go to ensure that\n",
    "                    # it's easy to retrieve the last n_frames_per_step_init frames.\n",
    "                    to_concat = (\n",
    "                        self.prenet(\n",
    "                            mel_outputs[:, -1, -1 * self.n_frames_per_step_current :]\n",
    "                        ),\n",
    "                    )\n",
    "                    decoder_input = torch.cat(to_concat, dim=1)\n",
    "                # NOTE(zach): When training with fp16_run == True, decoder_rnn seems to run into\n",
    "                # issues with NaNs in gradient, maybe due to vanishing gradients.\n",
    "                # Disable half-precision for this call to work around the issue.\n",
    "                with autocast(enabled=False):\n",
    "                    mel_output, gate_output, attention_weights = self.decode(\n",
    "                        decoder_input\n",
    "                    )\n",
    "                mel_outputs = torch.cat(\n",
    "                    [\n",
    "                        mel_outputs,\n",
    "                        mel_output[\n",
    "                            :, 0 : self.n_mel_channels * self.n_frames_per_step_current\n",
    "                        ].unsqueeze(1),\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "                gate_outputs += [gate_output.squeeze()] * self.n_frames_per_step_current\n",
    "                alignments += [attention_weights]\n",
    "\n",
    "        if self.non_attentive:\n",
    "            \n",
    "            self.initialize_decoder_states(\n",
    "                encoder_outputs, mask=~get_mask_from_lengths(output_lengths)\n",
    "            )\n",
    "            predicted_durations = self.duration_predictor(\n",
    "                encoder_outputs, input_lengths.cpu()\n",
    "            )\n",
    "            range_outputs = self.range_predictor(\n",
    "                encoder_outputs, durations, input_lengths.cpu()\n",
    "            )\n",
    "\n",
    "            encoder_upsampling_outputs = self.gaussian_upsampling(\n",
    "                encoder_outputs, durations, range_outputs, input_lengths\n",
    "            )\n",
    "\n",
    "            positional_embedding_outputs = self.get_positional_embedding(durations)\n",
    "            encoder_concated_outputs = torch.cat(\n",
    "                [encoder_upsampling_outputs, positional_embedding_outputs], dim=2\n",
    "            )\n",
    "            #pdb.set_trace()\n",
    "\n",
    "            guided_mel_specs = decoder_inputs[:, :, :-1]\n",
    "            dummy_spec = self.get_go_frame(encoder_outputs).unsqueeze(2)\n",
    "            guided_mel_specs = torch.cat([dummy_spec, guided_mel_specs], dim=2)\n",
    "            guided_mel_specs = rearrange(guided_mel_specs, \"b m o -> b o m\")\n",
    "            # guided_mel_specs = guided_mel_specs.transpose(1, 2)\n",
    "            prenet_outputs = self.prenet(guided_mel_specs)\n",
    "            concated_prenet_outputs = torch.cat(\n",
    "                [prenet_outputs, encoder_concated_outputs], dim=2\n",
    "            )\n",
    "            # NOTE (Sam): durations should sum to target lenghts\n",
    "            target_lengths= durations.long().sum(dim=1).detach().cpu()\n",
    "            packed_prenet_outputs = pack_padded_sequence(concated_prenet_outputs,target_lengths,batch_first=True,enforce_sorted=False,)\n",
    "            \n",
    "            #self.decoder_rnn.flatten_parameters()\n",
    "            decoder_lstm_outputs, _ =  self.decoder_rnn(concated_prenet_outputs)\n",
    "            #decoder_lstm_outputs, _ = self.decoder_rnn(concated_prenet_outputs,(self.decoder_hidden, self.decoder_cell))  # [B, T, decoder_rnn_dim]\n",
    "            decoder_lstm_outputs, _ = pad_packed_sequence(\n",
    "                decoder_lstm_outputs, batch_first=True\n",
    "            )\n",
    "            pdb.set_trace()\n",
    "            concated_decoder_lstm_outputs = torch.cat(\n",
    "                [decoder_lstm_outputs, encoder_concated_outputs], dim=2\n",
    "            )  # [B, T, decoder_lstm_dim + encoder_lstm_dim + positional_embedding_dim]\n",
    "            predicted_mel_specs = self.linear_projection(\n",
    "                concated_decoder_lstm_outputs\n",
    "            )  # [B, T, n_mel_channels]\n",
    "            predicted_mel_specs = predicted_mel_specs.transpose(\n",
    "                1, 2\n",
    "            )  # [B, n_mel_channels, T]\n",
    "\n",
    "        mel_outputs, gate_outputs, alignments = self.parse_decoder_outputs(\n",
    "            mel_outputs, gate_outputs, alignments\n",
    "        )\n",
    "\n",
    "        return mel_outputs, gate_outputs, alignments\n",
    "\n",
    "    def inference(self, memory, input_lengths):\n",
    "        \"\"\"Decoder inference\n",
    "        PARAMS\n",
    "        ------\n",
    "        memory: Encoder outputs\n",
    "        input_lengths (nee memory_lenghts): encoder input_lengths\n",
    "\n",
    "        RETURNS\n",
    "        -------\n",
    "        mel_outputs: mel outputs from the decoder\n",
    "        gate_outputs: gate outputs from the decoder\n",
    "        alignments: sequence of attention weights from the decoder\n",
    "        \"\"\"\n",
    "        decoder_input = self.get_go_frame(memory)\n",
    "        self.initialize_decoder_states(\n",
    "            memory, mask=~get_mask_from_lengths(input_lengths)\n",
    "        )\n",
    "\n",
    "        B = memory.size(0)\n",
    "        mel_outputs = torch.empty(\n",
    "            B, 0, self.n_frames_per_step_current * self.n_mel_channels\n",
    "        )\n",
    "        if torch.cuda.is_available() and self.cudnn_enabled:\n",
    "            mel_outputs = mel_outputs.cuda()\n",
    "        gate_outputs, alignments = [], []\n",
    "\n",
    "        mel_lengths = torch.zeros(\n",
    "            [memory.size(0)], dtype=torch.int32, device=memory.device\n",
    "        )\n",
    "        not_finished = torch.ones(\n",
    "            [memory.size(0)], dtype=torch.int32, device=memory.device\n",
    "        )\n",
    "\n",
    "        while True:\n",
    "            to_cat = (self.prenet(decoder_input),)\n",
    "\n",
    "            decoder_input = torch.cat(to_cat, dim=1)\n",
    "            mel_output, gate_output, alignment = self.decode(decoder_input)\n",
    "            mel_output = mel_output[\n",
    "                :, 0 : self.n_mel_channels * self.n_frames_per_step_current\n",
    "            ].unsqueeze(1)\n",
    "\n",
    "            mel_outputs = torch.cat([mel_outputs, mel_output], dim=1)\n",
    "            gate_outputs += [gate_output.squeeze()] * self.n_frames_per_step_current\n",
    "            alignments += [alignment]\n",
    "\n",
    "            dec = (\n",
    "                torch.le(torch.sigmoid(gate_output), self.gate_threshold)\n",
    "                .to(torch.int32)\n",
    "                .squeeze(1)\n",
    "            )\n",
    "\n",
    "            not_finished = not_finished * dec\n",
    "            mel_lengths += not_finished\n",
    "\n",
    "            if torch.sum(not_finished) == 0:\n",
    "                break\n",
    "            if mel_outputs.shape[1] == self.max_decoder_steps:\n",
    "                print(\"Warning! Reached max decoder steps\")\n",
    "                break\n",
    "\n",
    "            decoder_input = mel_output[:, -1, -1 * self.n_mel_channels :]\n",
    "        mel_outputs, gate_outputs, alignments = self.parse_decoder_outputs(\n",
    "            mel_outputs, gate_outputs, alignments\n",
    "        )\n",
    "\n",
    "        return mel_outputs, gate_outputs, alignments, mel_lengths\n",
    "\n",
    "    def inference_noattention(self, memory, attention_map):\n",
    "        \"\"\"Decoder inference\n",
    "        PARAMS\n",
    "        ------\n",
    "        memory: Encoder outputs\n",
    "\n",
    "        RETURNS\n",
    "        -------\n",
    "        mel_outputs: mel outputs from the decoder\n",
    "        gate_outputs: gate outputs from the decoder\n",
    "        alignments: sequence of attention weights from the decoder\n",
    "        \"\"\"\n",
    "        decoder_input = self.get_go_frame(memory)\n",
    "\n",
    "        self.initialize_decoder_states(memory, mask=None)\n",
    "\n",
    "        B = memory.size(0)\n",
    "        mel_outputs = torch.empty(\n",
    "            B, 0, self.n_frames_per_step_current * self.n_mel_channels\n",
    "        )\n",
    "        if torch.cuda.is_available() and self.cudnn_enabled:\n",
    "            mel_outputs = mel_outputs.cuda()\n",
    "        gate_outputs, alignments = [], []\n",
    "        for i in range(len(attention_map)):\n",
    "\n",
    "            attention = attention_map[i]\n",
    "            decoder_input = self.prenet(decoder_input)\n",
    "            mel_output, gate_output, alignment = self.decode(decoder_input, attention)\n",
    "            mel_output = mel_output[\n",
    "                :, 0 : self.n_mel_channels * self.n_frames_per_step_current\n",
    "            ].unsqueeze(1)\n",
    "\n",
    "            mel_outputs = torch.cat([mel_outputs, mel_output], dim=1)\n",
    "            gate_outputs += [gate_output.squeeze()] * self.n_frames_per_step_current\n",
    "            alignments += [alignment]\n",
    "\n",
    "            decoder_input = mel_output[:, -1, -1 * self.n_mel_channels :]\n",
    "\n",
    "        mel_outputs, gate_outputs, alignments = self.parse_decoder_outputs(\n",
    "            mel_outputs, gate_outputs, alignments\n",
    "        )\n",
    "\n",
    "        return mel_outputs, gate_outputs, alignments\n",
    "\n",
    "    def inference_partial_tf(self, memory, decoder_inputs, tf_until_idx, device=\"cpu\"):\n",
    "        \"\"\"Decoder inference with teacher-forcing up until tf_until_idx\n",
    "        PARAMS\n",
    "        ------\n",
    "        memory: Encoder outputs\n",
    "        decoder_inputs: Decoder inputs for teacher forcing. i.e. mel-specs\n",
    "        memory_lengths: Encoder output lengths (decoder input lengths) for attention masking.\n",
    "\n",
    "        RETURNS\n",
    "        -------\n",
    "        mel_outputs: mel outputs from the decoder\n",
    "        gate_outputs: gate outputs from the decoder\n",
    "        alignments: sequence of attention weights from the decoder\n",
    "        \"\"\"\n",
    "        if device == \"cuda\" and torch.cuda.is_available() and self.cudnn_enabled:\n",
    "            decoder_inputs = decoder_inputs.cuda()\n",
    "\n",
    "        B = memory.size(0)\n",
    "        decoder_inputs = self.parse_decoder_inputs(decoder_inputs)\n",
    "        decoder_inputs = decoder_inputs.reshape(\n",
    "            -1, decoder_inputs.size(1), self.n_mel_channels\n",
    "        )\n",
    "        decoder_input = self.get_go_frame(memory).unsqueeze(0)\n",
    "        decoder_inputs = torch.cat((decoder_input, decoder_inputs), dim=0)\n",
    "        decoder_inputs = self.prenet(decoder_inputs)\n",
    "\n",
    "        self.initialize_decoder_states(memory, mask=None)\n",
    "\n",
    "        mel_outputs = torch.empty(\n",
    "            B, 0, self.n_frames_per_step_current * self.n_mel_channels\n",
    "        )\n",
    "        if device == \"cuda\" and torch.cuda.is_available() and self.cudnn_enabled:\n",
    "            mel_outputs = mel_outputs.cuda()\n",
    "        gate_outputs, alignments = [], []\n",
    "\n",
    "        while True:\n",
    "            if mel_outputs.size(1) < tf_until_idx:\n",
    "                teacher_forced_frame = decoder_inputs[\n",
    "                    mel_outputs.size(1) * self.n_frames_per_step_current\n",
    "                ]\n",
    "\n",
    "                to_concat = (teacher_forced_frame,)\n",
    "                decoder_input = torch.cat(to_concat, dim=1)\n",
    "            else:\n",
    "\n",
    "                to_concat = (\n",
    "                    self.prenet(mel_outputs[:, -1, -1 * self.n_mel_channels :]),\n",
    "                )\n",
    "                decoder_input = torch.cat(to_concat, dim=1)\n",
    "            mel_output, gate_output, attention_weights = self.decode(decoder_input)\n",
    "            mel_outputs = torch.cat(\n",
    "                [\n",
    "                    mel_outputs,\n",
    "                    mel_output[\n",
    "                        :, 0 : self.n_mel_channels * self.n_frames_per_step_current\n",
    "                    ].unsqueeze(1),\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "            gate_outputs += [gate_output.squeeze()] * self.n_frames_per_step_current\n",
    "            alignments += [attention_weights]\n",
    "            if torch.sigmoid(gate_output.data) > self.gate_threshold:\n",
    "                break\n",
    "            elif mel_outputs.size(1) == self.max_decoder_steps:\n",
    "                print(\"Warning! Reached max decoder steps\")\n",
    "                break\n",
    "\n",
    "        mel_outputs, gate_outputs, alignments = self.parse_decoder_outputs(\n",
    "            mel_outputs, gate_outputs, alignments\n",
    "        )\n",
    "\n",
    "        return mel_outputs, gate_outputs, alignments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
