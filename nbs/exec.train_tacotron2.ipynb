{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d096a2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp exec.train_tacotron2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96fa8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# export\n",
    "from uberduck_ml_dev.trainer.tacotron2 import Tacotron2Trainer\n",
    "from uberduck_ml_dev.vendor.tfcompat.hparam import HParams\n",
    "from uberduck_ml_dev.trainer.tacotron2 import DEFAULTS as TACOTRON2_TRAINER_DEFAULTS\n",
    "import argparse\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "from torch import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7bf154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def parse_args(args):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--config\", help=\"Path to JSON config\")\n",
    "    args = parser.parse_args(args)\n",
    "    return args\n",
    "\n",
    "\n",
    "def run(rank, device_count, hparams):\n",
    "    trainer = Tacotron2Trainer(hparams, rank=rank, world_size=device_count)\n",
    "    try:\n",
    "        trainer.train()\n",
    "    except Exception as e:\n",
    "        print(f\"Exception raised while training: {e}\")\n",
    "        # TODO: save state.\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec68341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from uberduck_ml_dev.models.tacotron2.tacotron2 import NON_ATTENTIVE_DEFAULTS as NON_ATTENTIVE_DEFAULTS\n",
    "\n",
    "NON_ATTENTIVE_DEFAULTS.decoder_rnn_dim_nlayers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7382a0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('attention_dim', 128), ('attention_location_kernel_size', 31), ('attention_location_n_filters', 32), ('attention_rnn_dim', 1024), ('batch_size', 16), ('checkpoint_name', None), ('checkpoint_path', 'test/fixtures/results/checkpoints'), ('coarse_n_frames_per_step', None), ('compute_durations', True), ('cudnn_enabled', True), ('dataset_path', './dataset'), ('debug', False), ('decoder_rnn_dim', 1024), ('decoder_rnn_dim_nlayers', 2), ('distributed_run', False), ('duration_lstm_dim', 1024), ('encoder_embedding_dim', 512), ('encoder_kernel_size', 5), ('encoder_n_convolutions', 3), ('epochs', 5), ('epochs_per_checkpoint', 4), ('filter_length', 1024), ('fp16_run', False), ('gate_threshold', 0.5), ('grad_clip_thresh', 1.0), ('gst_type', None), ('has_speaker_embedding', False), ('hop_length', 256), ('ignore_layers', ['speaker_embedding.weight']), ('include_durations', True), ('include_f0', False), ('learning_rate', 0.001), ('location_specific_attention', False), ('log_dir', 'test/fixtures/results/logs'), ('mask_padding', True), ('max_decoder_steps', 1000), ('max_wav_value', 32768.0), ('mel_fmax', 8000), ('mel_fmin', 0), ('n_frames_per_step_initial', 1), ('n_mel_channels', 80), ('n_speakers', 1), ('n_symbols', 148), ('non_attentive', True), ('num_heads', 8), ('p_arpabet', 1.0), ('p_attention_dropout', 0.1), ('p_decoder_dropout', 0.1), ('p_teacher_forcing', 1.0), ('pos_weight', None), ('positional_embedding_dim', 32), ('postnet_embedding_dim', 512), ('postnet_kernel_size', 5), ('postnet_n_convolutions', 5), ('prenet_dim', 256), ('prenet_f0_dim', 1), ('prenet_f0_kernel_size', 1), ('prenet_f0_n_layers', 1), ('prenet_fms_kernel_size', 1), ('prenet_rms_dim', 0), ('range_lstm_dim', 1024), ('reduction_window_schedule', [{'until_step': 10000, 'batch_size': 16, 'n_frames_per_step': 1}, {'until_step': 50000, 'batch_size': 16, 'n_frames_per_step': 1}, {'until_step': 60000, 'batch_size': 16, 'n_frames_per_step': 1}, {'until_step': 70000, 'batch_size': 16, 'n_frames_per_step': 1}, {'until_step': None, 'batch_size': 16, 'n_frames_per_step': 1}]), ('ref_enc_filters', [32, 32, 64, 64, 128, 128]), ('ref_enc_gru_size', 128), ('ref_enc_pad', [1, 1]), ('ref_enc_size', [3, 3]), ('ref_enc_strides', [2, 2]), ('sample_inference_speaker_ids', [0]), ('sample_inference_text', 'That quick beige fox jumped in the air loudly over the thin dog fence.'), ('sampling_rate', 22050), ('seed', 1234), ('speaker_embedding_dim', 128), ('steps_per_sample', 100), ('symbol_set', 'nvidia_taco2'), ('symbols_embedding_dim', 512), ('text_cleaners', ['english_cleaners']), ('torchmoji_model_file', None), ('torchmoji_vocabulary_file', None), ('training_audiopaths_and_text', 'test/fixtures/ljtest/list.txt'), ('val_audiopaths_and_text', 'test/fixtures/ljtest/list.txt'), ('warm_start_name', 'test/fixtures/models/taco2ljdefault'), ('weight_decay', 1e-06), ('win_length', 1024), ('with_gst', False)]\n",
      "TTSTrainer start 409670.987205211\n",
      "Initializing trainer with hparams:\n",
      "{'attention_dim': 128,\n",
      " 'attention_location_kernel_size': 31,\n",
      " 'attention_location_n_filters': 32,\n",
      " 'attention_rnn_dim': 1024,\n",
      " 'batch_size': 16,\n",
      " 'checkpoint_name': None,\n",
      " 'checkpoint_path': 'test/fixtures/results/checkpoints',\n",
      " 'coarse_n_frames_per_step': None,\n",
      " 'compute_durations': True,\n",
      " 'cudnn_enabled': True,\n",
      " 'dataset_path': './dataset',\n",
      " 'debug': False,\n",
      " 'decoder_rnn_dim': 1024,\n",
      " 'decoder_rnn_dim_nlayers': 2,\n",
      " 'distributed_run': False,\n",
      " 'duration_lstm_dim': 1024,\n",
      " 'encoder_embedding_dim': 512,\n",
      " 'encoder_kernel_size': 5,\n",
      " 'encoder_n_convolutions': 3,\n",
      " 'epochs': 5,\n",
      " 'epochs_per_checkpoint': 4,\n",
      " 'filter_length': 1024,\n",
      " 'fp16_run': False,\n",
      " 'gate_threshold': 0.5,\n",
      " 'grad_clip_thresh': 1.0,\n",
      " 'gst_type': None,\n",
      " 'has_speaker_embedding': False,\n",
      " 'hop_length': 256,\n",
      " 'ignore_layers': ['speaker_embedding.weight'],\n",
      " 'include_durations': True,\n",
      " 'include_f0': False,\n",
      " 'learning_rate': 0.001,\n",
      " 'location_specific_attention': False,\n",
      " 'log_dir': 'test/fixtures/results/logs',\n",
      " 'mask_padding': True,\n",
      " 'max_decoder_steps': 1000,\n",
      " 'max_wav_value': 32768.0,\n",
      " 'mel_fmax': 8000,\n",
      " 'mel_fmin': 0,\n",
      " 'n_frames_per_step_initial': 1,\n",
      " 'n_mel_channels': 80,\n",
      " 'n_speakers': 1,\n",
      " 'n_symbols': 148,\n",
      " 'non_attentive': True,\n",
      " 'num_heads': 8,\n",
      " 'p_arpabet': 1.0,\n",
      " 'p_attention_dropout': 0.1,\n",
      " 'p_decoder_dropout': 0.1,\n",
      " 'p_teacher_forcing': 1.0,\n",
      " 'pos_weight': None,\n",
      " 'positional_embedding_dim': 32,\n",
      " 'postnet_embedding_dim': 512,\n",
      " 'postnet_kernel_size': 5,\n",
      " 'postnet_n_convolutions': 5,\n",
      " 'prenet_dim': 256,\n",
      " 'prenet_f0_dim': 1,\n",
      " 'prenet_f0_kernel_size': 1,\n",
      " 'prenet_f0_n_layers': 1,\n",
      " 'prenet_fms_kernel_size': 1,\n",
      " 'prenet_rms_dim': 0,\n",
      " 'range_lstm_dim': 1024,\n",
      " 'reduction_window_schedule': [{'batch_size': 16,\n",
      "                                'n_frames_per_step': 1,\n",
      "                                'until_step': 10000},\n",
      "                               {'batch_size': 16,\n",
      "                                'n_frames_per_step': 1,\n",
      "                                'until_step': 50000},\n",
      "                               {'batch_size': 16,\n",
      "                                'n_frames_per_step': 1,\n",
      "                                'until_step': 60000},\n",
      "                               {'batch_size': 16,\n",
      "                                'n_frames_per_step': 1,\n",
      "                                'until_step': 70000},\n",
      "                               {'batch_size': 16,\n",
      "                                'n_frames_per_step': 1,\n",
      "                                'until_step': None}],\n",
      " 'ref_enc_filters': [32, 32, 64, 64, 128, 128],\n",
      " 'ref_enc_gru_size': 128,\n",
      " 'ref_enc_pad': [1, 1],\n",
      " 'ref_enc_size': [3, 3],\n",
      " 'ref_enc_strides': [2, 2],\n",
      " 'sample_inference_speaker_ids': [0],\n",
      " 'sample_inference_text': 'That quick beige fox jumped in the air loudly over '\n",
      "                          'the thin dog fence.',\n",
      " 'sampling_rate': 22050,\n",
      " 'seed': 1234,\n",
      " 'speaker_embedding_dim': 128,\n",
      " 'steps_per_sample': 100,\n",
      " 'symbol_set': 'nvidia_taco2',\n",
      " 'symbols_embedding_dim': 512,\n",
      " 'text_cleaners': ['english_cleaners'],\n",
      " 'torchmoji_model_file': None,\n",
      " 'torchmoji_vocabulary_file': None,\n",
      " 'training_audiopaths_and_text': 'test/fixtures/ljtest/list.txt',\n",
      " 'val_audiopaths_and_text': 'test/fixtures/ljtest/list.txt',\n",
      " 'warm_start_name': 'test/fixtures/models/taco2ljdefault',\n",
      " 'weight_decay': 1e-06,\n",
      " 'win_length': 1024,\n",
      " 'with_gst': False}\n",
      "start train 409671.021428318\n",
      "[NeMo I 2022-01-15 18:21:43 cloud:56] Found existing object /home/s_uberduck_ai/.cache/torch/NeMo/NeMo_1.5.1/qn5x5_libri_tts_phonemes/656c7439dd3a0d614978529371be498b/qn5x5_libri_tts_phonemes.nemo.\n",
      "[NeMo I 2022-01-15 18:21:43 cloud:62] Re-using file from: /home/s_uberduck_ai/.cache/torch/NeMo/NeMo_1.5.1/qn5x5_libri_tts_phonemes/656c7439dd3a0d614978529371be498b/qn5x5_libri_tts_phonemes.nemo\n",
      "[NeMo I 2022-01-15 18:21:43 common:728] Instantiating model from pre-trained checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-01-15 18:21:44 features:243] Using torch_stft is deprecated and will be removed in 1.1.0. Please set stft_conv and stft_exact_pad to False for FilterbankFeatures and AudioToMelSpectrogramPreprocessor. Please set exact_pad to True as needed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-01-15 18:21:44 features:265] PADDING: 1\n",
      "[NeMo I 2022-01-15 18:21:44 features:275] STFT using conv\n",
      "[NeMo I 2022-01-15 18:21:48 save_restore_connector:149] Model EncDecCTCModel was successfully restored from /home/s_uberduck_ai/.cache/torch/NeMo/NeMo_1.5.1/qn5x5_libri_tts_phonemes/656c7439dd3a0d614978529371be498b/qn5x5_libri_tts_phonemes.nemo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo E 2022-01-15 18:21:48 vocabs:324] Torch distributed needs to be initialized before you initialized <nemo.collections.asr.data.vocabs.Phonemes object at 0x7f0937124c10>. This class is prone to data access race conditions. Now downloading corpora from global rank 0. If other ranks pass this before rank 0, errors might result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-01-15 18:21:48 cloud:56] Found existing object /home/s_uberduck_ai/.cache/torch/NeMo/NeMo_1.5.1/qn5x5_libri_tts_phonemes/656c7439dd3a0d614978529371be498b/qn5x5_libri_tts_phonemes.nemo.\n",
      "[NeMo I 2022-01-15 18:21:48 cloud:62] Re-using file from: /home/s_uberduck_ai/.cache/torch/NeMo/NeMo_1.5.1/qn5x5_libri_tts_phonemes/656c7439dd3a0d614978529371be498b/qn5x5_libri_tts_phonemes.nemo\n",
      "[NeMo I 2022-01-15 18:21:48 common:728] Instantiating model from pre-trained checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-01-15 18:21:49 features:243] Using torch_stft is deprecated and will be removed in 1.1.0. Please set stft_conv and stft_exact_pad to False for FilterbankFeatures and AudioToMelSpectrogramPreprocessor. Please set exact_pad to True as needed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-01-15 18:21:49 features:265] PADDING: 1\n",
      "[NeMo I 2022-01-15 18:21:49 features:275] STFT using conv\n",
      "[NeMo I 2022-01-15 18:21:50 save_restore_connector:149] Model EncDecCTCModel was successfully restored from /home/s_uberduck_ai/.cache/torch/NeMo/NeMo_1.5.1/qn5x5_libri_tts_phonemes/656c7439dd3a0d614978529371be498b/qn5x5_libri_tts_phonemes.nemo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo E 2022-01-15 18:21:50 vocabs:324] Torch distributed needs to be initialized before you initialized <nemo.collections.asr.data.vocabs.Phonemes object at 0x7f0930e4be50>. This class is prone to data access race conditions. Now downloading corpora from global rank 0. If other ranks pass this before rank 0, errors might result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using any style tokens\n",
      "Starting warm_start 409678.771819439\n",
      "WARNING! Attempting to load a model with out the decoder.range_predictor.lstm.weight_ih_l0 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.range_predictor.lstm.weight_hh_l0 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.range_predictor.lstm.bias_ih_l0 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.range_predictor.lstm.bias_hh_l0 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.range_predictor.lstm.weight_ih_l0_reverse layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.range_predictor.lstm.weight_hh_l0_reverse layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.range_predictor.lstm.bias_ih_l0_reverse layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.range_predictor.lstm.bias_hh_l0_reverse layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.range_predictor.lstm.weight_ih_l1 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.range_predictor.lstm.weight_hh_l1 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.range_predictor.lstm.bias_ih_l1 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.range_predictor.lstm.bias_hh_l1 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.range_predictor.lstm.weight_ih_l1_reverse layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.range_predictor.lstm.weight_hh_l1_reverse layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.range_predictor.lstm.bias_ih_l1_reverse layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.range_predictor.lstm.bias_hh_l1_reverse layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.range_predictor.proj.linear_layer.weight layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.range_predictor.proj.linear_layer.bias layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.duration_predictor.lstm.weight_ih_l0 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.duration_predictor.lstm.weight_hh_l0 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.duration_predictor.lstm.bias_ih_l0 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.duration_predictor.lstm.bias_hh_l0 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.duration_predictor.lstm.weight_ih_l0_reverse layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.duration_predictor.lstm.weight_hh_l0_reverse layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.duration_predictor.lstm.bias_ih_l0_reverse layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.duration_predictor.lstm.bias_hh_l0_reverse layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.duration_predictor.lstm.weight_ih_l1 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.duration_predictor.lstm.weight_hh_l1 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.duration_predictor.lstm.bias_ih_l1 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.duration_predictor.lstm.bias_hh_l1 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.duration_predictor.lstm.weight_ih_l1_reverse layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.duration_predictor.lstm.weight_hh_l1_reverse layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.duration_predictor.lstm.bias_ih_l1_reverse layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.duration_predictor.lstm.bias_hh_l1_reverse layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.duration_predictor.proj.linear_layer.weight layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.duration_predictor.proj.linear_layer.bias layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.positional_embedding.pe layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.decoder_rnn.weight_ih_l0 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.decoder_rnn.weight_hh_l0 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.decoder_rnn.bias_ih_l0 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.decoder_rnn.bias_hh_l0 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.decoder_rnn.weight_ih_l1 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.decoder_rnn.weight_hh_l1 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.decoder_rnn.bias_ih_l1 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.decoder_rnn.bias_hh_l1 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the duration_predictor.lstm.weight_ih_l0 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the duration_predictor.lstm.weight_hh_l0 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the duration_predictor.lstm.bias_ih_l0 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the duration_predictor.lstm.bias_hh_l0 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the duration_predictor.lstm.weight_ih_l0_reverse layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the duration_predictor.lstm.weight_hh_l0_reverse layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the duration_predictor.lstm.bias_ih_l0_reverse layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the duration_predictor.lstm.bias_hh_l0_reverse layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the duration_predictor.lstm.weight_ih_l1 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the duration_predictor.lstm.weight_hh_l1 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the duration_predictor.lstm.bias_ih_l1 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the duration_predictor.lstm.bias_hh_l1 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the duration_predictor.lstm.weight_ih_l1_reverse layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the duration_predictor.lstm.weight_hh_l1_reverse layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the duration_predictor.lstm.bias_ih_l1_reverse layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the duration_predictor.lstm.bias_hh_l1_reverse layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the duration_predictor.proj.linear_layer.weight layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the duration_predictor.proj.linear_layer.bias layer. This could lead to unexpected results during evaluation.\n",
      "Exception raised while training: Error(s) in loading state_dict for Tacotron2:\n",
      "\tUnexpected key(s) in state_dict: \"decoder.attention_rnn.weight_ih\", \"decoder.attention_rnn.weight_hh\", \"decoder.attention_rnn.bias_ih\", \"decoder.attention_rnn.bias_hh\", \"decoder.attention_layer.query_layer.linear_layer.weight\", \"decoder.attention_layer.memory_layer.linear_layer.weight\", \"decoder.attention_layer.v.linear_layer.weight\", \"decoder.attention_layer.location_layer.location_conv.conv.weight\", \"decoder.attention_layer.location_layer.location_dense.linear_layer.weight\", \"decoder.decoder_rnn.weight_ih\", \"decoder.decoder_rnn.weight_hh\", \"decoder.decoder_rnn.bias_ih\", \"decoder.decoder_rnn.bias_hh\". \n",
      "\tsize mismatch for decoder.linear_projection.linear_layer.weight: copying a param with shape torch.Size([80, 1536]) from checkpoint, the shape in current model is torch.Size([80, 1568]).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Tacotron2:\n\tUnexpected key(s) in state_dict: \"decoder.attention_rnn.weight_ih\", \"decoder.attention_rnn.weight_hh\", \"decoder.attention_rnn.bias_ih\", \"decoder.attention_rnn.bias_hh\", \"decoder.attention_layer.query_layer.linear_layer.weight\", \"decoder.attention_layer.memory_layer.linear_layer.weight\", \"decoder.attention_layer.v.linear_layer.weight\", \"decoder.attention_layer.location_layer.location_conv.conv.weight\", \"decoder.attention_layer.location_layer.location_dense.linear_layer.weight\", \"decoder.decoder_rnn.weight_ih\", \"decoder.decoder_rnn.weight_hh\", \"decoder.decoder_rnn.bias_ih\", \"decoder.decoder_rnn.bias_hh\". \n\tsize mismatch for decoder.linear_projection.linear_layer.weight: copying a param with shape torch.Size([80, 1536]) from checkpoint, the shape in current model is torch.Size([80, 1568]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22333/269145798.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_22333/1978015029.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(rank, device_count, hparams)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Exception raised while training: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# TODO: save state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_22333/1978015029.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(rank, device_count, hparams)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTacotron2Trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworld_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Exception raised while training: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/uberduck-experiments-v0/uberduck-ml-dev/uberduck_ml_dev/trainer/tacotron2.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    409\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarm_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp16_run\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/uberduck-experiments-v0/uberduck-ml-dev/uberduck_ml_dev/trainer/base.py\u001b[0m in \u001b[0;36mwarm_start\u001b[0;34m(self, model, optimizer, start_epoch)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mmodel_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_state_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mignore_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         )\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"optimizer\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_layers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/uberduck-experiments-v0/uberduck-ml-dev/uberduck_ml_dev/models/base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(self, warm_start_path, device, ignore_layers, model_dict)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mdummy_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mmodel_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdummy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1481\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1483\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1484\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Tacotron2:\n\tUnexpected key(s) in state_dict: \"decoder.attention_rnn.weight_ih\", \"decoder.attention_rnn.weight_hh\", \"decoder.attention_rnn.bias_ih\", \"decoder.attention_rnn.bias_hh\", \"decoder.attention_layer.query_layer.linear_layer.weight\", \"decoder.attention_layer.memory_layer.linear_layer.weight\", \"decoder.attention_layer.v.linear_layer.weight\", \"decoder.attention_layer.location_layer.location_conv.conv.weight\", \"decoder.attention_layer.location_layer.location_dense.linear_layer.weight\", \"decoder.decoder_rnn.weight_ih\", \"decoder.decoder_rnn.weight_hh\", \"decoder.decoder_rnn.bias_ih\", \"decoder.decoder_rnn.bias_hh\". \n\tsize mismatch for decoder.linear_projection.linear_layer.weight: copying a param with shape torch.Size([80, 1536]) from checkpoint, the shape in current model is torch.Size([80, 1568])."
     ]
    }
   ],
   "source": [
    "# skip\n",
    "config = NON_ATTENTIVE_DEFAULTS.values()\n",
    "with open(\"test/fixtures/ljtest/taco2_lj2lj.json\") as f:\n",
    "    config.update(json.load(f))\n",
    "hparams = HParams(**config)\n",
    "print(hparams)\n",
    "if hparams.distributed_run:\n",
    "    device_count = torch.cuda.device_count()\n",
    "    mp.spawn(run, (device_count, hparams), device_count)\n",
    "else:\n",
    "    run(None, None, hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959fb242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip\n",
    "config = TACOTRON2_TRAINER_DEFAULTS.values()\n",
    "with open(\"test/fixtures/ljtest/taco2_lj2lj.json\") as f:\n",
    "    config.update(json.load(f))\n",
    "hparams = HParams(**config)\n",
    "print(hparams)\n",
    "if hparams.distributed_run:\n",
    "    device_count = torch.cuda.device_count()\n",
    "    mp.spawn(run, (device_count, hparams), device_count)\n",
    "else:\n",
    "    run(None, None, hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227b9e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "try:\n",
    "    from nbdev.imports import IN_NOTEBOOK\n",
    "except:\n",
    "    IN_NOTEBOOK = False\n",
    "if __name__ == \"__main__\" and not IN_NOTEBOOK:\n",
    "    args = parse_args(sys.argv[1:])\n",
    "    config = TACOTRON2_TRAINER_DEFAULTS.values()\n",
    "    if args.config:\n",
    "        with open(args.config) as f:\n",
    "            config.update(json.load(f))\n",
    "    config.update(vars(args))\n",
    "    hparams = HParams(**config)\n",
    "    if hparams.distributed_run:\n",
    "        device_count = torch.cuda.device_count()\n",
    "        mp.spawn(run, (device_count, hparams), device_count)\n",
    "    else:\n",
    "        run(None, None, hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d43232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b80d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = None\n",
    "device_count = None\n",
    "trainer = Tacotron2Trainer(hparams, rank=rank, world_size=device_count)\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ce4b38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
