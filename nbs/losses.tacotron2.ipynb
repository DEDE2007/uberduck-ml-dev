{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce654ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp losses.tacotron2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3f73d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# export\n",
    "from random import choice, randint\n",
    "from uberduck_ml_dev.models.tacotron2.tacotron2 import Tacotron2\n",
    "import time\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from uberduck_ml_dev.utils.utils import get_mask_from_lengths\n",
    "from uberduck_ml_dev.data_loader import TextMelDataset, TextMelCollate\n",
    "\n",
    "from uberduck_ml_dev.utils.plot import save_figure_to_numpy\n",
    "from uberduck_ml_dev.utils.utils import reduce_tensor\n",
    "from uberduck_ml_dev.monitoring.statistics import get_alignment_metrics\n",
    "import nemo\n",
    "from typing import NamedTuple\n",
    "import pdb\n",
    "\n",
    "\n",
    "class Tacotron2Loss(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        pos_weight,\n",
    "        non_attentive=False,\n",
    "        location_specific_attention=True,\n",
    "        sampling_rate=None,\n",
    "        hop_length=None,\n",
    "    ):\n",
    "        if pos_weight is not None:\n",
    "            self.pos_weight = torch.tensor(pos_weight)\n",
    "        else:\n",
    "            self.pos_weight = pos_weight\n",
    "        self.non_attentive = non_attentive\n",
    "        self.location_specific_attention = location_specific_attention\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.hop_length = hop_length\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, model_output: NamedTuple, target: NamedTuple):\n",
    "\n",
    "        # NOTE (Sam) https://github.com/JoungheeKim/Non-Attentive-Tacotron/blob/c82049be0162f59898daaf8e1fa19fecaadeec31/tacotron/model.py\n",
    "        # uses mae loss as well on mels\n",
    "        mel_target = target.mel_padded\n",
    "        mel_target.requires_grad = False\n",
    "\n",
    "        mel_out_postnet = model_output.mel_outputs_postnet\n",
    "        mel_output = model_output.mel_outputs\n",
    "\n",
    "        if self.location_specific_attention:\n",
    "            gate_target = target.gate_target\n",
    "            gate_target.requires_grad = False\n",
    "            gate_pred = get_mask_from_lengths(\n",
    "                model_output.gate_pred[:, 0], max_len=gate_target.shape[1]\n",
    "            ).float()\n",
    "            gate_loss_batch = nn.BCEWithLogitsLoss(\n",
    "                pos_weight=self.pos_weight, reduce=False\n",
    "            )(gate_target, gate_pred).mean(axis=[1])\n",
    "            gate_loss = torch.mean(gate_loss_batch)\n",
    "\n",
    "        if self.non_attentive:\n",
    "            durations_padded = target.durations_padded\n",
    "            predicted_durations = model_output.predicted_durations\n",
    "            duration_mask = (durations_padded > 0.0).float()\n",
    "            non_zero_duration = duration_mask.sum()\n",
    "\n",
    "            in_seconds = self.sampling_rate / self.hop_length\n",
    "            predicted_durations = predicted_durations / in_seconds\n",
    "            durations_padded = durations_padded / in_seconds\n",
    "\n",
    "            predicted_durations_mse = nn.MSELoss(reduction=\"none\")(\n",
    "                predicted_durations, durations_padded\n",
    "            ).sum()\n",
    "            predicted_durations_mse = (predicted_durations_mse * duration_mask).sum(\n",
    "                axis=1\n",
    "            ) / non_zero_duration\n",
    "            durations_loss_batch = torch.nan_to_num(predicted_durations_mse)\n",
    "            durations_loss = durations_loss_batch.mean()\n",
    "\n",
    "        mel_loss_batch = nn.MSELoss(reduction=\"none\")(mel_output, mel_target).mean(\n",
    "            axis=[1, 2]\n",
    "        ) + nn.MSELoss(reduction=\"none\")(mel_out_postnet, mel_target).mean(axis=[1, 2])\n",
    "\n",
    "        mel_loss = mel_loss_batch.mean()\n",
    "\n",
    "        if self.location_specific_attention:\n",
    "            loss = mel_loss + gate_loss\n",
    "            loss_batch = mel_loss_batch + gate_loss_batch\n",
    "            return (\n",
    "                loss,\n",
    "                loss_batch,\n",
    "                mel_loss,\n",
    "                mel_loss_batch,\n",
    "                gate_loss,\n",
    "                gate_loss_batch,\n",
    "            )\n",
    "        if self.non_attentive:\n",
    "            loss = mel_loss + durations_loss\n",
    "            loss_batch = mel_loss_batch + durations_loss_batch\n",
    "            return (\n",
    "                loss,\n",
    "                loss_batch,\n",
    "                mel_loss,\n",
    "                mel_loss_batch,\n",
    "                durations_loss,\n",
    "                durations_loss_batch,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef7f8b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
